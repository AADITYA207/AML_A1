{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/Aaditya Gupta/OneDrive/Desktop/AML_A1/df_synA_test_hard_shuffled_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = pd.read_csv(\"C:/Users/Aaditya Gupta/OneDrive/Desktop/AML_A1/df_synA_train_shuffled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clean = df_0.drop(columns=['era', 'target_10_val','target_5_val', 'data_type' ])\n",
    "Y_clean = df_0['target_10_val']\n",
    "Y_clean_list = list(Y_clean.unique())\n",
    "Y_clean_new = np.zeros(len(Y_clean))\n",
    "Y_clean_Y_clean_new_dict = {}\n",
    "count = 0\n",
    "for i in range(len(Y_clean)):\n",
    "    if(Y_clean[i] not in Y_clean_Y_clean_new_dict):\n",
    "        Y_clean_Y_clean_new_dict[Y_clean[i]] = count\n",
    "        Y_clean_new[i] = Y_clean_Y_clean_new_dict[Y_clean[i]]\n",
    "        count+=1\n",
    "    else:\n",
    "        Y_clean_new[i] = Y_clean_Y_clean_new_dict[Y_clean[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['era', 'target_10_val','target_5_val','data_type' ])\n",
    "Y = df['target_10_val']\n",
    "Y_list = list(Y.unique())\n",
    "Y_new = np.zeros(len(Y))\n",
    "Y_Y_new_dict = {}\n",
    "count = 0\n",
    "for i in range(len(Y)):\n",
    "    if(Y[i] not in Y_Y_new_dict):\n",
    "        Y_Y_new_dict[Y[i]] = count\n",
    "        Y_new[i] = Y_Y_new_dict[Y[i]]\n",
    "        count+=1\n",
    "    else:\n",
    "        Y_new[i] = Y_Y_new_dict[Y[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 26)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               6912      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                384       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               16640     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 26)                6682      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 47,391\n",
      "Trainable params: 47,391\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# Define the dimensions of input and output\n",
    "input_dim = 26  # Dimension of input vector\n",
    "output_dim = 5  # Number of output classes\n",
    "\n",
    "# Encoder\n",
    "input_vector = Input(shape=(input_dim,))\n",
    "encoded = Dense(256, activation='relu')(input_vector)  # Encoding layer with 16 neurons\n",
    "encoded = Dense(64, activation='relu')(encoded)       # Encoding layer with 8 neurons\n",
    "encoded = Dense(5, activation='relu')(encoded)       # Encoding layer with 4 neurons\n",
    "\n",
    "# Decoder\n",
    "decoded = Dense(64, activation='relu')(encoded)       # Decoding layer with 8 neurons\n",
    "decoded = Dense(256, activation='relu')(decoded)      # Decoding layer with 16 neurons\n",
    "decoded = Dense(input_dim, activation='relu')(decoded)  # Decoding layer with original dimension\n",
    "\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(input_vector, decoded)\n",
    "\n",
    "# Define the encoder model\n",
    "encoder = Model(input_vector, encoded)\n",
    "\n",
    "# Compile the autoencoder model\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Print the summary of the autoencoder model\n",
    "autoencoder.summary()\n",
    "\n",
    "# Now you can use the encoder model to obtain encodings of all data points\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_n_val</th>\n",
       "      <th>High_n_val</th>\n",
       "      <th>Low_n_val</th>\n",
       "      <th>Close_n_val</th>\n",
       "      <th>Volume_n_val</th>\n",
       "      <th>SMA_10_val</th>\n",
       "      <th>SMA_20_val</th>\n",
       "      <th>CMO_14_val</th>\n",
       "      <th>High_n-Low_n_val</th>\n",
       "      <th>Open_n-Close_n_val</th>\n",
       "      <th>...</th>\n",
       "      <th>SMA_20-SMA_10_changelen_val</th>\n",
       "      <th>Close_n_slope_3_changelen_val</th>\n",
       "      <th>Close_n_slope_5_changelen_val</th>\n",
       "      <th>Close_n_slope_10_changelen_val</th>\n",
       "      <th>row_num</th>\n",
       "      <th>day</th>\n",
       "      <th>era</th>\n",
       "      <th>target_10_val</th>\n",
       "      <th>target_5_val</th>\n",
       "      <th>data_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1629</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>77</td>\n",
       "      <td>549</td>\n",
       "      <td>17</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53060</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>84</td>\n",
       "      <td>446</td>\n",
       "      <td>16</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148708</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>79</td>\n",
       "      <td>494</td>\n",
       "      <td>15</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31459</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>106</td>\n",
       "      <td>380</td>\n",
       "      <td>7</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176220</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>133</td>\n",
       "      <td>414</td>\n",
       "      <td>15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Open_n_val  High_n_val  Low_n_val  Close_n_val  Volume_n_val  \\\n",
       "1629           0.5         0.5       0.50         0.50           0.0   \n",
       "53060          1.0         0.5       0.75         0.75           0.0   \n",
       "148708         1.0         1.0       1.00         1.00           0.0   \n",
       "31459          0.0         0.0       0.00         0.00           0.0   \n",
       "176220         0.0         0.0       0.00         0.00           0.0   \n",
       "\n",
       "        SMA_10_val  SMA_20_val  CMO_14_val  High_n-Low_n_val  \\\n",
       "1629          0.50        0.75        0.25              0.25   \n",
       "53060         0.75        0.50        0.75              0.25   \n",
       "148708        0.25        0.00        1.00              0.75   \n",
       "31459         0.00        0.00        0.25              0.25   \n",
       "176220        0.75        1.00        0.25              0.00   \n",
       "\n",
       "        Open_n-Close_n_val  ...  SMA_20-SMA_10_changelen_val  \\\n",
       "1629                  0.75  ...                          1.0   \n",
       "53060                 0.75  ...                          0.0   \n",
       "148708                0.25  ...                          0.0   \n",
       "31459                 0.00  ...                          0.5   \n",
       "176220                1.00  ...                          1.0   \n",
       "\n",
       "        Close_n_slope_3_changelen_val  Close_n_slope_5_changelen_val  \\\n",
       "1629                             0.50                           0.50   \n",
       "53060                            0.50                           0.50   \n",
       "148708                           0.50                           0.50   \n",
       "31459                            0.25                           0.25   \n",
       "176220                           0.50                           0.00   \n",
       "\n",
       "        Close_n_slope_10_changelen_val  row_num  day  era  target_10_val  \\\n",
       "1629                              0.25       77  549   17           1.00   \n",
       "53060                             0.50       84  446   16           0.75   \n",
       "148708                            0.50       79  494   15           0.75   \n",
       "31459                             0.50      106  380    7           0.25   \n",
       "176220                            0.00      133  414   15           0.00   \n",
       "\n",
       "        target_5_val   data_type  \n",
       "1629            1.00  validation  \n",
       "53060           0.25  validation  \n",
       "148708          0.25  validation  \n",
       "31459           0.00  validation  \n",
       "176220          0.00  validation  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df.drop(columns=['data_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_clean, X_test_clean, y_train_clean, y_test_clean = train_test_split(X_clean, Y_clean_new, test_size=0.6, random_state=42)\n",
    "X_train, X_test_dn, y_train, y_test_dn = train_test_split(X, Y_new, test_size=0.6, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_train_clean = X_train_clean.astype('float32')\n",
    "X_test_dn = X_test_dn.astype('float32')\n",
    "X_test_clean = X_test_clean.astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Instantiate the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier on your data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = rf_classifier.feature_importances_\n",
    "\n",
    "# Sort feature importances in descending order\n",
    "indices = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.bar(range(X_train.shape[1]), feature_importances[indices], color=\"b\", align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), indices)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25, 24,  8, 20, 10], dtype=int64)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_filtered = X.drop(df.columns[indices[-21:]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(X_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99840, 124800, 149760, 187200)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train),len( X_train_clean),len(X_test_dn),len(X_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "780/780 [==============================] - 6s 7ms/step - loss: 194.0681 - val_loss: 189.9166\n",
      "Epoch 2/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 185.4269 - val_loss: 184.1690\n",
      "Epoch 3/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 176.3280 - val_loss: 170.2428\n",
      "Epoch 4/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 168.2385 - val_loss: 183.8716\n",
      "Epoch 5/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 160.7639 - val_loss: 152.3736\n",
      "Epoch 6/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 154.3518 - val_loss: 147.5452\n",
      "Epoch 7/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 148.6755 - val_loss: 139.8854\n",
      "Epoch 8/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 142.5918 - val_loss: 136.5620\n",
      "Epoch 9/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 139.0198 - val_loss: 134.6144\n",
      "Epoch 10/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 137.2871 - val_loss: 136.9155\n",
      "Epoch 11/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 133.5404 - val_loss: 142.2435\n",
      "Epoch 12/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 132.9525 - val_loss: 128.9102\n",
      "Epoch 13/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 131.2431 - val_loss: 130.0352\n",
      "Epoch 14/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 130.0368 - val_loss: 125.5048\n",
      "Epoch 15/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 129.2525 - val_loss: 125.7841\n",
      "Epoch 16/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 128.6611 - val_loss: 136.4656\n",
      "Epoch 17/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 129.1568 - val_loss: 126.8645\n",
      "Epoch 18/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 129.0572 - val_loss: 124.2152\n",
      "Epoch 19/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 128.0464 - val_loss: 137.8911\n",
      "Epoch 20/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 128.0850 - val_loss: 126.0745\n",
      "Epoch 21/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 126.8836 - val_loss: 123.6563\n",
      "Epoch 22/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 126.7896 - val_loss: 123.7311\n",
      "Epoch 23/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 126.2108 - val_loss: 123.7067\n",
      "Epoch 24/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 125.6396 - val_loss: 125.1572\n",
      "Epoch 25/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 125.3395 - val_loss: 123.6173\n",
      "Epoch 26/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 125.2937 - val_loss: 124.5190\n",
      "Epoch 27/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 124.8867 - val_loss: 123.3422\n",
      "Epoch 28/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 124.1443 - val_loss: 125.2368\n",
      "Epoch 29/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 124.6478 - val_loss: 123.3718\n",
      "Epoch 30/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 124.3715 - val_loss: 124.2125\n",
      "Epoch 31/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 124.0096 - val_loss: 123.3373\n",
      "Epoch 32/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 123.9562 - val_loss: 123.2953\n",
      "Epoch 33/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.8209 - val_loss: 123.7002\n",
      "Epoch 34/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 123.6994 - val_loss: 123.3306\n",
      "Epoch 35/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 123.7349 - val_loss: 123.1304\n",
      "Epoch 36/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.6350 - val_loss: 123.3891\n",
      "Epoch 37/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.7299 - val_loss: 123.2045\n",
      "Epoch 38/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 123.7617 - val_loss: 123.1413\n",
      "Epoch 39/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 123.7716 - val_loss: 123.6141\n",
      "Epoch 40/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 123.7373 - val_loss: 123.3049\n",
      "Epoch 41/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 123.7592 - val_loss: 123.1309\n",
      "Epoch 42/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 123.7544 - val_loss: 124.9039\n",
      "Epoch 43/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 123.7163 - val_loss: 123.2046\n",
      "Epoch 44/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.6569 - val_loss: 123.5746\n",
      "Epoch 45/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.6800 - val_loss: 123.2430\n",
      "Epoch 46/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.7349 - val_loss: 123.4575\n",
      "Epoch 47/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.6561 - val_loss: 123.1595\n",
      "Epoch 48/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.8048 - val_loss: 123.3379\n",
      "Epoch 49/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 123.6806 - val_loss: 123.6000\n",
      "Epoch 50/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.6715 - val_loss: 123.1974\n",
      "Epoch 51/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.7903 - val_loss: 123.4473\n",
      "Epoch 52/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.7357 - val_loss: 123.4567\n",
      "Epoch 53/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.7318 - val_loss: 123.3410\n",
      "Epoch 54/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.7187 - val_loss: 123.3902\n",
      "Epoch 55/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.6384 - val_loss: 123.5874\n",
      "Epoch 56/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.6626 - val_loss: 123.8230\n",
      "Epoch 57/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.6330 - val_loss: 123.6380\n",
      "Epoch 58/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 123.6794 - val_loss: 123.1855\n",
      "Epoch 59/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.6795 - val_loss: 123.3380\n",
      "Epoch 60/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 123.6393 - val_loss: 123.4943\n",
      "Epoch 61/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 123.6111 - val_loss: 123.4417\n",
      "Epoch 62/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 123.6418 - val_loss: 123.3880\n",
      "Epoch 63/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.7421 - val_loss: 123.2675\n",
      "Epoch 64/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.7021 - val_loss: 123.2893\n",
      "Epoch 65/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 123.6646 - val_loss: 123.5329\n",
      "Epoch 66/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 123.7208 - val_loss: 123.2432\n",
      "Epoch 67/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 123.5984 - val_loss: 123.8556\n",
      "Epoch 68/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.6204 - val_loss: 123.1956\n",
      "Epoch 69/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.7045 - val_loss: 123.5422\n",
      "Epoch 70/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 123.7098 - val_loss: 123.4058\n",
      "Epoch 71/100\n",
      "780/780 [==============================] - 6s 7ms/step - loss: 123.6360 - val_loss: 124.0702\n",
      "Epoch 72/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 123.6316 - val_loss: 123.2392\n",
      "Epoch 73/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 123.6553 - val_loss: 123.6690\n",
      "Epoch 74/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.6050 - val_loss: 123.8407\n",
      "Epoch 75/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.6749 - val_loss: 123.4384\n",
      "Epoch 76/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 123.5951 - val_loss: 123.1608\n",
      "Epoch 77/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.5837 - val_loss: 123.3217\n",
      "Epoch 78/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.6788 - val_loss: 123.4519\n",
      "Epoch 79/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 123.6558 - val_loss: 123.3845\n",
      "Epoch 80/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.6426 - val_loss: 123.7340\n",
      "Epoch 81/100\n",
      "780/780 [==============================] - 6s 7ms/step - loss: 123.6525 - val_loss: 123.1728\n",
      "Epoch 82/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.6098 - val_loss: 123.2316\n",
      "Epoch 83/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.6427 - val_loss: 123.3441\n",
      "Epoch 84/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.6398 - val_loss: 123.5992\n",
      "Epoch 85/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 123.6499 - val_loss: 123.1532\n",
      "Epoch 86/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.5935 - val_loss: 123.2131\n",
      "Epoch 87/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.6540 - val_loss: 123.4733\n",
      "Epoch 88/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.5772 - val_loss: 123.1893\n",
      "Epoch 89/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.6255 - val_loss: 123.1691\n",
      "Epoch 90/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.6342 - val_loss: 123.5425\n",
      "Epoch 91/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.6061 - val_loss: 123.6916\n",
      "Epoch 92/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 123.6791 - val_loss: 123.3228\n",
      "Epoch 93/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.5883 - val_loss: 123.1401\n",
      "Epoch 94/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 123.5974 - val_loss: 123.1761\n",
      "Epoch 95/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.6069 - val_loss: 123.6347\n",
      "Epoch 96/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.6783 - val_loss: 123.1469\n",
      "Epoch 97/100\n",
      "780/780 [==============================] - 5s 7ms/step - loss: 123.6569 - val_loss: 123.2022\n",
      "Epoch 98/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.6253 - val_loss: 123.3049\n",
      "Epoch 99/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.6158 - val_loss: 123.3557\n",
      "Epoch 100/100\n",
      "780/780 [==============================] - 5s 6ms/step - loss: 123.6628 - val_loss: 123.7425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x249deded6a0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(X_train, X_train_clean[:99840], epochs=100, batch_size=128,validation_data=(X_test_dn, X_test_clean[:149760]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4680/4680 [==============================] - 6s 1ms/step\n",
      "3120/3120 [==============================] - 4s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "output_test = autoencoder.predict(X_test_dn)\n",
    "output_train = autoencoder.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7800/7800 [==============================] - 12s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "output = autoencoder.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0: 0, 1.0: 0, 2.0: 0, 3.0: 0, 4.0: 0}\n"
     ]
    }
   ],
   "source": [
    "d={}\n",
    "for i in Y_new:\n",
    "    d[i]=0\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from keras.regularizers import l1\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# Generating synthetic sinusoidal dataset with noise\n",
    "# np.random.seed(0)\n",
    "# X = np.linspace(0, 1, 1000)\n",
    "# y = np.sin(2 * np.pi * X) + np.random.normal(0, 0.1, X.shape[0])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Splitting dataset into train and test sets\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "# Assuming X contains features and y contains labels\n",
    "# Perform data preprocessing to reduce noise\n",
    "# For example, use robust scaling to handle outliers\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 2: Feature Engineering\n",
    "# Select the most relevant features that are less affected by noise\n",
    "# Use SelectFromModel with RandomForestClassifier for feature selection\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "selector = SelectFromModel(rf_classifier, threshold='median')\n",
    "X_selected = selector.fit_transform(X_scaled, Y_new)\n",
    "\n",
    "# Step 3: Train-Test Split\n",
    "\n",
    "\n",
    "\n",
    "# Define the MLP model\n",
    "def adap(epoch):\n",
    "    return 0.0001*epoch\n",
    "\n",
    "my_sched = LearningRateScheduler(adap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249600"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity as cos\n",
    "\n",
    "\n",
    "arr = cos(X_scaled[:20000], X_scaled[:20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "arr_norm = normalize(arr, norm=\"l2\")\n",
    "means=[]\n",
    "for i in arr_norm:\n",
    "    x = np.mean(i)\n",
    "    means.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.002655540906971975,\n",
       " 0.001304340035065715,\n",
       " 0.0019325911746702136,\n",
       " 0.0016369000928562044,\n",
       " 0.0010023557361442592,\n",
       " 0.001732928045578689,\n",
       " 0.0005188753674415234,\n",
       " 0.002062246062908745,\n",
       " 0.0023309849471829947,\n",
       " 0.0012355054179489012,\n",
       " -0.0015274413802977996,\n",
       " -0.00038574306102361273,\n",
       " 0.0021018970543007333,\n",
       " -0.0015461885109281487,\n",
       " 0.0014763593217752946,\n",
       " 0.0022161009393182026,\n",
       " 0.0008765699907644789,\n",
       " 0.001363575087631704,\n",
       " 0.0013584171074638,\n",
       " -0.0008381143740390497,\n",
       " 0.0024151936032440814,\n",
       " 0.002201163714438763,\n",
       " 0.001343204992292787,\n",
       " 0.0025442593955800016,\n",
       " 0.00276490383290671,\n",
       " -0.0015354370743453653,\n",
       " -0.0014232584486361872,\n",
       " 0.0015697245296543867,\n",
       " 0.002286517642197622,\n",
       " 0.0020706465642844813,\n",
       " -0.0005543596411729846,\n",
       " -0.0011974180583089484,\n",
       " 0.002398265628420064,\n",
       " 0.00194734408196536,\n",
       " 0.0028111411953969066,\n",
       " 0.0034840582012773103,\n",
       " 0.002112887836442892,\n",
       " 0.0025059013534903594,\n",
       " 0.002095432500798798,\n",
       " 0.0018271345757475575,\n",
       " 0.0019051701550483252,\n",
       " 0.00263715805452732,\n",
       " 0.002161479206852968,\n",
       " 0.0016613053137316783,\n",
       " 0.0021516414877125172,\n",
       " 0.00012632529727151374,\n",
       " 0.002668884178736173,\n",
       " 0.002555361873269667,\n",
       " 0.00225343602141765,\n",
       " 0.001925668743584583,\n",
       " 0.0018116801898776873,\n",
       " 0.0003423008831166776,\n",
       " 0.0005396581802104435,\n",
       " 0.0018170691958917836,\n",
       " 0.00177018643852458,\n",
       " 0.001934139469189492,\n",
       " -0.00028735591769901063,\n",
       " 0.0011347490006566377,\n",
       " 0.0018669690390533428,\n",
       " 0.002678354975736969,\n",
       " 0.0018207688580614248,\n",
       " 0.0032721272330900504,\n",
       " 0.00269673560362036,\n",
       " 0.0006002210321413631,\n",
       " -0.0022175503276864214,\n",
       " 0.002096592380325019,\n",
       " 0.00014248156573787863,\n",
       " 0.002778961395946042,\n",
       " -0.0014441555648522276,\n",
       " -0.002258372978620712,\n",
       " -0.0019809729607128787,\n",
       " 0.0010882453596730997,\n",
       " 0.0027131347680952396,\n",
       " 0.0023551917714525914,\n",
       " 0.0022032804810803745,\n",
       " 0.0010777050514542534,\n",
       " 0.00141294064192852,\n",
       " 0.0018391114961542228,\n",
       " 0.0015324269646862397,\n",
       " 0.0019986515239791836,\n",
       " 0.002892470487352794,\n",
       " -0.0011471971752651307,\n",
       " -0.002482081518748108,\n",
       " 0.0018547375253856985,\n",
       " -0.001203780246493208,\n",
       " 0.0028031123061162126,\n",
       " -0.001001615983466872,\n",
       " -0.0019076429297215992,\n",
       " 0.003331101051964081,\n",
       " 0.0014977225125940483,\n",
       " -0.0008895357589426109,\n",
       " -0.0007440013932934718,\n",
       " -0.0006305396160339621,\n",
       " 0.002261774828704116,\n",
       " 0.00037239420602580635,\n",
       " 0.001680990667560519,\n",
       " 0.0018270092946880243,\n",
       " 0.0031756505474756073,\n",
       " 0.0014926306790930223,\n",
       " 0.001354555700406337,\n",
       " 0.0019180463542383937,\n",
       " -0.0018308116864680496,\n",
       " 0.0018066447795620764,\n",
       " 0.0025778562251093525,\n",
       " 0.002198456145622962,\n",
       " 0.002269553086250251,\n",
       " 0.0020691749827453773,\n",
       " -0.0013013118145464655,\n",
       " -0.0008119521116843349,\n",
       " -0.0022376743162749123,\n",
       " 0.0020894955072803946,\n",
       " 0.0024523071416735044,\n",
       " 0.002343445550128747,\n",
       " -0.0019587039944795443,\n",
       " 0.0035286366680228193,\n",
       " 0.0016263156753510905,\n",
       " 0.0013807900874046749,\n",
       " 0.00011383653275394416,\n",
       " -0.0014038799237220427,\n",
       " -0.001243406024140398,\n",
       " -0.0019189818076119202,\n",
       " 0.001126688659682729,\n",
       " -0.00061071129864515,\n",
       " -0.000834058804734763,\n",
       " 0.001503587291255678,\n",
       " 0.0019917328353042272,\n",
       " 0.0011434466952650463,\n",
       " 0.0015281393046484679,\n",
       " 0.0025082241995910523,\n",
       " -0.0022109797809520297,\n",
       " 0.0014352871346390572,\n",
       " -0.0009231728639356023,\n",
       " 0.0009184120821039034,\n",
       " 0.0023377195526975724,\n",
       " 0.0022603283797406255,\n",
       " 0.0002425574944146526,\n",
       " 0.0020484641786954,\n",
       " -0.0004954140154922286,\n",
       " 0.0012694871319126908,\n",
       " 0.0018305494101781602,\n",
       " 0.0014582762375708357,\n",
       " -0.0021844853271038005,\n",
       " 0.001986518380442196,\n",
       " 0.0030212697769516936,\n",
       " 0.0018239568604058092,\n",
       " 0.001888174090555543,\n",
       " 0.0010538951464110124,\n",
       " 0.0023144555837781816,\n",
       " 0.0013435760249374546,\n",
       " 0.0015589433432873817,\n",
       " 0.0024696279464011626,\n",
       " 0.0021799439736769136,\n",
       " 0.0017292074665043544,\n",
       " 0.0021028361174091673,\n",
       " 0.0017652577806827378,\n",
       " -0.0005555345239546735,\n",
       " 0.0021077835611997987,\n",
       " 0.0018557707982780616,\n",
       " 0.0006616484368745427,\n",
       " 6.8827075259287965e-06,\n",
       " 0.002564345145840686,\n",
       " 0.002480403457060181,\n",
       " 0.00038027076884289526,\n",
       " 0.0018835613882259834,\n",
       " 0.001637377306008096,\n",
       " 0.0016286421379643097,\n",
       " 0.0015811326475789972,\n",
       " 0.0016461163931192236,\n",
       " 0.001880386959020058,\n",
       " -0.0023699595755893687,\n",
       " 0.002217350694525407,\n",
       " -0.0011566135757768426,\n",
       " -0.001400608814933572,\n",
       " -0.0002206663971959037,\n",
       " 0.001591515604634279,\n",
       " 0.0031143862312552058,\n",
       " 0.0014856784044078508,\n",
       " 0.0012281306469062111,\n",
       " 0.0010934417983621614,\n",
       " 0.0012144585155264968,\n",
       " 0.0017568274442536854,\n",
       " 0.002386964700568329,\n",
       " 0.001587305406531599,\n",
       " 0.0019712938152891667,\n",
       " 0.00011499569854903175,\n",
       " -0.00012616906364720993,\n",
       " -0.0012126048932009368,\n",
       " -0.001537001136717199,\n",
       " 0.0013380442689138652,\n",
       " 0.0019789491969227613,\n",
       " 0.0002244454751437794,\n",
       " 0.0012727493926676197,\n",
       " 0.0008709751662949618,\n",
       " 0.0021237599005045093,\n",
       " -0.0020382051182481413,\n",
       " -0.0009607398450542599,\n",
       " 7.038913046747223e-05,\n",
       " 0.0021966412768143037,\n",
       " 0.002420350987327898,\n",
       " -0.001636322550519196,\n",
       " 0.0010252757426830583,\n",
       " -0.002040272408684785,\n",
       " -0.0002012098888100684,\n",
       " 0.002686821444684329,\n",
       " 0.0015652470826053087,\n",
       " 0.0020737480268830373,\n",
       " 0.00045378550281687386,\n",
       " -0.0019473252817910525,\n",
       " 0.000862256507761774,\n",
       " 0.0008903693761740509,\n",
       " 0.0029224441428510415,\n",
       " -0.000645516375707731,\n",
       " 0.002098513852545229,\n",
       " 0.001857177061033323,\n",
       " -0.001609491522025627,\n",
       " -0.00023758930999259712,\n",
       " 0.0027382274443227017,\n",
       " 0.000855491884805212,\n",
       " 0.001548431913865388,\n",
       " -0.002188810519755881,\n",
       " 0.002666836555783424,\n",
       " 0.0030921058350937363,\n",
       " 0.0018954487750366888,\n",
       " 0.0024569969463166296,\n",
       " -0.0009260216206803334,\n",
       " 0.002289849448120862,\n",
       " 8.244450593646237e-05,\n",
       " 0.0024107951133250515,\n",
       " 0.0025949262024425684,\n",
       " 0.0002878880142779972,\n",
       " -0.000602117485802768,\n",
       " 0.0016800619329815085,\n",
       " 0.001541735217308722,\n",
       " 0.002283837610581158,\n",
       " -0.001016128873596488,\n",
       " 0.002117712057262689,\n",
       " 0.0013632747372314096,\n",
       " 0.002635212720655176,\n",
       " 0.00016799455069938015,\n",
       " -9.96370765104011e-05,\n",
       " 0.0034249237040994197,\n",
       " 0.0010187094882608497,\n",
       " 0.002296501349440055,\n",
       " -0.0021374700871721027,\n",
       " 0.002363422762801608,\n",
       " 0.0013503102762378472,\n",
       " -0.0018788551553782355,\n",
       " 0.0012265732964222963,\n",
       " 0.0018895319864274832,\n",
       " -0.0006268909302005522,\n",
       " 0.0003472227525600777,\n",
       " -0.0016460076718995686,\n",
       " 0.002388416918029508,\n",
       " 0.0025989446550776433,\n",
       " 7.899150374221998e-05,\n",
       " 0.003616515174532818,\n",
       " 0.001976433693341628,\n",
       " 0.0019478611397349731,\n",
       " 0.0007574870548414964,\n",
       " 0.0022360161692684183,\n",
       " 0.0025592936880344907,\n",
       " 0.0008720710419797762,\n",
       " -0.0013120413512575364,\n",
       " 0.00145388570630192,\n",
       " 0.001330123823471734,\n",
       " 0.0009527917383599542,\n",
       " 0.002817558712072657,\n",
       " 0.0018583843058929447,\n",
       " -0.0018437733404249655,\n",
       " 0.001659731629813834,\n",
       " 0.0019714588566640484,\n",
       " 0.00175026117110941,\n",
       " 0.002281273183428921,\n",
       " 0.002210411009128304,\n",
       " -0.0021350382867639877,\n",
       " 0.0021423648336963653,\n",
       " 8.380801848097266e-05,\n",
       " 0.0008167025794076793,\n",
       " -0.00090392202863249,\n",
       " -0.0020051407679955438,\n",
       " -0.001985482220815879,\n",
       " 0.0012994820147531033,\n",
       " 0.002211797648416626,\n",
       " 0.0016574110985067346,\n",
       " 0.002623901515546212,\n",
       " 0.0018783589865769567,\n",
       " -0.0006219840807022565,\n",
       " 0.0003339316691044068,\n",
       " 0.00044062956971789786,\n",
       " 0.0022162290180169917,\n",
       " 0.001020701517133188,\n",
       " 0.002415375226674319,\n",
       " -0.0016228240882171817,\n",
       " 0.002306962126029711,\n",
       " 0.0018876891349350665,\n",
       " 0.001010695942091956,\n",
       " 0.0025836462564408384,\n",
       " 0.0015057807302316267,\n",
       " 0.0005806143683162835,\n",
       " 0.0022712786409345544,\n",
       " 0.0019328778209686047,\n",
       " 6.509748585408451e-05,\n",
       " 7.663739853571015e-05,\n",
       " 0.0023948523040630307,\n",
       " 0.002030155154295272,\n",
       " -0.0022802363264795298,\n",
       " 0.0029562952301943005,\n",
       " -0.0010606459898284406,\n",
       " 0.0019170690102498086,\n",
       " 0.0031411917027756916,\n",
       " -0.0018557241337295153,\n",
       " -0.00011653209728526597,\n",
       " 0.0022254230916148123,\n",
       " 0.0018227985282283058,\n",
       " 0.0019094647020986909,\n",
       " -0.001105012272658487,\n",
       " 0.0015453220729647035,\n",
       " -0.0018861724575797644,\n",
       " 0.002083468686397653,\n",
       " -0.0020382908635619506,\n",
       " 0.0008219087072228642,\n",
       " -0.001378859479989955,\n",
       " 0.0006064210050547666,\n",
       " 0.00019018900889332721,\n",
       " 0.0005314006210329815,\n",
       " -0.00011636324731899445,\n",
       " -0.0020995399370291835,\n",
       " -0.001798879794742416,\n",
       " -0.0008410482945227926,\n",
       " 4.783753353952808e-05,\n",
       " -0.0019827230758141844,\n",
       " 3.644261055819464e-05,\n",
       " 0.0023563451164618733,\n",
       " 0.0027757742958468023,\n",
       " 0.001941846170922908,\n",
       " -0.0003918529414566055,\n",
       " 0.002764376663934027,\n",
       " 0.002115922175614217,\n",
       " 0.003028164716930347,\n",
       " 0.0023709997907638593,\n",
       " 0.0022174729906097477,\n",
       " 0.0017576551962377913,\n",
       " -0.0005735621636416314,\n",
       " 0.0014952599802171222,\n",
       " -0.002007494242328364,\n",
       " -0.0009602303102183324,\n",
       " 0.0027016084584122685,\n",
       " 0.00248894125064209,\n",
       " -0.002186640368093195,\n",
       " 0.002747627981204914,\n",
       " 0.0021512424134470273,\n",
       " 0.0028676300372104044,\n",
       " 0.0006885858982850061,\n",
       " 0.0014901163796625365,\n",
       " 0.001813973239743336,\n",
       " 0.0017498482243238377,\n",
       " 0.0021131663772277587,\n",
       " 0.00022655971589168828,\n",
       " 0.0024470273787597975,\n",
       " 0.00043547518982995086,\n",
       " 0.0034103674561755384,\n",
       " 0.0023586251467953797,\n",
       " 0.0012946531244018066,\n",
       " 0.0020159063415463563,\n",
       " -3.886230245117658e-05,\n",
       " 0.002036896329705317,\n",
       " 0.002953856402160227,\n",
       " 0.0016517251437283133,\n",
       " 0.0022074302124499632,\n",
       " 0.0020537313796260607,\n",
       " 0.002277525476787422,\n",
       " -0.0011390003249613543,\n",
       " 0.001466828242073582,\n",
       " 0.00043316627217028785,\n",
       " 0.0027360492437679676,\n",
       " 0.0004642480745761791,\n",
       " 0.00293968958062537,\n",
       " 0.0019262289991802423,\n",
       " 0.0019318819865008,\n",
       " -0.001532958822364728,\n",
       " 0.0011529090976526567,\n",
       " 0.002173656369450155,\n",
       " -0.001793594200052841,\n",
       " 0.0023218576809373587,\n",
       " 0.001355785845839667,\n",
       " 0.0022398602837585715,\n",
       " -0.0023700406165893238,\n",
       " -0.0006493856425995337,\n",
       " 0.002459706811587096,\n",
       " -0.00104049937824952,\n",
       " -0.0011396707028763456,\n",
       " 0.0023017115398144086,\n",
       " 0.0016627885334986017,\n",
       " 0.0020219831893918522,\n",
       " 0.0024636678762341704,\n",
       " 0.002459338333720712,\n",
       " -0.0006882032120306325,\n",
       " 0.00029672631807938036,\n",
       " 0.003245955466928919,\n",
       " 0.002236106415507631,\n",
       " 2.622515479978668e-05,\n",
       " 0.002070614074161451,\n",
       " -0.0005823285384015892,\n",
       " 0.0019244924573253647,\n",
       " 0.0018817567511595224,\n",
       " 0.00022243125345772064,\n",
       " 0.003052614009656044,\n",
       " 0.0014967680686885246,\n",
       " -0.0006289033168616971,\n",
       " -0.002646272902564274,\n",
       " 0.0026875525776107224,\n",
       " 0.0016652955653314273,\n",
       " 0.0026963710652134877,\n",
       " -0.0008966704840210525,\n",
       " 0.0017027532198358522,\n",
       " 0.002238006798308211,\n",
       " 0.001022997234751804,\n",
       " 0.0019583880068111048,\n",
       " 0.00019813316818468753,\n",
       " 0.0016537035007971063,\n",
       " 0.001711441235390302,\n",
       " 0.001830199915505717,\n",
       " 0.0016454076322686292,\n",
       " 0.001967981136191332,\n",
       " 0.0015114707466255888,\n",
       " 0.001362577461903139,\n",
       " 0.0019673343755334644,\n",
       " 0.002838888834975322,\n",
       " -0.0020683743225811324,\n",
       " 0.003092026641632058,\n",
       " 0.0010651068249213982,\n",
       " -0.0004927846001318607,\n",
       " 0.0008234435816233406,\n",
       " 0.0019553716999997947,\n",
       " 6.687190426238566e-05,\n",
       " 0.0024545385195318503,\n",
       " 0.0017374350559309227,\n",
       " 0.0010685562989610367,\n",
       " 0.0001658079171722662,\n",
       " 0.00325760003131631,\n",
       " -0.0008382162935202798,\n",
       " 0.002613410363674013,\n",
       " -0.002094126797738045,\n",
       " -0.0021468319164006417,\n",
       " 0.002054123919700949,\n",
       " 0.0019111761902256762,\n",
       " 0.0020248585581090573,\n",
       " -6.548875439675881e-05,\n",
       " -0.000639026024021424,\n",
       " -0.001299517814739847,\n",
       " 0.001623364323826242,\n",
       " -0.001017874526646564,\n",
       " 0.0018344606501878758,\n",
       " 0.000520369364293509,\n",
       " -0.0003361465195195417,\n",
       " 0.0030000248576207205,\n",
       " 0.0008932392979611839,\n",
       " 0.0007584759976085366,\n",
       " 0.0016858192069178547,\n",
       " 0.0023535419960337856,\n",
       " -0.002294351601586756,\n",
       " 0.0005960132596540059,\n",
       " 0.0015761394502223486,\n",
       " 0.0022823770733751587,\n",
       " 0.0010537910233270136,\n",
       " 0.00294295823557395,\n",
       " 0.0015450551080931011,\n",
       " -0.0010674283360316739,\n",
       " 0.0021439924325597974,\n",
       " -0.0006517299542433227,\n",
       " 0.0007158832336582233,\n",
       " 0.0027663814170933473,\n",
       " -0.00044622531865079153,\n",
       " 0.00026258675447075305,\n",
       " 0.002118563748960901,\n",
       " 0.0024439455404528273,\n",
       " 0.0011220994804154376,\n",
       " -0.001891527446735467,\n",
       " -0.001244867409566677,\n",
       " 0.0014156505008282305,\n",
       " 1.9373362651160904e-05,\n",
       " 0.0023587795943877834,\n",
       " -0.002109221920105995,\n",
       " 0.0019005524521653455,\n",
       " -0.0014155133863037463,\n",
       " 0.0011299115053053474,\n",
       " -0.0012822967261505855,\n",
       " 0.001839693432395758,\n",
       " -0.000538065164327243,\n",
       " 0.002958461220139492,\n",
       " 0.0017362305754577137,\n",
       " -0.0014217069273382624,\n",
       " 0.0017800210530862781,\n",
       " -0.001994561983587877,\n",
       " -0.0005397771799968033,\n",
       " 0.0036551492942992264,\n",
       " 0.0005398100151269487,\n",
       " 0.0017940921393368818,\n",
       " 0.002129520315347527,\n",
       " 0.0022581498869767517,\n",
       " 0.001589614509186008,\n",
       " 0.002094624644409734,\n",
       " -0.0012535941299180496,\n",
       " -0.0012007483059171041,\n",
       " 0.0019427809996297796,\n",
       " -0.0018854636000341565,\n",
       " 0.002268259113567884,\n",
       " -0.0006898126793582589,\n",
       " 0.0022597929178284244,\n",
       " 0.0028840814554058846,\n",
       " -0.0012398931816480709,\n",
       " 0.0004622473323497469,\n",
       " 0.0009054126496478159,\n",
       " 0.0025456009996171714,\n",
       " 0.00199884822778506,\n",
       " 0.0014988218793676508,\n",
       " 0.002409594943291595,\n",
       " 0.0003436956742103167,\n",
       " -0.0010947064951294702,\n",
       " 0.0010310649403623125,\n",
       " 0.0009577143220496792,\n",
       " 0.0017395469031548135,\n",
       " 0.001391603869237639,\n",
       " 0.001643090639495854,\n",
       " -0.0018395634011435533,\n",
       " 0.002147919627215439,\n",
       " -0.0011156632061191496,\n",
       " 0.0029111807613949312,\n",
       " 0.0018548515096013251,\n",
       " 0.001681729741126199,\n",
       " -0.001973269486884378,\n",
       " 0.0027054452131912215,\n",
       " 9.587205979185642e-05,\n",
       " -0.0021313586697896134,\n",
       " 0.0029881700177807166,\n",
       " -0.0006056067620075324,\n",
       " -0.0005509328841802618,\n",
       " 0.0021099766327012906,\n",
       " -0.0013791191000500603,\n",
       " 0.0005804942895988183,\n",
       " 0.001325142559513515,\n",
       " 0.0021401324668394637,\n",
       " 0.002044878452120595,\n",
       " -0.0019722646979837906,\n",
       " 0.002549911700066901,\n",
       " 0.0020286018963022096,\n",
       " 0.0032801512895765396,\n",
       " 0.0021511610317903333,\n",
       " 0.002664538835257172,\n",
       " 0.0019286288927936738,\n",
       " -0.00028788632745411146,\n",
       " 0.0017902829076400998,\n",
       " -4.791474632880143e-05,\n",
       " 0.00226034384428547,\n",
       " -0.001840006780751953,\n",
       " 0.0021356541978313237,\n",
       " 0.0017245532946664646,\n",
       " -0.0005205139950744008,\n",
       " 0.0008982148028847181,\n",
       " 0.001590243194265239,\n",
       " 0.001939672111849918,\n",
       " 0.0003354637686973975,\n",
       " 0.0019321991938182198,\n",
       " 0.0013891165659830323,\n",
       " 0.0013510599914679992,\n",
       " 0.001970334709064459,\n",
       " -0.0028027045017368353,\n",
       " 0.0017151177098762855,\n",
       " 0.0012293529466522817,\n",
       " 0.00010503059439149046,\n",
       " 0.0019859396270485584,\n",
       " -0.0005191899556081953,\n",
       " -1.8586460326931387e-05,\n",
       " -0.0004090658997525785,\n",
       " -0.0005018473627104801,\n",
       " -0.0018330230656985278,\n",
       " 0.002253823824321877,\n",
       " 0.0020498392171748557,\n",
       " 0.0023970417805645816,\n",
       " 0.0016834806281223393,\n",
       " -0.0014189102941783568,\n",
       " 0.002560735017194115,\n",
       " 0.0001333491004355552,\n",
       " 0.001873117858384794,\n",
       " -0.001800331886144309,\n",
       " 0.0004397014565354155,\n",
       " -0.0019240027858664366,\n",
       " 0.00013138079950063787,\n",
       " 0.0025161595653135885,\n",
       " 0.00163766985082963,\n",
       " 0.0020874424039926754,\n",
       " 0.0012831330983933988,\n",
       " 0.0021928187526807108,\n",
       " 0.0007911715530048471,\n",
       " -0.0019753017091919327,\n",
       " 0.0018288791080868546,\n",
       " -0.0009732704105347715,\n",
       " 0.0007196875543511639,\n",
       " -0.0017276850317720618,\n",
       " 0.0018556452271068938,\n",
       " 0.0020559661257721308,\n",
       " 0.002261821509581802,\n",
       " 0.0015431654350008255,\n",
       " -0.0022044111860465387,\n",
       " 0.0023343886954983063,\n",
       " -0.0012500827792589822,\n",
       " 0.0007642748363882432,\n",
       " 0.001170173042060587,\n",
       " 0.0019442989523417325,\n",
       " 0.003770620468925178,\n",
       " 0.0015208836224810676,\n",
       " 0.0003544933985911879,\n",
       " 0.0017090728084293825,\n",
       " 0.0022073277851139394,\n",
       " 0.0012945891906074893,\n",
       " 0.0016098854927993228,\n",
       " 0.002661323958932057,\n",
       " 0.0021709910865032878,\n",
       " 0.0025546768916969212,\n",
       " 0.0019114251899336947,\n",
       " 0.002833554329867815,\n",
       " 0.001925466251459568,\n",
       " 0.0014916818547790365,\n",
       " 0.0022677660095451425,\n",
       " -0.0013870538808389325,\n",
       " -0.0007232997213063575,\n",
       " -0.0012323958344587626,\n",
       " -0.001946016587799054,\n",
       " -0.0019394954121864554,\n",
       " 0.002705359574250267,\n",
       " 0.002232762381497492,\n",
       " -0.00132311024865016,\n",
       " 0.0020384917164106664,\n",
       " 0.0004475628262818222,\n",
       " -0.000699362379694927,\n",
       " 0.0021132824499219397,\n",
       " 0.002097495533905767,\n",
       " -0.0014383514836990836,\n",
       " 0.0007877617167026964,\n",
       " 0.0021511557218864895,\n",
       " 0.0031095601492235054,\n",
       " 0.0010624542243658879,\n",
       " -0.0026627158891528524,\n",
       " 0.00027766674149755214,\n",
       " -0.0009333006471740689,\n",
       " 0.001501740097446276,\n",
       " 0.0025034244119923913,\n",
       " 0.0027665497570143584,\n",
       " 0.0022622548723183196,\n",
       " -0.00011042158737753626,\n",
       " 0.0009596949958161276,\n",
       " 0.003279360458875069,\n",
       " 0.0031566411890356348,\n",
       " 0.0019208785739982914,\n",
       " 0.001972814436353653,\n",
       " 0.0025614283396567746,\n",
       " -0.0006544622184089788,\n",
       " 0.001390745881339799,\n",
       " 0.0023133836416931097,\n",
       " 0.0022818601973903186,\n",
       " -0.0014915127931370423,\n",
       " 0.002712198309282717,\n",
       " -0.0013349092171470699,\n",
       " -6.038298478869287e-05,\n",
       " 0.0001994057822441624,\n",
       " -0.001495812253843773,\n",
       " 0.0024579008798291556,\n",
       " 0.002338136744008637,\n",
       " -0.001094056776190777,\n",
       " 0.00010041238337786335,\n",
       " -0.0018685839886980102,\n",
       " -0.0013444867252163499,\n",
       " -0.0025388062759586724,\n",
       " 0.002202273440765765,\n",
       " 0.0006438648342453995,\n",
       " -0.002202141562215241,\n",
       " -0.0018584242541220734,\n",
       " 0.0023353744616437582,\n",
       " -0.0017110570088785075,\n",
       " 0.0019703134298890953,\n",
       " 8.268197458096686e-05,\n",
       " 0.0021620483749612072,\n",
       " 0.002277316339811467,\n",
       " 0.002768657290887989,\n",
       " 0.0028492664475023737,\n",
       " 0.0015089858389548507,\n",
       " -0.0013133345702023402,\n",
       " 0.0020228254118937203,\n",
       " -0.0005769901834112028,\n",
       " 0.00222163295561011,\n",
       " -5.5062702878107004e-05,\n",
       " 0.0017367175449332568,\n",
       " 0.00248404010268501,\n",
       " -0.001957708553772824,\n",
       " 0.0020449366883368908,\n",
       " 0.0019224862444571567,\n",
       " 0.0021093068860387407,\n",
       " 0.0010522305666284177,\n",
       " 0.0016739014586945327,\n",
       " 0.001355555288678319,\n",
       " 0.0022673521027015984,\n",
       " 0.00047295980523742574,\n",
       " 0.0024276157004391318,\n",
       " -0.0006315124813312524,\n",
       " 0.0025101966251370434,\n",
       " 0.0026721270298548185,\n",
       " -0.0011949299631629686,\n",
       " -0.002238292399608442,\n",
       " 0.0028165230036390977,\n",
       " 0.0018752985302933403,\n",
       " 0.0003568944006962374,\n",
       " 0.0034144598872050515,\n",
       " 0.0028423473303842554,\n",
       " -0.0013816559720813426,\n",
       " 0.002527080779891449,\n",
       " 0.00035567276868173693,\n",
       " 0.000313123472363291,\n",
       " 0.0017476750668695609,\n",
       " 0.0022176247540561713,\n",
       " 0.002054430571167841,\n",
       " 0.0007648339613412567,\n",
       " 0.0019704505912966166,\n",
       " 0.003127335485664905,\n",
       " -0.0019297929298895422,\n",
       " 0.002183541993430319,\n",
       " 0.0012289228473776653,\n",
       " 0.001469330016847783,\n",
       " 0.0029689955164426724,\n",
       " 0.0024733333698662603,\n",
       " 0.00011922048591119328,\n",
       " 0.0022490696791358992,\n",
       " 0.00286658018016464,\n",
       " -0.001033443950563102,\n",
       " 0.0017304489911787748,\n",
       " 0.0025925568378936384,\n",
       " 0.0002233752228143493,\n",
       " 6.925443731316077e-05,\n",
       " -0.0009434032747317439,\n",
       " -0.0005972944707247778,\n",
       " 0.0021251155259377325,\n",
       " 0.0014254844150632853,\n",
       " 0.0022501973246474567,\n",
       " -0.002220889881848449,\n",
       " 0.0027742837882943866,\n",
       " 0.0019409954496679652,\n",
       " 0.002543107086285083,\n",
       " 0.0011276984829614591,\n",
       " 0.0009470336738674138,\n",
       " 0.0016780303240267053,\n",
       " 0.0023711904724118466,\n",
       " 0.0005525103379343412,\n",
       " 0.002420060313326217,\n",
       " 0.002612788518757456,\n",
       " 0.0013972777032236685,\n",
       " 0.0017284344773702443,\n",
       " -0.0020686530028191136,\n",
       " 0.0028785720373422008,\n",
       " 0.0006795464117900187,\n",
       " -0.0009658744504767298,\n",
       " 0.0023429410577822116,\n",
       " 0.0013577435286957478,\n",
       " -0.0005782247388522553,\n",
       " 0.0021334893034985415,\n",
       " -0.00011575773801416554,\n",
       " 0.0008480160238835992,\n",
       " 0.00017187355599221582,\n",
       " 0.002922099557655542,\n",
       " -0.0015324908745064426,\n",
       " 0.0006744157608122521,\n",
       " 0.0017643843142066769,\n",
       " 0.00047785678712900756,\n",
       " 0.002340002907884402,\n",
       " 0.0013205709059473374,\n",
       " 0.002821720091771018,\n",
       " 0.0010043974062236367,\n",
       " 0.0022035597136509503,\n",
       " 0.000755505124499317,\n",
       " 4.036087625697669e-05,\n",
       " 0.0001405604715450195,\n",
       " -0.0019803740218089496,\n",
       " 0.0009092756578634859,\n",
       " 0.0009863260813375544,\n",
       " -0.0015599054275714564,\n",
       " -0.0013165283501832229,\n",
       " 0.002667717757558877,\n",
       " 0.0025654706066962556,\n",
       " 1.7133560690932465e-05,\n",
       " 0.002437466942053904,\n",
       " -0.0015775826328525066,\n",
       " 0.0015565792710304368,\n",
       " 0.0024606274120996074,\n",
       " 0.0030044949900560015,\n",
       " 0.0022034239952140665,\n",
       " -0.00017796060738621135,\n",
       " 0.0022763046206026017,\n",
       " -0.002801533440286741,\n",
       " 0.002521251841003315,\n",
       " 0.0027207587629600348,\n",
       " 0.0027746723445997157,\n",
       " 0.0015360944094239678,\n",
       " 0.001573133427507418,\n",
       " 0.000660135051503564,\n",
       " 0.00215634180907371,\n",
       " 0.0014040989961148181,\n",
       " 0.002403841563948065,\n",
       " -0.0021857125751081033,\n",
       " 0.0023762614861966708,\n",
       " 0.002097622298698205,\n",
       " 0.003040283558026687,\n",
       " 0.0017663362445198153,\n",
       " 0.0018918352872994571,\n",
       " -0.002123349565076607,\n",
       " 0.0018666473705257268,\n",
       " 0.0025734836623431934,\n",
       " 0.0022953574605206464,\n",
       " 0.0019064735996977287,\n",
       " 0.0020195652504261686,\n",
       " 0.003093810105068888,\n",
       " 0.002973017516146456,\n",
       " 0.00013212521975868935,\n",
       " 0.002580509203771905,\n",
       " -0.0011195308717443556,\n",
       " 0.0026452631785730306,\n",
       " 0.0017059854313379017,\n",
       " 0.0009501880632374463,\n",
       " 0.001860694080876052,\n",
       " 0.0022211001919440233,\n",
       " 0.0030031311168391968,\n",
       " 0.0017685822221861045,\n",
       " 0.002083585069206978,\n",
       " 0.0019998342480694663,\n",
       " 0.001981725060264758,\n",
       " -3.1508192526075584e-05,\n",
       " 0.0010249305454774584,\n",
       " 0.002624384339032258,\n",
       " -0.002149310293889269,\n",
       " 0.0007840420808967565,\n",
       " 0.0010471685750503175,\n",
       " -0.0021662178146640426,\n",
       " 0.0021378163826869002,\n",
       " 0.0021184946037613823,\n",
       " -0.0019878121361795163,\n",
       " 0.002036120675986219,\n",
       " -2.51402772328565e-05,\n",
       " 0.002572094841165746,\n",
       " 0.0020054602277785835,\n",
       " -0.0011234602134005047,\n",
       " -0.0022193138994971145,\n",
       " -0.0006238111792186155,\n",
       " 0.002047461071309464,\n",
       " 0.0009748564883285962,\n",
       " 0.001773802675730922,\n",
       " 0.0024292700168503188,\n",
       " -0.000847625748409755,\n",
       " 0.0026548070198737653,\n",
       " 0.0025557039589059554,\n",
       " 0.002194459151125616,\n",
       " -0.0018459065890016199,\n",
       " 0.003031241651233068,\n",
       " 6.12455025859801e-05,\n",
       " 0.0012998700632410373,\n",
       " 0.0018770740212192202,\n",
       " -0.001656040375803975,\n",
       " 0.0006651020296767397,\n",
       " 0.0028508820295554877,\n",
       " -0.0002626678419281634,\n",
       " -0.002927124006930994,\n",
       " -0.0008125162675208017,\n",
       " 0.002301235226919933,\n",
       " -0.0011560872150740695,\n",
       " -8.215175837320376e-05,\n",
       " 0.002549736523881416,\n",
       " 0.002935822792029836,\n",
       " 0.002795652539907318,\n",
       " -0.001438148312393136,\n",
       " 0.0002895260499885649,\n",
       " -0.00034161007207247386,\n",
       " -0.00035048207892308974,\n",
       " 0.0018186350817503765,\n",
       " 0.0016280248380150016,\n",
       " 0.001433630406924741,\n",
       " 0.003506704551273888,\n",
       " 0.0029291744103442812,\n",
       " 0.0014094669387854082,\n",
       " -0.0019643381361859674,\n",
       " 0.0028655481722928444,\n",
       " 0.0019294268263218086,\n",
       " 0.001372548898147344,\n",
       " -0.00024583792058704236,\n",
       " 7.615157547801775e-05,\n",
       " 0.0028995265290092417,\n",
       " 0.0026986714205593614,\n",
       " -0.0007460070965477648,\n",
       " 0.0022480876833169488,\n",
       " 0.0014646775031209364,\n",
       " 0.002059701734944208,\n",
       " -0.0026558083781025405,\n",
       " 0.0025947837789820484,\n",
       " 0.0019163396942883239,\n",
       " 0.00165161019026345,\n",
       " 4.819239266072841e-05,\n",
       " 0.0013676731454199819,\n",
       " 0.00021852756230897428,\n",
       " 0.002251734454877147,\n",
       " 0.002204552276060104,\n",
       " -0.001330546767067812,\n",
       " 0.001842425957477969,\n",
       " 0.0023501740130612856,\n",
       " 0.002744246560109899,\n",
       " 0.0015317669130307239,\n",
       " 0.0021031189495063682,\n",
       " -0.0019045611828608852,\n",
       " 0.0009466046616231089,\n",
       " -0.0010374038824599121,\n",
       " -0.0015227429291633503,\n",
       " 0.001259806911722289,\n",
       " 0.00255283899406632,\n",
       " 0.0014968356430065412,\n",
       " 0.000630656728816553,\n",
       " 0.00015417174444716037,\n",
       " 0.0006257292092508105,\n",
       " 0.0017023015848544824,\n",
       " 0.00281412212154595,\n",
       " 3.33693271884008e-05,\n",
       " 0.0033247391921975337,\n",
       " 0.002215262444787678,\n",
       " -0.0015045068126796704,\n",
       " -0.0015386330749001709,\n",
       " 0.001434810498995627,\n",
       " 0.0026559760547618825,\n",
       " 0.002417252237986043,\n",
       " 0.0023923310984925767,\n",
       " 0.001817935825366212,\n",
       " 0.002420695649368738,\n",
       " 0.0009810136245062477,\n",
       " 0.0031666205818339475,\n",
       " 0.002361616116247631,\n",
       " 0.0009713975266998084,\n",
       " 0.0029991853594399627,\n",
       " 0.0018784872634867496,\n",
       " -0.0018917273776189944,\n",
       " 0.002270395710270578,\n",
       " -0.0011317814819476228,\n",
       " 0.002190021232424631,\n",
       " 0.0007894837095513327,\n",
       " 0.0006523257784291597,\n",
       " 0.0016466888424794153,\n",
       " 8.14075262378403e-05,\n",
       " 0.0018559633949935318,\n",
       " -0.0009286691787426743,\n",
       " -0.0011073720608049795,\n",
       " 0.002901994062910115,\n",
       " 0.00025480310110119676,\n",
       " 0.00046064018844376857,\n",
       " 0.000289824573357871,\n",
       " -0.0008395933910040518,\n",
       " -0.0014335205219609386,\n",
       " 0.001456040608433317,\n",
       " 0.0023507283962860624,\n",
       " -0.002087700384131983,\n",
       " 0.002699561602450907,\n",
       " 0.0020872902970131787,\n",
       " -0.0007052241189608762,\n",
       " -0.00021426660848140736,\n",
       " 0.002964602974657749,\n",
       " 0.003025185088390726,\n",
       " -0.0016410275438971818,\n",
       " 0.0021915979517192655,\n",
       " 0.0019732819552821025,\n",
       " 0.002186122587812937,\n",
       " 0.0003053189264758979,\n",
       " 0.0016150476662183553,\n",
       " -0.0020175250198401187,\n",
       " 0.0023050339341707422,\n",
       " -0.002290455102326382,\n",
       " 0.0023364077852632645,\n",
       " -0.001106121663629418,\n",
       " 0.002646919482723836,\n",
       " -0.001981687967659726,\n",
       " -0.0018476474176338119,\n",
       " -0.00150933166694883,\n",
       " 0.0007414951828320005,\n",
       " -0.002259923169875476,\n",
       " -0.0009207969183804245,\n",
       " -0.0019253070824039526,\n",
       " 0.0022211340148295786,\n",
       " 0.0019777359641961895,\n",
       " 0.0022027031782265423,\n",
       " 0.002506427760004072,\n",
       " -0.001295245106052748,\n",
       " 0.00014130597395761862,\n",
       " -0.0011952028319801267,\n",
       " 0.0037476113710388434,\n",
       " 0.003605182779943923,\n",
       " 0.0022986415674033167,\n",
       " -0.0005737710001826108,\n",
       " -0.000554579313116935,\n",
       " -0.0005872363493887633,\n",
       " 0.0017712092863941095,\n",
       " -0.0022189491654592735,\n",
       " ...]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "new_labels=[]\n",
    "for i in range(0,20000):\n",
    "    lab = []\n",
    "    for j in range(0,20000):\n",
    "        # print(i,j)\n",
    "        if arr[i][j] > means[i]:\n",
    "            lab.append(Y[j])\n",
    "    count = Counter(lab)\n",
    "    most_common_element = max(count, key=count.get)    \n",
    "    new_labels.append(most_common_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(output, Y_new, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1798/1798 [==============================] - 10s 5ms/step - loss: 1.6437 - accuracy: 0.2480 - val_loss: 1.7003 - val_accuracy: 0.2440\n",
      "Epoch 2/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4571 - accuracy: 0.2496 - val_loss: 1.4346 - val_accuracy: 0.2440\n",
      "Epoch 3/500\n",
      "1798/1798 [==============================] - 8s 5ms/step - loss: 1.4382 - accuracy: 0.2509 - val_loss: 1.4332 - val_accuracy: 0.2618\n",
      "Epoch 4/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4358 - accuracy: 0.2508 - val_loss: 1.4316 - val_accuracy: 0.2618\n",
      "Epoch 5/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4351 - accuracy: 0.2533 - val_loss: 1.4322 - val_accuracy: 0.2618\n",
      "Epoch 6/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4354 - accuracy: 0.2530 - val_loss: 1.4321 - val_accuracy: 0.2618\n",
      "Epoch 7/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4347 - accuracy: 0.2544 - val_loss: 1.4321 - val_accuracy: 0.2417\n",
      "Epoch 8/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4341 - accuracy: 0.2560 - val_loss: 1.4316 - val_accuracy: 0.2618\n",
      "Epoch 9/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4341 - accuracy: 0.2566 - val_loss: 1.4312 - val_accuracy: 0.2618\n",
      "Epoch 10/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4337 - accuracy: 0.2577 - val_loss: 1.4312 - val_accuracy: 0.2618\n",
      "Epoch 11/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4336 - accuracy: 0.2577 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 12/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4335 - accuracy: 0.2590 - val_loss: 1.4315 - val_accuracy: 0.2417\n",
      "Epoch 13/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4334 - accuracy: 0.2584 - val_loss: 1.4312 - val_accuracy: 0.2618\n",
      "Epoch 14/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4333 - accuracy: 0.2586 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 15/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4334 - accuracy: 0.2592 - val_loss: 1.4313 - val_accuracy: 0.2618\n",
      "Epoch 16/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4333 - accuracy: 0.2589 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 17/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 18/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2595 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 19/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2593 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 20/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2594 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 21/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2589 - val_loss: 1.4309 - val_accuracy: 0.2618\n",
      "Epoch 22/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4333 - accuracy: 0.2592 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 23/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2595 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 24/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 25/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2590 - val_loss: 1.4312 - val_accuracy: 0.2618\n",
      "Epoch 26/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 27/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2586 - val_loss: 1.4312 - val_accuracy: 0.2618\n",
      "Epoch 28/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4312 - val_accuracy: 0.2618\n",
      "Epoch 29/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 30/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2594 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 31/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 32/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 33/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2594 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 34/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2594 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 35/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 36/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 37/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 38/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2592 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 39/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2594 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 40/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 41/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2595 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 42/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2595 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 43/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 44/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2594 - val_loss: 1.4313 - val_accuracy: 0.2618\n",
      "Epoch 45/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 46/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2594 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 47/500\n",
      "1798/1798 [==============================] - 8s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 48/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2594 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 49/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2593 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 50/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4312 - val_accuracy: 0.2618\n",
      "Epoch 51/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2593 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 52/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 53/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 54/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 55/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 56/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2589 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 57/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2595 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 58/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4312 - val_accuracy: 0.2618\n",
      "Epoch 59/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4312 - val_accuracy: 0.2618\n",
      "Epoch 60/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 61/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4312 - val_accuracy: 0.2618\n",
      "Epoch 62/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2595 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 63/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 64/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 65/500\n",
      "1798/1798 [==============================] - 10s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 66/500\n",
      "1798/1798 [==============================] - 10s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 67/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 68/500\n",
      "1798/1798 [==============================] - 10s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 69/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4312 - val_accuracy: 0.2618\n",
      "Epoch 70/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 71/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 72/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 73/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 74/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 75/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 76/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 77/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4309 - val_accuracy: 0.2618\n",
      "Epoch 78/500\n",
      "1798/1798 [==============================] - 11s 6ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 79/500\n",
      "1798/1798 [==============================] - 10s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 80/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 81/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 82/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4313 - val_accuracy: 0.2618\n",
      "Epoch 83/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 84/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 85/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 86/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 87/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 88/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4309 - val_accuracy: 0.2618\n",
      "Epoch 89/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 90/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 91/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 92/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 93/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 94/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 95/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 96/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 97/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 98/500\n",
      "1798/1798 [==============================] - 10s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 99/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 100/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 101/500\n",
      "1798/1798 [==============================] - 10s 6ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4312 - val_accuracy: 0.2618\n",
      "Epoch 102/500\n",
      "1798/1798 [==============================] - 10s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4309 - val_accuracy: 0.2618\n",
      "Epoch 103/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 104/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 105/500\n",
      "1798/1798 [==============================] - 10s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 106/500\n",
      "1798/1798 [==============================] - 10s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 107/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 108/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 109/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 110/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 111/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 112/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 113/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 114/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 115/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 116/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 117/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 118/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 119/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 120/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 121/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 122/500\n",
      "1798/1798 [==============================] - 10s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 123/500\n",
      "1798/1798 [==============================] - 10s 6ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 124/500\n",
      "1798/1798 [==============================] - 10s 6ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 125/500\n",
      "1798/1798 [==============================] - 10s 6ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 126/500\n",
      "1798/1798 [==============================] - 11s 6ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 127/500\n",
      "1798/1798 [==============================] - 11s 6ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 128/500\n",
      "1798/1798 [==============================] - 11s 6ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 129/500\n",
      "1798/1798 [==============================] - 11s 6ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 130/500\n",
      "1798/1798 [==============================] - 12s 7ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 131/500\n",
      "1798/1798 [==============================] - 13s 7ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 132/500\n",
      "1798/1798 [==============================] - 12s 7ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 133/500\n",
      "1798/1798 [==============================] - 11s 6ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 134/500\n",
      "1798/1798 [==============================] - 11s 6ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 135/500\n",
      "1798/1798 [==============================] - 11s 6ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 136/500\n",
      "1798/1798 [==============================] - 10s 6ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 137/500\n",
      "1798/1798 [==============================] - 11s 6ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 138/500\n",
      "1798/1798 [==============================] - 11s 6ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 139/500\n",
      "1798/1798 [==============================] - 11s 6ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 140/500\n",
      "1798/1798 [==============================] - 11s 6ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 141/500\n",
      "1798/1798 [==============================] - 11s 6ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 142/500\n",
      "1798/1798 [==============================] - 10s 6ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 143/500\n",
      "1798/1798 [==============================] - 10s 6ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 144/500\n",
      "1798/1798 [==============================] - 10s 6ms/step - loss: 1.4332 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 145/500\n",
      "1798/1798 [==============================] - 10s 6ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 146/500\n",
      "1798/1798 [==============================] - 11s 6ms/step - loss: 1.4331 - accuracy: 0.2593 - val_loss: 1.4313 - val_accuracy: 0.2618\n",
      "Epoch 147/500\n",
      "1798/1798 [==============================] - 11s 6ms/step - loss: 1.4331 - accuracy: 0.2593 - val_loss: 1.4309 - val_accuracy: 0.2618\n",
      "Epoch 148/500\n",
      "1798/1798 [==============================] - 11s 6ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 149/500\n",
      "1798/1798 [==============================] - 10s 6ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 150/500\n",
      "1798/1798 [==============================] - 10s 6ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 151/500\n",
      "1798/1798 [==============================] - 10s 6ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 152/500\n",
      "1798/1798 [==============================] - 10s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 153/500\n",
      "1798/1798 [==============================] - 10s 6ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 154/500\n",
      "1798/1798 [==============================] - 10s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 155/500\n",
      "1798/1798 [==============================] - 10s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 156/500\n",
      "1798/1798 [==============================] - 10s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 157/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 158/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 159/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 160/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 161/500\n",
      "1798/1798 [==============================] - 10s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 162/500\n",
      "1798/1798 [==============================] - 10s 6ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 163/500\n",
      "1798/1798 [==============================] - 10s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 164/500\n",
      "1798/1798 [==============================] - 10s 6ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 165/500\n",
      "1798/1798 [==============================] - 10s 6ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4309 - val_accuracy: 0.2618\n",
      "Epoch 166/500\n",
      "1798/1798 [==============================] - 10s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 167/500\n",
      "1798/1798 [==============================] - 10s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 168/500\n",
      "1798/1798 [==============================] - 10s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4309 - val_accuracy: 0.2618\n",
      "Epoch 169/500\n",
      "1798/1798 [==============================] - 10s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4309 - val_accuracy: 0.2618\n",
      "Epoch 170/500\n",
      "1798/1798 [==============================] - 10s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 171/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 172/500\n",
      "1798/1798 [==============================] - 10s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4309 - val_accuracy: 0.2618\n",
      "Epoch 173/500\n",
      "1798/1798 [==============================] - 10s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 174/500\n",
      "1798/1798 [==============================] - 10s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 175/500\n",
      "1798/1798 [==============================] - 10s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 176/500\n",
      "1798/1798 [==============================] - 10s 6ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 177/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 178/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 179/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 180/500\n",
      "1798/1798 [==============================] - 10s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 181/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 182/500\n",
      "1798/1798 [==============================] - 10s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 183/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 184/500\n",
      "1798/1798 [==============================] - 10s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 185/500\n",
      "1798/1798 [==============================] - 10s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 186/500\n",
      "1798/1798 [==============================] - 10s 6ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 187/500\n",
      "1798/1798 [==============================] - 13s 7ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 188/500\n",
      "1798/1798 [==============================] - 13s 7ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 189/500\n",
      "1798/1798 [==============================] - 12s 7ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 190/500\n",
      "1798/1798 [==============================] - 13s 7ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4309 - val_accuracy: 0.2618\n",
      "Epoch 191/500\n",
      "1798/1798 [==============================] - 12s 7ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 192/500\n",
      "1798/1798 [==============================] - 12s 7ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 193/500\n",
      "1798/1798 [==============================] - 12s 7ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 194/500\n",
      "1798/1798 [==============================] - 12s 7ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 195/500\n",
      "1798/1798 [==============================] - 12s 7ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 196/500\n",
      "1798/1798 [==============================] - 12s 7ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 197/500\n",
      "1798/1798 [==============================] - 12s 7ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 198/500\n",
      "1798/1798 [==============================] - 12s 7ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 199/500\n",
      "1798/1798 [==============================] - 12s 7ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 200/500\n",
      "1798/1798 [==============================] - 12s 7ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 201/500\n",
      "1798/1798 [==============================] - 12s 7ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 202/500\n",
      "1798/1798 [==============================] - 12s 7ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 203/500\n",
      "1798/1798 [==============================] - 12s 7ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4309 - val_accuracy: 0.2618\n",
      "Epoch 204/500\n",
      "1798/1798 [==============================] - 12s 7ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 205/500\n",
      "1798/1798 [==============================] - 12s 7ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 206/500\n",
      "1798/1798 [==============================] - 12s 7ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 207/500\n",
      "1798/1798 [==============================] - 12s 7ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 208/500\n",
      "1798/1798 [==============================] - 12s 7ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4309 - val_accuracy: 0.2618\n",
      "Epoch 209/500\n",
      "1798/1798 [==============================] - 12s 7ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 210/500\n",
      "1798/1798 [==============================] - 12s 7ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 211/500\n",
      "1798/1798 [==============================] - 12s 7ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 212/500\n",
      "1798/1798 [==============================] - 13s 7ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 213/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 214/500\n",
      "1798/1798 [==============================] - 8s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 215/500\n",
      "1798/1798 [==============================] - 8s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 216/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 217/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 218/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4309 - val_accuracy: 0.2618\n",
      "Epoch 219/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 220/500\n",
      "1798/1798 [==============================] - 8s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 221/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 222/500\n",
      "1798/1798 [==============================] - 8s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 223/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 224/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 225/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 226/500\n",
      "1798/1798 [==============================] - 8s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 227/500\n",
      "1798/1798 [==============================] - 8s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 228/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 229/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 230/500\n",
      "1798/1798 [==============================] - 8s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 231/500\n",
      "1798/1798 [==============================] - 8s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 232/500\n",
      "1798/1798 [==============================] - 8s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 233/500\n",
      "1798/1798 [==============================] - 8s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 234/500\n",
      "1798/1798 [==============================] - 8s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 235/500\n",
      "1798/1798 [==============================] - 8s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 236/500\n",
      "1798/1798 [==============================] - 8s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 237/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 238/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 239/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 240/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 241/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 242/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 243/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 244/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 245/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 246/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4309 - val_accuracy: 0.2618\n",
      "Epoch 247/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 248/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4309 - val_accuracy: 0.2618\n",
      "Epoch 249/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 250/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 251/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 252/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 253/500\n",
      "1798/1798 [==============================] - 7s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 254/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 255/500\n",
      "1798/1798 [==============================] - 7s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 256/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 257/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 258/500\n",
      "1798/1798 [==============================] - 7s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 259/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 260/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 261/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4309 - val_accuracy: 0.2618\n",
      "Epoch 262/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 263/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 264/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 265/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 266/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 267/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 268/500\n",
      "1798/1798 [==============================] - 7s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 269/500\n",
      "1798/1798 [==============================] - 7s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 270/500\n",
      "1798/1798 [==============================] - 7s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 271/500\n",
      "1798/1798 [==============================] - 7s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 272/500\n",
      "1798/1798 [==============================] - 7s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 273/500\n",
      "1798/1798 [==============================] - 7s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 274/500\n",
      "1798/1798 [==============================] - 7s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4309 - val_accuracy: 0.2618\n",
      "Epoch 275/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 276/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 277/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 278/500\n",
      "1798/1798 [==============================] - 7s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 279/500\n",
      "1798/1798 [==============================] - 7s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 280/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 281/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 282/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 283/500\n",
      "1798/1798 [==============================] - 7s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 284/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 285/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 286/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 287/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 288/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 289/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 290/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 291/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 292/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 293/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4309 - val_accuracy: 0.2618\n",
      "Epoch 294/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 295/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 296/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 297/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4309 - val_accuracy: 0.2618\n",
      "Epoch 298/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 299/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 300/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 301/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 302/500\n",
      "1798/1798 [==============================] - 7s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4309 - val_accuracy: 0.2618\n",
      "Epoch 303/500\n",
      "1798/1798 [==============================] - 7s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 304/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 305/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 306/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 307/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 308/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 309/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 310/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 311/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 312/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 313/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 314/500\n",
      "1798/1798 [==============================] - 7s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 315/500\n",
      "1798/1798 [==============================] - 7s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 316/500\n",
      "1798/1798 [==============================] - 7s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4312 - val_accuracy: 0.2618\n",
      "Epoch 317/500\n",
      "1798/1798 [==============================] - 7s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 318/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 319/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 320/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 321/500\n",
      "1798/1798 [==============================] - 7s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 322/500\n",
      "1798/1798 [==============================] - 7s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 323/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 324/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 325/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 326/500\n",
      "1798/1798 [==============================] - 7s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4309 - val_accuracy: 0.2618\n",
      "Epoch 327/500\n",
      "1798/1798 [==============================] - 7s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 328/500\n",
      "1798/1798 [==============================] - 7s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 329/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 330/500\n",
      "1798/1798 [==============================] - 7s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 331/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 332/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 333/500\n",
      "1798/1798 [==============================] - 7s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 334/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 335/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 336/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 337/500\n",
      "1798/1798 [==============================] - 7s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4309 - val_accuracy: 0.2618\n",
      "Epoch 338/500\n",
      "1798/1798 [==============================] - 7s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 339/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 340/500\n",
      "1798/1798 [==============================] - 7s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 341/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 342/500\n",
      "1798/1798 [==============================] - 7s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 343/500\n",
      "1798/1798 [==============================] - 7s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4309 - val_accuracy: 0.2618\n",
      "Epoch 344/500\n",
      "1798/1798 [==============================] - 7s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 345/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 346/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 347/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 348/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 349/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 350/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 351/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 352/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 353/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 354/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 355/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 356/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 357/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 358/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 359/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 360/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4309 - val_accuracy: 0.2618\n",
      "Epoch 361/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4309 - val_accuracy: 0.2618\n",
      "Epoch 362/500\n",
      "1798/1798 [==============================] - 8s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 363/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4309 - val_accuracy: 0.2618\n",
      "Epoch 364/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 365/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 366/500\n",
      "1798/1798 [==============================] - 10s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 367/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4309 - val_accuracy: 0.2618\n",
      "Epoch 368/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 369/500\n",
      "1798/1798 [==============================] - 10s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 370/500\n",
      "1798/1798 [==============================] - 10s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 371/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 372/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 373/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 374/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4311 - val_accuracy: 0.2618\n",
      "Epoch 375/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 376/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 377/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 378/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 379/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 380/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 381/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 382/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 383/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4309 - val_accuracy: 0.2618\n",
      "Epoch 384/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 385/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 386/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 387/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 388/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 389/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 390/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 391/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 392/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 393/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 394/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 395/500\n",
      "1798/1798 [==============================] - 9s 5ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4309 - val_accuracy: 0.2618\n",
      "Epoch 396/500\n",
      "1798/1798 [==============================] - 7s 4ms/step - loss: 1.4331 - accuracy: 0.2596 - val_loss: 1.4310 - val_accuracy: 0.2618\n",
      "Epoch 397/500\n",
      " 975/1798 [===============>..............] - ETA: 3s - loss: 1.4328 - accuracy: 0.2600"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(512, input_shape=(26,), activation='relu'),\n",
    "    Dense(256, activation='relu'),\n",
    "    # # Dropout(0.2),\n",
    "    # Dense(128, activation='relu'),\n",
    "    # # Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(5, activation='softmax')  # Output layer with softmax activation for multiclass classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=500, batch_size=100, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "joblib.dump(model, \"C:/Users/Aaditya Gupta/OneDrive/Desktop/AML_A1/model_0.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
