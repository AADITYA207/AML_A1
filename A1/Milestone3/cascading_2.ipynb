{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hard = pd.read_csv(\"C:/Users/Aaditya Gupta/OneDrive/Desktop/AML_A1_M3/df_synA_test_hard_shuffled_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_n_val</th>\n",
       "      <th>High_n_val</th>\n",
       "      <th>Low_n_val</th>\n",
       "      <th>Close_n_val</th>\n",
       "      <th>Volume_n_val</th>\n",
       "      <th>SMA_10_val</th>\n",
       "      <th>SMA_20_val</th>\n",
       "      <th>CMO_14_val</th>\n",
       "      <th>High_n-Low_n_val</th>\n",
       "      <th>Open_n-Close_n_val</th>\n",
       "      <th>...</th>\n",
       "      <th>SMA_20-SMA_10_changelen_val</th>\n",
       "      <th>Close_n_slope_3_changelen_val</th>\n",
       "      <th>Close_n_slope_5_changelen_val</th>\n",
       "      <th>Close_n_slope_10_changelen_val</th>\n",
       "      <th>row_num</th>\n",
       "      <th>day</th>\n",
       "      <th>era</th>\n",
       "      <th>target_10_val</th>\n",
       "      <th>target_5_val</th>\n",
       "      <th>data_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>75</td>\n",
       "      <td>526</td>\n",
       "      <td>7</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>75</td>\n",
       "      <td>380</td>\n",
       "      <td>17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>75</td>\n",
       "      <td>521</td>\n",
       "      <td>17</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>75</td>\n",
       "      <td>500</td>\n",
       "      <td>17</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>75</td>\n",
       "      <td>488</td>\n",
       "      <td>0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>75</td>\n",
       "      <td>467</td>\n",
       "      <td>22</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>75</td>\n",
       "      <td>474</td>\n",
       "      <td>15</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>75</td>\n",
       "      <td>514</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>75</td>\n",
       "      <td>544</td>\n",
       "      <td>8</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>75</td>\n",
       "      <td>386</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>75</td>\n",
       "      <td>512</td>\n",
       "      <td>14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Open_n_val  High_n_val  Low_n_val  Close_n_val  Volume_n_val  SMA_10_val  \\\n",
       "0         0.25        0.50       0.50          0.5           0.0        0.50   \n",
       "1         0.25        0.25       0.50          0.5           0.0        0.00   \n",
       "2         0.75        0.75       0.75          0.5           0.0        1.00   \n",
       "3         0.75        0.50       0.50          0.5           0.0        0.25   \n",
       "4         1.00        0.75       0.75          0.5           0.0        1.00   \n",
       "5         0.75        0.75       0.50          0.5           0.0        0.00   \n",
       "6         0.50        0.50       0.50          0.5           0.0        1.00   \n",
       "7         0.50        0.75       0.75          0.5           0.0        1.00   \n",
       "8         0.75        0.75       0.50          0.5           0.0        0.50   \n",
       "9         0.50        0.50       0.50          0.5           0.0        1.00   \n",
       "10        0.50        0.50       0.50          0.5           0.0        0.00   \n",
       "\n",
       "    SMA_20_val  CMO_14_val  High_n-Low_n_val  Open_n-Close_n_val  ...  \\\n",
       "0         1.00        0.00              0.50                0.00  ...   \n",
       "1         0.25        0.50              0.00                0.00  ...   \n",
       "2         1.00        0.00              0.00                1.00  ...   \n",
       "3         0.50        0.50              0.75                0.75  ...   \n",
       "4         1.00        0.00              0.50                1.00  ...   \n",
       "5         0.00        1.00              0.50                1.00  ...   \n",
       "6         1.00        0.25              0.00                0.25  ...   \n",
       "7         1.00        0.00              0.50                0.75  ...   \n",
       "8         0.25        0.75              1.00                1.00  ...   \n",
       "9         1.00        0.00              0.25                1.00  ...   \n",
       "10        0.00        0.75              0.25                0.75  ...   \n",
       "\n",
       "    SMA_20-SMA_10_changelen_val  Close_n_slope_3_changelen_val  \\\n",
       "0                          0.25                           1.00   \n",
       "1                          0.50                           1.00   \n",
       "2                          0.75                           0.25   \n",
       "3                          1.00                           0.00   \n",
       "4                          1.00                           0.25   \n",
       "5                          0.50                           0.75   \n",
       "6                          1.00                           1.00   \n",
       "7                          0.25                           1.00   \n",
       "8                          0.50                           0.50   \n",
       "9                          1.00                           0.00   \n",
       "10                         0.00                           0.50   \n",
       "\n",
       "    Close_n_slope_5_changelen_val  Close_n_slope_10_changelen_val  row_num  \\\n",
       "0                            1.00                            1.00       75   \n",
       "1                            1.00                            0.75       75   \n",
       "2                            0.00                            0.00       75   \n",
       "3                            0.00                            0.25       75   \n",
       "4                            0.25                            0.25       75   \n",
       "5                            1.00                            1.00       75   \n",
       "6                            0.75                            0.75       75   \n",
       "7                            0.50                            0.50       75   \n",
       "8                            0.50                            0.50       75   \n",
       "9                            0.00                            0.00       75   \n",
       "10                           0.50                            0.50       75   \n",
       "\n",
       "    day  era  target_10_val  target_5_val   data_type  \n",
       "0   526    7           1.00          1.00  validation  \n",
       "1   380   17           0.00          1.00  validation  \n",
       "2   521   17           0.75          0.75  validation  \n",
       "3   500   17           0.25          1.00  validation  \n",
       "4   488    0           0.75          1.00  validation  \n",
       "5   467   22           0.25          0.75  validation  \n",
       "6   474   15           1.00          1.00  validation  \n",
       "7   514    7           0.00          0.00  validation  \n",
       "8   544    8           1.00          0.75  validation  \n",
       "9   386    7           0.00          0.00  validation  \n",
       "10  512   14           0.00          0.00  validation  \n",
       "\n",
       "[11 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hard_ = df_hard.sort_values(by=['row_num'])\n",
    "df_hard_.reset_index(inplace = True)\n",
    "df_hard_ = df_hard_.drop(columns=['index'])\n",
    "df_hard_.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, y_data = df_hard_.drop(columns=['era','target_10_val'\t,'target_5_val',\t'data_type']), df_hard_['target_10_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.  , 0.  , 0.75, 0.25, 0.5 ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_data.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d={}\n",
    "y_=[]\n",
    "count=0\n",
    "for i in y:\n",
    "  if i not in d:\n",
    "    d[i]=count\n",
    "    count+=1\n",
    "  y_.append(d[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_n_val</th>\n",
       "      <th>High_n_val</th>\n",
       "      <th>Low_n_val</th>\n",
       "      <th>Close_n_val</th>\n",
       "      <th>Volume_n_val</th>\n",
       "      <th>SMA_10_val</th>\n",
       "      <th>SMA_20_val</th>\n",
       "      <th>CMO_14_val</th>\n",
       "      <th>High_n-Low_n_val</th>\n",
       "      <th>Open_n-Close_n_val</th>\n",
       "      <th>...</th>\n",
       "      <th>Low_n_changelen_val</th>\n",
       "      <th>Close_n_changelen_val</th>\n",
       "      <th>High_n-Low_n_changelen_val</th>\n",
       "      <th>Open_n-Close_n_changelen_val</th>\n",
       "      <th>SMA_20-SMA_10_changelen_val</th>\n",
       "      <th>Close_n_slope_3_changelen_val</th>\n",
       "      <th>Close_n_slope_5_changelen_val</th>\n",
       "      <th>Close_n_slope_10_changelen_val</th>\n",
       "      <th>row_num</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>75</td>\n",
       "      <td>526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>75</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>75</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>75</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>75</td>\n",
       "      <td>488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249595</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>139</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249596</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>139</td>\n",
       "      <td>457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249597</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>139</td>\n",
       "      <td>431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249598</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>139</td>\n",
       "      <td>491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249599</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>139</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>249600 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Open_n_val  High_n_val  Low_n_val  Close_n_val  Volume_n_val  \\\n",
       "0             0.25        0.50       0.50         0.50           0.0   \n",
       "1             0.25        0.25       0.50         0.50           0.0   \n",
       "2             0.75        0.75       0.75         0.50           0.0   \n",
       "3             0.75        0.50       0.50         0.50           0.0   \n",
       "4             1.00        0.75       0.75         0.50           0.0   \n",
       "...            ...         ...        ...          ...           ...   \n",
       "249595        0.25        0.00       0.00         0.25           0.0   \n",
       "249596        0.75        0.75       0.75         0.75           0.0   \n",
       "249597        0.50        0.50       0.50         0.75           0.0   \n",
       "249598        0.00        0.00       0.00         0.00           0.0   \n",
       "249599        1.00        1.00       1.00         1.00           0.0   \n",
       "\n",
       "        SMA_10_val  SMA_20_val  CMO_14_val  High_n-Low_n_val  \\\n",
       "0             0.50        1.00        0.00              0.50   \n",
       "1             0.00        0.25        0.50              0.00   \n",
       "2             1.00        1.00        0.00              0.00   \n",
       "3             0.25        0.50        0.50              0.75   \n",
       "4             1.00        1.00        0.00              0.50   \n",
       "...            ...         ...         ...               ...   \n",
       "249595        0.00        0.00        0.50              0.00   \n",
       "249596        0.75        0.75        1.00              1.00   \n",
       "249597        1.00        0.75        0.50              0.75   \n",
       "249598        0.00        0.00        0.25              0.00   \n",
       "249599        1.00        1.00        0.75              0.25   \n",
       "\n",
       "        Open_n-Close_n_val  ...  Low_n_changelen_val  Close_n_changelen_val  \\\n",
       "0                     0.00  ...                 0.25                   0.75   \n",
       "1                     0.00  ...                 0.50                   0.50   \n",
       "2                     1.00  ...                 0.25                   0.00   \n",
       "3                     0.75  ...                 0.25                   0.00   \n",
       "4                     1.00  ...                 0.25                   0.25   \n",
       "...                    ...  ...                  ...                    ...   \n",
       "249595                0.50  ...                 0.25                   0.25   \n",
       "249596                0.50  ...                 1.00                   1.00   \n",
       "249597                0.25  ...                 0.00                   0.00   \n",
       "249598                0.75  ...                 1.00                   0.50   \n",
       "249599                0.25  ...                 0.75                   0.25   \n",
       "\n",
       "        High_n-Low_n_changelen_val  Open_n-Close_n_changelen_val  \\\n",
       "0                             0.25                          0.00   \n",
       "1                             0.00                          0.00   \n",
       "2                             0.00                          0.25   \n",
       "3                             0.25                          0.50   \n",
       "4                             0.25                          0.50   \n",
       "...                            ...                           ...   \n",
       "249595                        0.00                          0.50   \n",
       "249596                        0.75                          0.75   \n",
       "249597                        0.25                          0.25   \n",
       "249598                        0.00                          0.50   \n",
       "249599                        0.25                          0.25   \n",
       "\n",
       "        SMA_20-SMA_10_changelen_val  Close_n_slope_3_changelen_val  \\\n",
       "0                              0.25                           1.00   \n",
       "1                              0.50                           1.00   \n",
       "2                              0.75                           0.25   \n",
       "3                              1.00                           0.00   \n",
       "4                              1.00                           0.25   \n",
       "...                             ...                            ...   \n",
       "249595                         0.75                           0.25   \n",
       "249596                         0.50                           0.75   \n",
       "249597                         0.00                           0.50   \n",
       "249598                         0.75                           0.50   \n",
       "249599                         0.75                           0.50   \n",
       "\n",
       "        Close_n_slope_5_changelen_val  Close_n_slope_10_changelen_val  \\\n",
       "0                                1.00                            1.00   \n",
       "1                                1.00                            0.75   \n",
       "2                                0.00                            0.00   \n",
       "3                                0.00                            0.25   \n",
       "4                                0.25                            0.25   \n",
       "...                               ...                             ...   \n",
       "249595                           0.00                            0.25   \n",
       "249596                           0.25                            0.75   \n",
       "249597                           0.00                            0.25   \n",
       "249598                           0.50                            0.25   \n",
       "249599                           0.25                            0.25   \n",
       "\n",
       "        row_num  day  \n",
       "0            75  526  \n",
       "1            75  380  \n",
       "2            75  521  \n",
       "3            75  500  \n",
       "4            75  488  \n",
       "...         ...  ...  \n",
       "249595      139  376  \n",
       "249596      139  457  \n",
       "249597      139  431  \n",
       "249598      139  491  \n",
       "249599      139  512  \n",
       "\n",
       "[249600 rows x 26 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Noise mitigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming you have your features in X and labels in y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y_ = np.array(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((249600,), (249600, 26))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_.shape, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train_one_hot = to_categorical(y_, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2901\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for i in y_:\n",
    "    if i==4:\n",
    "        n+=1\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_dic = {}\n",
    "for i in range(len(y_)):\n",
    "    if(y_[i] not in one_hot_dic):\n",
    "        one_hot_dic[y_[i]] = y_train_one_hot[i]\n",
    "\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rev_one_hot(y):\n",
    "    y_new=[]\n",
    "    for i in y:\n",
    "        for j in range(len(i)):\n",
    "            if(i[j]==1):\n",
    "                y_new.append(j)\n",
    "    return y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.layers import PReLU\n",
    "from tensorflow.keras.layers import LeakyReLU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_models():\n",
    "    models_list = []\n",
    "\n",
    "    # Model 1\n",
    "    input_shape=26\n",
    "    num_classes=5\n",
    "    model = Sequential([\n",
    "        Dense(16, activation='relu', input_shape=(input_shape,)),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    models_list.append(model)\n",
    "\n",
    "    # Model 2\n",
    "    model = Sequential([\n",
    "        Dense(16, activation='sigmoid', input_shape=(input_shape,)),\n",
    "        Dense(8, activation='sigmoid'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    models_list.append(model)\n",
    "\n",
    "    # Model 3\n",
    "    model = Sequential([\n",
    "        Dense(16, activation='tanh', input_shape=(input_shape,)),\n",
    "        Dense(8, activation='tanh'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    models_list.append(model)\n",
    "\n",
    "    # Model 4\n",
    "    model = Sequential([\n",
    "        Dense(20, input_shape=(26,)),\n",
    "        PReLU(),\n",
    "        Dense(10),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(5, activation='softmax')  # Assuming num_classes output classes\n",
    "    ])\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    models_list.append(model)\n",
    "\n",
    "    # Model 5\n",
    "    model = Sequential([\n",
    "        Dense(16, activation='tanh', input_shape=(input_shape,)),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adagrad', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    models_list.append(model)\n",
    "\n",
    "    # Model 6\n",
    "    model = Sequential([\n",
    "        Dense(16, activation='linear', input_shape=(input_shape,)),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adamax', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    models_list.append(model)\n",
    "\n",
    "    # Model 7\n",
    "    model = Sequential([\n",
    "        Dense(16, activation='sigmoid', input_shape=(input_shape,)),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    models_list.append(model)\n",
    "\n",
    "    # Model 8\n",
    "    model = Sequential([\n",
    "        Dense(16, activation='relu', input_shape=(input_shape,)),\n",
    "        Dense(8, activation='tanh'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    models_list.append(model)\n",
    "\n",
    "    # Model 9\n",
    "    model = Sequential([\n",
    "        Dense(16, activation='relu', input_shape=(input_shape,)),\n",
    "        Dense(8, activation='sigmoid'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    models_list.append(model)\n",
    "\n",
    "    # Model 10\n",
    "    model = Sequential([\n",
    "        Dense(20, input_shape=(26,), activation='relu'),\n",
    "        Dense(10),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(5, activation='softmax')  # Assuming num_classes output classes\n",
    "    ])\n",
    "    # Compile the model\n",
    "    model.compile(optimizer = 'adagrad',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    models_list.append(model)\n",
    "\n",
    "\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(20, input_shape=(26,)),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(10),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(5, activation='softmax')  # Assuming num_classes output classes\n",
    "    ])\n",
    "    # Compile the model\n",
    "    model.compile(optimizer = 'adagrad',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    models_list.append(model)\n",
    "\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(16, input_shape=(26,)),\n",
    "        PReLU(),\n",
    "        Dense(8),\n",
    "        PReLU(),\n",
    "        Dense(5, activation='softmax')  # Assuming num_classes output classes\n",
    "    ])\n",
    "    # Compile the model\n",
    "    model.compile(optimizer = 'adagrad',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    models_list.append(model)\n",
    "    return models_list\n",
    "# Train and predict for each model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "# Define your neural network architecture\n",
    "\n",
    "\n",
    "def one_model(X,y_train_one_hot ):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_train_one_hot, test_size=0.2, random_state=42)\n",
    "    X_val, X_test_2, y_val, y_test_2 = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "    \n",
    "    '''\n",
    "    model1 = models.Sequential([\n",
    "        layers.Dense(10, activation='relu', input_shape=(26,)),\n",
    "        # layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(5, activation='softmax')  # Assuming num_classes output classes\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model1.fit(X_train, y_train, epochs=100, batch_size=32)\n",
    "\n",
    "    # Predict probabilities on test data\n",
    "    probabilities1 = model1.predict(X_test)\n",
    "    predicted_prob1 = np.max(probabilities1, axis=1)\n",
    "    predicted_label1 = np.argmax(probabilities1, axis=1)\n",
    "##########################################################################\n",
    "    l2_regularizer = tf.keras.regularizers.l2(0.01)\n",
    "    model2 = models.Sequential([\n",
    "        layers.Dense(10, activation='linear', input_shape=(26,), kernel_regularizer=l2_regularizer),\n",
    "        # layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(5, activation='softmax')  # Assuming num_classes output classes\n",
    "    ])\n",
    "    # Compile the model\n",
    "    model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # Train the model\n",
    "    model2.fit(X_train, y_train, epochs=100, batch_size=32)\n",
    "    # Predict probabilities on test data\n",
    "    probabilities2 = model2.predict(X_test)\n",
    "    predicted_prob2 = np.max(probabilities2, axis=1)\n",
    "    predicted_label2 = np.argmax(probabilities2, axis=1)\n",
    "#############################################################################\n",
    "    model3 = models.Sequential([\n",
    "        layers.Dense(20, activation='tanh', input_shape=(26,)),\n",
    "        layers.Dense(10, activation='tanh'),\n",
    "        layers.Dense(5, activation='softmax')  # Assuming num_classes output classes\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model3.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model3.fit(X_train, y_train, epochs=100, batch_size=32)\n",
    "\n",
    "    # Predict probabilities on test data\n",
    "    probabilities3 = model3.predict(X_test)\n",
    "    predicted_prob3 = np.max(probabilities3, axis=1)\n",
    "    predicted_label3 = np.argmax(probabilities3, axis=1)\n",
    "##################################################################################\n",
    "    model4 = models.Sequential([\n",
    "        layers.Dense(20, input_shape=(26,)),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dense(10),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dense(5, activation='softmax')  # Assuming num_classes output classes\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model4.compile(optimizer = 'adagrad',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model4.fit(X_train, y_train, epochs=100, batch_size=32)\n",
    "\n",
    "    # Predict probabilities on test data\n",
    "    probabilities4 = model4.predict(X_test)\n",
    "    predicted_prob4 = np.max(probabilities4, axis=1)\n",
    "    predicted_label4 = np.argmax(probabilities4, axis=1)\n",
    "    # Apply thresholding\n",
    "    # threshold = 0.5  # Adjust threshold as needed\n",
    "    # thresholded_predictions = (probabilities > threshold).astype(int)\n",
    "\n",
    "    # # Convert one-hot encoded predictions back to class labels if needed\n",
    "    # predicted_labels = np.argmax(thresholded_predictions, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    # # Assume probabilities is the output of model.predict(X_test)\n",
    "\n",
    "    # # Apply thresholding\n",
    "    # threshold = 0.5  # Adjust threshold as needed\n",
    "    # thresholded_predictions = (probabilities > threshold).astype(int)\n",
    "\n",
    "    # # Calculate maximum probability for each prediction\n",
    "    # max_probabilities = np.max(probabilities, axis=1)\n",
    "\n",
    "    # # Find indices where all class probabilities are below the threshold\n",
    "    # underconfident_indices = np.where(max_probabilities < threshold)[0]\n",
    "\n",
    "    # # Separate the underconfident data points\n",
    "    # underconfident_data_points = X_test[underconfident_indices]\n",
    "    # underconf_label = y_test[underconfident_indices]\n",
    "\n",
    "    \n",
    "    import joblib\n",
    "    joblib.dump(model1,\"C:/Users/Aaditya Gupta/OneDrive/Desktop/AML_A1_M3/model1.joblib\" )\n",
    "    joblib.dump(model2,\"C:/Users/Aaditya Gupta/OneDrive/Desktop/AML_A1_M3/model2.joblib\" )\n",
    "    # joblib.dump(model3,\"C:/Users/Aaditya Gupta/OneDrive/Desktop/AML_A1_M3/model3.joblib\" )\n",
    "    model1_cert = -np.sum(probabilities1 * np.log(probabilities1), axis=1)\n",
    "    model2_cert = -np.sum(probabilities2 * np.log(probabilities2), axis=1)\n",
    "    # model3_cert = -np.sum(probabilities3 * np.log(probabilities3), axis=1)\n",
    "    threshold = 0.56  # Adjust threshold based on desired trade-off between accuracy and coverage\n",
    "    '''\n",
    "    probabilities_list = []\n",
    "    predicted_prob_list = []\n",
    "    predicted_label_list = []\n",
    "    probabilities_list_val = []\n",
    "    predicted_prob_list_val = []\n",
    "    predicted_label_list_val = []\n",
    "\n",
    "    models_list = make_models()\n",
    "    \n",
    "    for i in range(len( models_list)):\n",
    "        # print(models_list[i].summary())\n",
    "        models_list[i].fit(X_train, y_train, epochs=100, batch_size=32)\n",
    "        \n",
    "        probabilities = models_list[i].predict(X_test)\n",
    "        predicted_prob = np.max(probabilities, axis=1)\n",
    "        predicted_label = np.argmax(probabilities, axis=1)\n",
    "        probabilities_list.append(probabilities)\n",
    "        predicted_prob_list.append(predicted_prob)\n",
    "        predicted_label_list.append(predicted_label)\n",
    "\n",
    "        probabilities_val = models_list[i].predict(X_train)\n",
    "        predicted_prob_val = np.max(probabilities_val, axis=1)\n",
    "        predicted_label_val = np.argmax(probabilities_val, axis=1)\n",
    "        probabilities_list_val.append(probabilities_val)\n",
    "        predicted_prob_list_val.append(predicted_prob_val)\n",
    "        predicted_label_list_val.append(predicted_label_val)\n",
    "    \n",
    "    threshold=0.56\n",
    "    X_uncert=[]\n",
    "    y_uncert=[]\n",
    "    preds=[]\n",
    "    X_cert=[]\n",
    "    y_cert=[]\n",
    "    # print(predicted_labels1, predicted_labels2,probabilities3)\n",
    "    # print(max(predicted_labels1[i], predicted_labels2[i]))\n",
    "    for i in range(len(X_test)):\n",
    "        max_prob = max(predicted_prob[i] for predicted_prob in predicted_prob_list)\n",
    "        if max_prob > threshold:\n",
    "            # Select classifier with higher certainty\n",
    "            for j, predicted_prob in enumerate(predicted_prob_list):\n",
    "                if max_prob == predicted_prob[i]:\n",
    "                    prediction = predicted_label_list[j][i]\n",
    "                    break\n",
    "        else:\n",
    "            # X_uncert.append(X_test[i])\n",
    "            # y_uncert.append(y_test[i])\n",
    "            continue\n",
    "        X_cert.append(X_test[i])\n",
    "        y_cert.append(y_test[i])\n",
    "        preds.append(prediction)\n",
    "    y_cert_ = rev_one_hot(y_cert)\n",
    "    # print(len(y_test),len(y_cert), len(y_uncert))\n",
    "    cm = confusion_matrix(y_cert_, preds)\n",
    "\n",
    "    # Plot confusion matrix with colors\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    print(classification_report(y_cert_, preds))\n",
    "\n",
    "    counter=0\n",
    "    threshold = 0.8\n",
    "    for i in range(len(X_train)):\n",
    "        max_prob = max(predicted_prob_val[i] for predicted_prob_val in predicted_prob_list_val)\n",
    "        if max_prob > threshold:\n",
    "            # Select classifier with higher certainty\n",
    "            for j, predicted_prob_val in enumerate(predicted_prob_list_val):\n",
    "                if max_prob == predicted_prob_val[i]:\n",
    "                    prediction = predicted_label_list_val[j][i]\n",
    "                    break\n",
    "        else:\n",
    "            X_uncert.append(X_train[i])\n",
    "            y_uncert.append(y_train[i])\n",
    "            counter+=1\n",
    "            continue\n",
    "\n",
    "    X_uncert, y_uncert = np.array(X_uncert), np.array(y_uncert)\n",
    "    print(counter,\"Number of pruned data points, i.e. which couldnt be predicted by a probab of 0.8\")\n",
    "    return y_cert, preds, X_uncert, y_uncert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trial:\n",
    "# x,a,b,c=one_model(X, y_train_one_hot)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cascade(X,y_train_one_hot,Y_TEST,Y_PRED):\n",
    "    if(len(X)<=30 or len(y_train_one_hot)<=30):\n",
    "        return Y_TEST,Y_PRED\n",
    "    y1,p1,b,c=one_model(X, y_train_one_hot)\n",
    "    Y_TEST.append(y1)\n",
    "    Y_PRED.append(p1)\n",
    "    print(len(p1), len(y1))\n",
    "    return cascade(b,c,Y_TEST,Y_PRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6240/6240 [==============================] - 13s 2ms/step - loss: 1.6652 - accuracy: 0.4176\n",
      "Epoch 2/100\n",
      "6240/6240 [==============================] - 14s 2ms/step - loss: 1.2321 - accuracy: 0.4966\n",
      "Epoch 3/100\n",
      "6240/6240 [==============================] - 15s 2ms/step - loss: 1.2083 - accuracy: 0.5036\n",
      "Epoch 4/100\n",
      "6240/6240 [==============================] - 12s 2ms/step - loss: 1.1981 - accuracy: 0.5074\n",
      "Epoch 5/100\n",
      "6240/6240 [==============================] - 12s 2ms/step - loss: 1.1931 - accuracy: 0.5099\n",
      "Epoch 6/100\n",
      "6240/6240 [==============================] - 11s 2ms/step - loss: 1.1887 - accuracy: 0.5111\n",
      "Epoch 7/100\n",
      "6240/6240 [==============================] - 12s 2ms/step - loss: 1.1853 - accuracy: 0.5128\n",
      "Epoch 8/100\n",
      "6240/6240 [==============================] - 11s 2ms/step - loss: 1.1793 - accuracy: 0.5137\n",
      "Epoch 9/100\n",
      "6240/6240 [==============================] - 11s 2ms/step - loss: 1.1688 - accuracy: 0.5150\n",
      "Epoch 10/100\n",
      "6240/6240 [==============================] - 12s 2ms/step - loss: 1.1620 - accuracy: 0.5161\n",
      "Epoch 11/100\n",
      "6240/6240 [==============================] - 11s 2ms/step - loss: 1.1574 - accuracy: 0.5198\n",
      "Epoch 12/100\n",
      "6240/6240 [==============================] - 11s 2ms/step - loss: 1.1529 - accuracy: 0.5204\n",
      "Epoch 13/100\n",
      "6240/6240 [==============================] - 11s 2ms/step - loss: 1.1480 - accuracy: 0.5209\n",
      "Epoch 14/100\n",
      " 801/6240 [==>...........................] - ETA: 9s - loss: 1.1456 - accuracy: 0.5237 "
     ]
    }
   ],
   "source": [
    "Y_TEST=[]\n",
    "Y_PRED=[]\n",
    "Y_TEST_got, Y_PRED_got = cascade(X, y_train_one_hot,Y_TEST,Y_PRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_PRED_t = [item for sublist in Y_PRED_got for item in sublist]\n",
    "Y_TEST_t = [item for sublist in Y_TEST_got for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_TEST_ = np.array(Y_TEST_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_PRED_ = np.array(Y_PRED_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, ..., 0, 0, 3], dtype=int64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_PRED_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_TEST_ = rev_one_hot(Y_TEST_)\n",
    "# Y_PRED_ = rev_one_hot(Y_PRED_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_TEST_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249600"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29491, 29491)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_PRED_), len(Y_TEST_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.86      0.82      8906\n",
      "           1       0.74      0.88      0.80      8701\n",
      "           2       0.76      0.60      0.67      5797\n",
      "           3       0.72      0.58      0.64      5871\n",
      "           4       0.00      0.00      0.00       216\n",
      "\n",
      "    accuracy                           0.75     29491\n",
      "   macro avg       0.60      0.58      0.59     29491\n",
      "weighted avg       0.75      0.75      0.74     29491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaditya Gupta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Aaditya Gupta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Aaditya Gupta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(Y_TEST_, Y_PRED_)\n",
    "\n",
    "# Plot confusion matrix with colors\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "print(classification_report(Y_TEST_, Y_PRED_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = rev_one_hot(y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
