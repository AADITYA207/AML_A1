{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_interest = pd.read_csv(\"C:/Users/Aaditya Gupta/OneDrive/Desktop/AML_A3_M1/papers_of_interest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily = pd.read_csv(\"C:/Users/Aaditya Gupta/OneDrive/Desktop/AML_A3_M1/daily_arxiv_papers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_emd=[]\n",
    "for i in range(len(daily)):\n",
    "    emd = model.encode(daily['abstract'][i])\n",
    "    abstract_emd.append(emd)\n",
    "abstract_emd = np.array(abstract_emd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily['abs_emb'] = abstract_emd.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_emd=[]\n",
    "for i in range(len(daily)):\n",
    "    emd = model.encode(daily['title'][i])\n",
    "    title_emd.append(emd)\n",
    "daily['title_emb'] = title_emd\n",
    "title_emd = np.array(title_emd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(938, 768) 313 938\n"
     ]
    }
   ],
   "source": [
    "print(abstract_emd.shape, len(p_interest), len(daily))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>date</th>\n",
       "      <th>abs_emb</th>\n",
       "      <th>title_emb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oai:arXiv.org:2403.19669v1</td>\n",
       "      <td>Analyzing the Roles of Language and Vision in ...</td>\n",
       "      <td>arXiv:2403.19669v1 Announce Type: new  Abstrac...</td>\n",
       "      <td>01-Apr-2024</td>\n",
       "      <td>[0.017098456621170044, 0.03898002952337265, -0...</td>\n",
       "      <td>[0.02266968, 0.08227675, -0.045680787, 0.04368...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oai:arXiv.org:2403.19717v1</td>\n",
       "      <td>A Picture is Worth 500 Labels: A Case Study of...</td>\n",
       "      <td>arXiv:2403.19717v1 Announce Type: new  Abstrac...</td>\n",
       "      <td>01-Apr-2024</td>\n",
       "      <td>[0.03491734713315964, 0.0838918387889862, -0.0...</td>\n",
       "      <td>[0.027676515, 0.07284404, -0.034094423, -0.042...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oai:arXiv.org:2403.19721v1</td>\n",
       "      <td>Computationally and Memory-Efficient Robust Pr...</td>\n",
       "      <td>arXiv:2403.19721v1 Announce Type: new  Abstrac...</td>\n",
       "      <td>01-Apr-2024</td>\n",
       "      <td>[-0.04559684544801712, 0.08678299933671951, -0...</td>\n",
       "      <td>[0.0033859517, 0.1171573, -0.06582342, -0.0120...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oai:arXiv.org:2403.19792v1</td>\n",
       "      <td>MAPL: Model Agnostic Peer-to-peer Learning</td>\n",
       "      <td>arXiv:2403.19792v1 Announce Type: new  Abstrac...</td>\n",
       "      <td>01-Apr-2024</td>\n",
       "      <td>[0.022128501906991005, 0.07788766920566559, 0....</td>\n",
       "      <td>[-0.04529728, 0.09195001, 0.013698801, 0.02492...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>oai:arXiv.org:2403.19800v1</td>\n",
       "      <td>Gegenbauer Graph Neural Networks for Time-vary...</td>\n",
       "      <td>arXiv:2403.19800v1 Announce Type: new  Abstrac...</td>\n",
       "      <td>01-Apr-2024</td>\n",
       "      <td>[-0.03688305988907814, 0.07381314039230347, 0....</td>\n",
       "      <td>[-0.041430797, 0.074615225, 0.0045213224, -0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id  \\\n",
       "0  oai:arXiv.org:2403.19669v1   \n",
       "1  oai:arXiv.org:2403.19717v1   \n",
       "2  oai:arXiv.org:2403.19721v1   \n",
       "3  oai:arXiv.org:2403.19792v1   \n",
       "4  oai:arXiv.org:2403.19800v1   \n",
       "\n",
       "                                               title  \\\n",
       "0  Analyzing the Roles of Language and Vision in ...   \n",
       "1  A Picture is Worth 500 Labels: A Case Study of...   \n",
       "2  Computationally and Memory-Efficient Robust Pr...   \n",
       "3         MAPL: Model Agnostic Peer-to-peer Learning   \n",
       "4  Gegenbauer Graph Neural Networks for Time-vary...   \n",
       "\n",
       "                                            abstract         date  \\\n",
       "0  arXiv:2403.19669v1 Announce Type: new  Abstrac...  01-Apr-2024   \n",
       "1  arXiv:2403.19717v1 Announce Type: new  Abstrac...  01-Apr-2024   \n",
       "2  arXiv:2403.19721v1 Announce Type: new  Abstrac...  01-Apr-2024   \n",
       "3  arXiv:2403.19792v1 Announce Type: new  Abstrac...  01-Apr-2024   \n",
       "4  arXiv:2403.19800v1 Announce Type: new  Abstrac...  01-Apr-2024   \n",
       "\n",
       "                                             abs_emb  \\\n",
       "0  [0.017098456621170044, 0.03898002952337265, -0...   \n",
       "1  [0.03491734713315964, 0.0838918387889862, -0.0...   \n",
       "2  [-0.04559684544801712, 0.08678299933671951, -0...   \n",
       "3  [0.022128501906991005, 0.07788766920566559, 0....   \n",
       "4  [-0.03688305988907814, 0.07381314039230347, 0....   \n",
       "\n",
       "                                           title_emb  \n",
       "0  [0.02266968, 0.08227675, -0.045680787, 0.04368...  \n",
       "1  [0.027676515, 0.07284404, -0.034094423, -0.042...  \n",
       "2  [0.0033859517, 0.1171573, -0.06582342, -0.0120...  \n",
       "3  [-0.04529728, 0.09195001, 0.013698801, 0.02492...  \n",
       "4  [-0.041430797, 0.074615225, 0.0045213224, -0.0...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/Aaditya Gupta/OneDrive/Desktop/AML_A3_M1/daily_embd.joblib']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(daily,\"C:/Users/Aaditya Gupta/OneDrive/Desktop/AML_A3_M1/daily_embd.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_my_list = joblib.load(\"C:/Users/Aaditya Gupta/OneDrive/Desktop/AML_A3_M1/summary.joblib\")\n",
    "title_my_list = joblib.load(\"C:/Users/Aaditya Gupta/OneDrive/Desktop/AML_A3_M1/title.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_interest_2 = joblib.load(\"C:/Users/Aaditya Gupta/OneDrive/Desktop/AML_A3_M1/papers_of_interest_with_summary_title.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_interest_2.head()\n",
    "summary_emd=[]\n",
    "for i in range(len(p_interest_2)):\n",
    "    emd = model.encode(p_interest_2['summary'][i])\n",
    "    summary_emd.append(emd)\n",
    "summary_emd = np.array(summary_emd)\n",
    "\n",
    "title_emd=[]\n",
    "for i in range(len(p_interest_2)):\n",
    "    emd = model.encode(p_interest_2['title'][i])\n",
    "    title_emd.append(emd)\n",
    "p_interest_2['title_emb'] = title_emd\n",
    "title_emd = np.array(title_emd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_interest_2['summary_emb'] = summary_emd.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no</th>\n",
       "      <th>url</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>title_emb</th>\n",
       "      <th>summary_emb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://arxiv.org/pdf/2209.11142.pdf</td>\n",
       "      <td>The cornerstone of neural algorithmic reasonin...</td>\n",
       "      <td>A Generalist Neural Algorithmic Learner</td>\n",
       "      <td>[0.0050590066, 0.075324915, -0.021246592, -0.0...</td>\n",
       "      <td>[0.014286473393440247, 0.04717475548386574, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://arxiv.org/pdf/2212.02475.pdf</td>\n",
       "      <td>Dynamic evaluation of language models (LMs) ad...</td>\n",
       "      <td>Meta-Learning Fast Weight Language Models</td>\n",
       "      <td>[0.035396647, 0.039115362, 0.021043783, 0.0258...</td>\n",
       "      <td>[0.0034925471991300583, 0.062363401055336, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://arxiv.org/pdf/2211.16564.pdf</td>\n",
       "      <td>The GLOM architecture proposed by Hinton [2021...</td>\n",
       "      <td>Testing GLOM's ability to infer wholes from am...</td>\n",
       "      <td>[0.012229637, 0.07378764, -0.011308909, 0.0535...</td>\n",
       "      <td>[0.022099114954471588, 0.0502488948404789, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://arxiv.org/abs/1803.05316</td>\n",
       "      <td>This book is an invitation to discover advance...</td>\n",
       "      <td>Seven Sketches in Compositionality: An Invitat...</td>\n",
       "      <td>[0.034200516, 0.040522955, -0.051570572, -0.00...</td>\n",
       "      <td>[-0.01335581298917532, 0.06820952147245407, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://arxiv.org/abs/2206.08896</td>\n",
       "      <td>This paper pursues the insight that large lang...</td>\n",
       "      <td>Evolution through Large Models</td>\n",
       "      <td>[-0.030068131, 0.08170434, -0.031072088, -0.00...</td>\n",
       "      <td>[-0.003080987837165594, 0.06612099707126617, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   no                                   url  \\\n",
       "0   1  https://arxiv.org/pdf/2209.11142.pdf   \n",
       "1   2  https://arxiv.org/pdf/2212.02475.pdf   \n",
       "2   3  https://arxiv.org/pdf/2211.16564.pdf   \n",
       "3   4      https://arxiv.org/abs/1803.05316   \n",
       "4   5      https://arxiv.org/abs/2206.08896   \n",
       "\n",
       "                                             summary  \\\n",
       "0  The cornerstone of neural algorithmic reasonin...   \n",
       "1  Dynamic evaluation of language models (LMs) ad...   \n",
       "2  The GLOM architecture proposed by Hinton [2021...   \n",
       "3  This book is an invitation to discover advance...   \n",
       "4  This paper pursues the insight that large lang...   \n",
       "\n",
       "                                               title  \\\n",
       "0            A Generalist Neural Algorithmic Learner   \n",
       "1          Meta-Learning Fast Weight Language Models   \n",
       "2  Testing GLOM's ability to infer wholes from am...   \n",
       "3  Seven Sketches in Compositionality: An Invitat...   \n",
       "4                     Evolution through Large Models   \n",
       "\n",
       "                                           title_emb  \\\n",
       "0  [0.0050590066, 0.075324915, -0.021246592, -0.0...   \n",
       "1  [0.035396647, 0.039115362, 0.021043783, 0.0258...   \n",
       "2  [0.012229637, 0.07378764, -0.011308909, 0.0535...   \n",
       "3  [0.034200516, 0.040522955, -0.051570572, -0.00...   \n",
       "4  [-0.030068131, 0.08170434, -0.031072088, -0.00...   \n",
       "\n",
       "                                         summary_emb  \n",
       "0  [0.014286473393440247, 0.04717475548386574, 0....  \n",
       "1  [0.0034925471991300583, 0.062363401055336, -0....  \n",
       "2  [0.022099114954471588, 0.0502488948404789, 0.0...  \n",
       "3  [-0.01335581298917532, 0.06820952147245407, -0...  \n",
       "4  [-0.003080987837165594, 0.06612099707126617, -...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_interest_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/Aaditya Gupta/OneDrive/Desktop/AML_A3_M1/p_interest_2_embd.joblib']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(p_interest_2,\"C:/Users/Aaditya Gupta/OneDrive/Desktop/AML_A3_M1/p_interest_2_embd.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>date</th>\n",
       "      <th>abs_emb</th>\n",
       "      <th>title_emb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oai:arXiv.org:2403.19669v1</td>\n",
       "      <td>Analyzing the Roles of Language and Vision in ...</td>\n",
       "      <td>arXiv:2403.19669v1 Announce Type: new  Abstrac...</td>\n",
       "      <td>01-Apr-2024</td>\n",
       "      <td>[0.017098456621170044, 0.03898002952337265, -0...</td>\n",
       "      <td>[0.02266968, 0.08227675, -0.045680787, 0.04368...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oai:arXiv.org:2403.19717v1</td>\n",
       "      <td>A Picture is Worth 500 Labels: A Case Study of...</td>\n",
       "      <td>arXiv:2403.19717v1 Announce Type: new  Abstrac...</td>\n",
       "      <td>01-Apr-2024</td>\n",
       "      <td>[0.03491734713315964, 0.0838918387889862, -0.0...</td>\n",
       "      <td>[0.027676515, 0.07284404, -0.034094423, -0.042...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oai:arXiv.org:2403.19721v1</td>\n",
       "      <td>Computationally and Memory-Efficient Robust Pr...</td>\n",
       "      <td>arXiv:2403.19721v1 Announce Type: new  Abstrac...</td>\n",
       "      <td>01-Apr-2024</td>\n",
       "      <td>[-0.04559684544801712, 0.08678299933671951, -0...</td>\n",
       "      <td>[0.0033859517, 0.1171573, -0.06582342, -0.0120...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oai:arXiv.org:2403.19792v1</td>\n",
       "      <td>MAPL: Model Agnostic Peer-to-peer Learning</td>\n",
       "      <td>arXiv:2403.19792v1 Announce Type: new  Abstrac...</td>\n",
       "      <td>01-Apr-2024</td>\n",
       "      <td>[0.022128501906991005, 0.07788766920566559, 0....</td>\n",
       "      <td>[-0.04529728, 0.09195001, 0.013698801, 0.02492...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>oai:arXiv.org:2403.19800v1</td>\n",
       "      <td>Gegenbauer Graph Neural Networks for Time-vary...</td>\n",
       "      <td>arXiv:2403.19800v1 Announce Type: new  Abstrac...</td>\n",
       "      <td>01-Apr-2024</td>\n",
       "      <td>[-0.03688305988907814, 0.07381314039230347, 0....</td>\n",
       "      <td>[-0.041430797, 0.074615225, 0.0045213224, -0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id  \\\n",
       "0  oai:arXiv.org:2403.19669v1   \n",
       "1  oai:arXiv.org:2403.19717v1   \n",
       "2  oai:arXiv.org:2403.19721v1   \n",
       "3  oai:arXiv.org:2403.19792v1   \n",
       "4  oai:arXiv.org:2403.19800v1   \n",
       "\n",
       "                                               title  \\\n",
       "0  Analyzing the Roles of Language and Vision in ...   \n",
       "1  A Picture is Worth 500 Labels: A Case Study of...   \n",
       "2  Computationally and Memory-Efficient Robust Pr...   \n",
       "3         MAPL: Model Agnostic Peer-to-peer Learning   \n",
       "4  Gegenbauer Graph Neural Networks for Time-vary...   \n",
       "\n",
       "                                            abstract         date  \\\n",
       "0  arXiv:2403.19669v1 Announce Type: new  Abstrac...  01-Apr-2024   \n",
       "1  arXiv:2403.19717v1 Announce Type: new  Abstrac...  01-Apr-2024   \n",
       "2  arXiv:2403.19721v1 Announce Type: new  Abstrac...  01-Apr-2024   \n",
       "3  arXiv:2403.19792v1 Announce Type: new  Abstrac...  01-Apr-2024   \n",
       "4  arXiv:2403.19800v1 Announce Type: new  Abstrac...  01-Apr-2024   \n",
       "\n",
       "                                             abs_emb  \\\n",
       "0  [0.017098456621170044, 0.03898002952337265, -0...   \n",
       "1  [0.03491734713315964, 0.0838918387889862, -0.0...   \n",
       "2  [-0.04559684544801712, 0.08678299933671951, -0...   \n",
       "3  [0.022128501906991005, 0.07788766920566559, 0....   \n",
       "4  [-0.03688305988907814, 0.07381314039230347, 0....   \n",
       "\n",
       "                                           title_emb  \n",
       "0  [0.02266968, 0.08227675, -0.045680787, 0.04368...  \n",
       "1  [0.027676515, 0.07284404, -0.034094423, -0.042...  \n",
       "2  [0.0033859517, 0.1171573, -0.06582342, -0.0120...  \n",
       "3  [-0.04529728, 0.09195001, 0.013698801, 0.02492...  \n",
       "4  [-0.041430797, 0.074615225, 0.0045213224, -0.0...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,) (768,) (768,) (768,)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(p_interest_2['title_emb'][0]).shape,\n",
    "np.array(p_interest_2['summary_emb'][0]).shape,\n",
    "np.array(daily['title_emb'][0]).shape,\n",
    "np.array(daily['abs_emb'][0]).shape,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "# # Example vectors\n",
    "# vector1 = np.array([1, 2, 3, 4])\n",
    "# vector2 = np.array([2, 3, 4, 5])\n",
    "\n",
    "# # Reshape the vectors to ensure they are 2D arrays\n",
    "# vector1 = vector1.reshape(1, -1)\n",
    "# vector2 = vector2.reshape(1, -1)\n",
    "\n",
    "# # Compute cosine similarity\n",
    "# cosine_sim = cosine_similarity(vector1, vector2)\n",
    "# print(\"Cosine Similarity:\", cosine_sim[0][0])\n",
    "\n",
    "# # Compute Euclidean distance\n",
    "# euclidean_dist = euclidean_distances(vector1, vector2)\n",
    "# print(\"Euclidean Distance:\", euclidean_dist[0][0])\n",
    "\n",
    "cos_score_title=[]\n",
    "euc_dis_title=[]\n",
    "cos_score_summary=[]\n",
    "euc_dis_summary=[]\n",
    "for i in range(len(p_interest_2)):\n",
    "    cos_title=[]\n",
    "    cos_summary=[]\n",
    "    euc_title=[]\n",
    "    euc_summary=[]\n",
    "    t_p = np.array(p_interest_2['title_emb'][i]).reshape(1,-1)\n",
    "    s_p = np.array(p_interest_2['summary_emb'][i]).reshape(1,-1)\n",
    "    for j in range(len(daily)):\n",
    "        t_d = np.array(daily['title_emb'][j]).reshape(1,-1)\n",
    "        s_d = np.array(daily['abs_emb'][j]).reshape(1,-1)\n",
    "        cos_title.append(cosine_similarity(t_p, t_d))\n",
    "        cos_summary.append(cosine_similarity(s_p, s_d))\n",
    "        euc_title.append(euclidean_distances(t_p, t_d))\n",
    "        euc_summary.append(euclidean_distances(s_p, s_d))\n",
    "\n",
    "    cos_score_title.append(cos_title)\n",
    "    euc_dis_title.append(euc_title)\n",
    "    cos_score_summary.append(cos_summary)\n",
    "    euc_dis_summary.append(euc_summary)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_score_title_ = np.array(cos_score_title).reshape(313,938)\n",
    "euc_dis_title_ = np.array(euc_dis_title).reshape(313,938)\n",
    "cos_score_summary_ = np.array(cos_score_summary).reshape(313,938)\n",
    "euc_dis_summary_ = np.array(euc_dis_summary).reshape(313,938)\n",
    "# print(cos_score_title_[:10])\n",
    "# cos_score_title_ = cos_score_title_.reshape(313,938)\n",
    "# print(cos_score_title_[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(313, 938) (313, 938) (313, 938) (313, 938)\n"
     ]
    }
   ],
   "source": [
    "print(cos_score_title_.shape,\n",
    "euc_dis_title_.shape,\n",
    "cos_score_summary_.shape,\n",
    "euc_dis_summary_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_largest_values_indices(arr, n=10):\n",
    "    indices = np.argpartition(arr, -n)[-n:]  # Get indices of n largest values\n",
    "    values = arr[indices]  # Get the corresponding values\n",
    "    sorted_indices = indices[np.argsort(values)][::-1]  # Sort indices by values in descending order\n",
    "    sorted_values = values[np.argsort(values)][::-1]  # Sort values in descending order\n",
    "    return sorted_values, sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_smallest_values_indices(arr, n=10):\n",
    "    indices = np.argpartition(arr, n)[:n]  # Get indices of n smallest values\n",
    "    values = arr[indices]  # Get the corresponding values\n",
    "    sorted_indices = np.sort(indices)  # Sort indices in ascending order\n",
    "    sorted_values = values[np.argsort(values)]  # Sort values in ascending order\n",
    "    return sorted_values, sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-99, -10,   1,   1,   2,   3,   8,   8,   9,  25]),\n",
       " array([ 0,  1,  2,  3,  4,  5,  6,  7,  8, 13], dtype=int64))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr=np.array([1,8,9,8,2,1,3,-10,25,78,100,123,894,-99,456])\n",
    "find_smallest_values_indices(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.5334933 , 0.5218463 , 0.5172602 , 0.5172602 , 0.5172602 ,\n",
      "       0.5123295 , 0.51229835, 0.51078105, 0.49922538, 0.49808612],\n",
      "      dtype=float32), array([124, 196, 630, 554, 478,  49, 795, 432, 896, 929], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(find_largest_values_indices(cos_score_title_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "recom_by_title_index_cos=[]\n",
    "recom_by_title_score_cos=[]\n",
    "for i in range(len(cos_score_title_)):\n",
    "    a,b=find_largest_values_indices(cos_score_title_[i])\n",
    "    recom_by_title_score_cos.append(a)\n",
    "    recom_by_title_index_cos.append(b)\n",
    "\n",
    "recom_by_summary_index_cos=[]\n",
    "recom_by_summary_score_cos=[]\n",
    "for i in range(len(cos_score_summary_)):\n",
    "    a,b=find_largest_values_indices(cos_score_summary_[i])\n",
    "    recom_by_summary_score_cos.append(a)\n",
    "    recom_by_summary_index_cos.append(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "recom_by_title_index_euc=[]\n",
    "recom_by_title_score_euc=[]\n",
    "for i in range(len(euc_dis_title_)):\n",
    "    a,b=find_smallest_values_indices(euc_dis_title_[i])\n",
    "    recom_by_title_score_euc.append(a)\n",
    "    recom_by_title_index_euc.append(b)\n",
    "\n",
    "recom_by_summary_index_euc=[]\n",
    "recom_by_summary_score_euc=[]\n",
    "for i in range(len(euc_dis_summary_)):\n",
    "    a,b=find_smallest_values_indices(euc_dis_summary_[i])\n",
    "    recom_by_summary_score_euc.append(a)\n",
    "    recom_by_summary_index_euc.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "recom={}\n",
    "for i in range(len(p_interest_2)):\n",
    "    row_i=set()\n",
    "    for j in recom_by_summary_index_cos[i]:\n",
    "        row_i.add(daily['title'][j])\n",
    "    recom[p_interest_2['title'][i]] = row_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers similar to papers_of_interest recommended via cosine similarity are :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'A Generalist Neural Algorithmic Learner': {'Beyond the Known: Novel Class Discovery for Open-world Graph Learning',\n",
       "  'Delayed Bottlenecking: Alleviating Forgetting in Pre-trained Graph\\n  Neural Networks',\n",
       "  'FR-NAS: Forward-and-Reverse Graph Predictor for Efficient Neural\\n  Architecture Search',\n",
       "  'GNNavigator: Towards Adaptive Training of Graph Neural Networks via\\n  Automatic Guideline Exploration',\n",
       "  'Graph Continual Learning with Debiased Lossless Memory Replay',\n",
       "  'Graph Machine Learning in the Era of Large Language Models (LLMs)',\n",
       "  'Improving the interpretability of GNN predictions through\\n  conformal-based graph sparsification',\n",
       "  'On the Scalability of GNNs for Molecular Graphs'},\n",
       " 'Meta-Learning Fast Weight Language Models': {'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LayerNorm: A key component in parameter-efficient fine-tuning',\n",
       "  'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling'},\n",
       " \"Testing GLOM's ability to infer wholes from ambiguous parts\": {'Bridging Vision and Language Spaces with Assignment Prediction',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Concept-based Analysis of Neural Networks via Vision-Language Models',\n",
       "  'GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Neuro-Inspired Information-Theoretic Hierarchical Perception for\\n  Multimodal Learning',\n",
       "  'Residual-based Language Models are Free Boosters for Biomedical Imaging',\n",
       "  'Sparse multimodal fusion with modal channel attention',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'Seven Sketches in Compositionality: An Invitation to Applied Category Theory': {'Accounting for AI and Users Shaping One Another: The Role of\\n  Mathematical Models',\n",
       "  'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions',\n",
       "  'Complexity of Probabilistic Reasoning for Neurosymbolic Classification\\n  Techniques',\n",
       "  'Generalized Gradient Descent is a Hypergraph Functor',\n",
       "  'On permutation-invariant neural networks',\n",
       "  'Semantic Cells: Evolutional Process to Acquire Sense Diversity of Items',\n",
       "  'Transformers Can Represent $n$-gram Language Models',\n",
       "  'What do Transformers Know about Government?'},\n",
       " 'Evolution through Large Models': {'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'Next Generation Loss Function for Image Classification',\n",
       "  'OpenBezoar: Small, Cost-Effective and Open Models Trained on Mixes of\\n  Instruction Data',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\\n  Upcycled Mixture-of-Experts'},\n",
       " 'Teaching Algorithmic Reasoning via In-context Learning': {'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Emergent Analogical Reasoning in Large Language Models': {'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Transformers Can Represent $n$-gram Language Models'},\n",
       " 'Autoformalization with Large Language Models': {'Automating REST API Postman Test Cases Using LLM',\n",
       "  'Concept-based Analysis of Neural Networks via Vision-Language Models',\n",
       "  'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Learning representations of learning representations',\n",
       "  'PARAMANU-GANITA: Language Model with Mathematical Capabilities',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'Symbolic Integration Algorithm Selection with Machine Learning: LSTMs vs\\n  Tree LSTMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing'},\n",
       " 'In-context Reinforcement Learning with Algorithm Distillation': {'Adaptive Regularization of Representation Rank as an Implicit Constraint\\n  of Bellman Equation',\n",
       "  'Adversarial Imitation Learning via Boosting',\n",
       "  'Continual Offline Reinforcement Learning via Diffusion-based Dual\\n  Generative Replay',\n",
       "  'DPO: Differential reinforcement learning with application to optimal\\n  configuration search',\n",
       "  'Decision Mamba: Reinforcement Learning via Sequence Modeling with Selective State Spaces',\n",
       "  'Deep Policy Optimization with Temporal Logic Constraints',\n",
       "  'Goal Exploration via Adaptive Skill Distribution for Goal-Conditioned\\n  Reinforcement Learning',\n",
       "  'Inferring Behavior-Specific Context Improves Zero-Shot Generalization in\\n  Reinforcement Learning',\n",
       "  'Offline Trajectory Generalization for Offline Reinforcement Learning',\n",
       "  'Zero-Shot Stitching in Reinforcement Learning using Relative\\n  Representations'},\n",
       " 'What learning algorithm is in-context learning? Investigations with linear models': {'Decision Mamba: Reinforcement Learning via Sequence Modeling with Selective State Spaces',\n",
       "  'Decomposing and Editing Predictions by Modeling Model Computation',\n",
       "  'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'Toward a Theory of Tokenization in LLMs',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'Transformers learn in-context by gradient descent': {'Can We Break Free from Strong Data Augmentations in Self-Supervised\\n  Learning?',\n",
       "  'Decomposing and Editing Predictions by Modeling Model Computation',\n",
       "  'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Employing Layerwised Unsupervised Learning to Lessen Data and Loss\\n  Requirements in Forward-Forward Algorithms',\n",
       "  'Enhancing Efficiency in Vision Transformer Networks: Design Techniques and Insights',\n",
       "  'Improving Generalization via Meta-Learning on Hard Samples',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'NC-TTT: A Noise Contrastive Approach for Test-Time Training',\n",
       "  'Reactive Model Correction: Mitigating Harm to Task-Relevant Features via\\n  Conditional Bias Suppression',\n",
       "  'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective'},\n",
       " 'Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers': {'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LayerNorm: A key component in parameter-efficient fine-tuning',\n",
       "  'Localizing Paragraph Memorization in Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models': {'EdgeFusion: On-Device Text-to-Image Generation',\n",
       "  'FlashSpeech: Efficient Zero-Shot Speech Synthesis',\n",
       "  'Guided Discrete Diffusion for Electronic Health Record Generation',\n",
       "  'LD-Pruner: Efficient Pruning of Latent Diffusion Models using\\n  Task-Agnostic Insights',\n",
       "  'Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level\\n  Knowledge Distillation'},\n",
       " 'Tracr: Compiled Transformers as a Laboratory for Interpretability': {'Concept-based Analysis of Neural Networks via Vision-Language Models',\n",
       "  'Decomposing and Editing Predictions by Modeling Model Computation',\n",
       "  'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'LayerNorm: A key component in parameter-efficient fine-tuning',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'Toward a Theory of Tokenization in LLMs',\n",
       "  'Transformers Can Represent $n$-gram Language Models',\n",
       "  'What do Transformers Know about Government?'},\n",
       " 'Learning-Rate-Free Learning by D-Adaptation': {'ColA: Collaborative Adaptation with Gradient Learning',\n",
       "  'DFWLayer: Differentiable Frank-Wolfe Optimization Layer',\n",
       "  'Data-Driven Performance Guarantees for Classical and Learned Optimizers',\n",
       "  'Federated Optimization with Doubly Regularized Drift Correction',\n",
       "  'Generalized Population-Based Training for Hyperparameter Optimization in\\n  Reinforcement Learning',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'Personalized Collaborative Fine-Tuning for On-Device Large Language\\n  Models',\n",
       "  'Regularized Gauss-Newton for Optimizing Overparameterized Neural\\n  Networks'},\n",
       " 'A Watermark for Large Language Models': {'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'Negative Label Guided OOD Detection with Pretrained Vision-Language Models',\n",
       "  'Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path\\n  Forward',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Private Attribute Inference from Images with Vision-Language Models',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'Talk Too Much: Poisoning Large Language Models under Token Limit',\n",
       "  'Toward a Theory of Tokenization in LLMs',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature': {'Does It Make Sense to Explain a Black Box With Another Black Box?',\n",
       "  'MUGC: Machine Generated versus User Generated Content Detection',\n",
       "  'Negative Label Guided OOD Detection with Pretrained Vision-Language Models',\n",
       "  'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'Robust CLIP-Based Detector for Exposing Diffusion Model-Generated Images',\n",
       "  'Talk Too Much: Poisoning Large Language Models under Token Limit',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Grounding Language Models to Images for Multimodal Inputs and Outputs': {'Bridging Vision and Language Spaces with Assignment Prediction',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Cross-Modal Adapter: Parameter-Efficient Transfer Learning Approach for\\n  Vision-Language Models',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Find The Gap: Knowledge Base Reasoning For Visual Question Answering',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept\\n  Understanding',\n",
       "  'RankCLIP: Ranking-Consistent Language-Image Pretraining',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'Stronger Random Baselines for In-Context Learning',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'ReAct: Synergizing Reasoning and Acting in Language Models': {'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction'},\n",
       " 'STaR: Bootstrapping Reasoning With Reasoning': {'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Hungry Hungry Hippos: Towards Language Modeling with State Space Models': {'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey',\n",
       "  'TriForce: Lossless Acceleration of Long Sequence Generation with\\n  Hierarchical Speculative Decoding'},\n",
       " 'Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment': {'Bridging Vision and Language Spaces with Assignment Prediction',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Cross-Modal Adapter: Parameter-Efficient Transfer Learning Approach for\\n  Vision-Language Models',\n",
       "  'Data-Efficient Multimodal Fusion on a Single GPU',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'RankCLIP: Ranking-Consistent Language-Image Pretraining',\n",
       "  'Residual-based Language Models are Free Boosters for Biomedical Imaging',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'Visual Time Series Forecasting: An Image-driven Approach': {'DeepHeteroIoT: Deep Local and Global Learning over Heterogeneous IoT Sensor Data',\n",
       "  'Generating Synthetic Time Series Data for Cyber-Physical Systems',\n",
       "  'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting',\n",
       "  'Integrating Marketing Channels into Quantile Transformation and Bayesian\\n  Optimization of Ensemble Kernels for Sales Prediction with Gaussian Process\\n  Models',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'TSLANet: Rethinking Transformers for Time Series Representation Learning',\n",
       "  'Using ARIMA to Predict the Expansion of Subscriber Data Consumption',\n",
       "  'Variational quantization for state space models'},\n",
       " 'Unveiling Transformers with LEGO: a synthetic reasoning task': {'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'LayerNorm: A key component in parameter-efficient fine-tuning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'What do Transformers Know about Government?'},\n",
       " 'Looped Transformers as Programmable Computers': {'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'Transformer tricks: Removing weights for skipless transformers'},\n",
       " 'Evaluating Large Language Models in Theory of Mind Tasks': {'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  \"On Large Language Models' Hallucination with Regard to Known Facts\",\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'Transformers Can Represent $n$-gram Language Models',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Learning threshold neurons via the \"edge of stability\"': {'A Mean-Field Analysis of Neural Gradient Descent-Ascent: Applications to\\n  Functional Conditional Moment Equations',\n",
       "  'A mean curvature flow arising in adversarial training',\n",
       "  'Regularized Gauss-Newton for Optimizing Overparameterized Neural\\n  Networks',\n",
       "  'Regularized Gradient Clipping Provably Trains Wide and Deep Neural\\n  Networks',\n",
       "  'Singular-limit analysis of gradient descent with noise injection',\n",
       "  'Training neural networks with structured noise improves classification and generalization'},\n",
       " 'Symbolic Discovery of Optimization Algorithms': {'Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World',\n",
       "  'FR-NAS: Forward-and-Reverse Graph Predictor for Efficient Neural\\n  Architecture Search',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LD-Pruner: Efficient Pruning of Latent Diffusion Models using\\n  Task-Agnostic Insights',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'PQA: Exploring the Potential of Product Quantization in DNN Hardware Acceleration',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens'},\n",
       " 'MoËT: Mixture of Expert Trees and its Application to Verifiable Reinforcement Learning': {'Actor-Critic Reinforcement Learning with Phased Actor',\n",
       "  'Adversarial Imitation Learning via Boosting',\n",
       "  'Application of the representative measure approach to assess the\\n  reliability of decision trees in dealing with unseen vehicle collision data',\n",
       "  'BayesJudge: Bayesian Kernel Language Modelling with Confidence\\n  Uncertainty in Legal Judgment Prediction',\n",
       "  'CtRL-Sim: Reactive and Controllable Driving Agents with Offline Reinforcement Learning',\n",
       "  'Enhancing Autonomous Vehicle Training with Language Model Integration\\n  and Critical Scenario Generation',\n",
       "  'Generalized Population-Based Training for Hyperparameter Optimization in\\n  Reinforcement Learning',\n",
       "  'Random Network Distillation Based Deep Reinforcement Learning for AGV\\n  Path Planning'},\n",
       " 'Continual Backprop: Stochastic Gradient Descent with Persistent Randomness': {'Actor-Critic Reinforcement Learning with Phased Actor',\n",
       "  'Continual Offline Reinforcement Learning via Diffusion-based Dual\\n  Generative Replay',\n",
       "  'Dynamically Anchored Prompting for Task-Imbalanced Continual Learning',\n",
       "  'FedMeS: Personalized Federated Continual Learning Leveraging Local\\n  Memory',\n",
       "  'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective',\n",
       "  'Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer',\n",
       "  'Single-Task Continual Offline Reinforcement Learning',\n",
       "  'Training neural networks with structured noise improves classification and generalization'},\n",
       " 'General-Purpose In-Context Learning by Meta-Learning Transformers': {'Distilled Datamodel with Reverse Gradient Matching',\n",
       "  'Improving Generalization via Meta-Learning on Hard Samples',\n",
       "  'MPC of Uncertain Nonlinear Systems with Meta-Learning for Fast\\n  Adaptation of Neural Predictive Models',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'Meta Reinforcement Learning with Finite Training Tasks -- a Density Estimation Approach',\n",
       "  'NC-TTT: A Noise Contrastive Approach for Test-Time Training',\n",
       "  'SOPHON: Non-Fine-Tunable Learning to Restrain Task Transferability For\\n  Pre-trained Models',\n",
       "  'Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer'},\n",
       " 'You Only Live Once: Single-Life Reinforcement Learning': {'Adversarial Imitation Learning via Boosting',\n",
       "  'Continual Offline Reinforcement Learning via Diffusion-based Dual\\n  Generative Replay',\n",
       "  'DPO: Differential reinforcement learning with application to optimal\\n  configuration search',\n",
       "  'Dataset Reset Policy Optimization for RLHF',\n",
       "  'Deep Policy Optimization with Temporal Logic Constraints',\n",
       "  'Goal Exploration via Adaptive Skill Distribution for Goal-Conditioned\\n  Reinforcement Learning',\n",
       "  'Inferring Behavior-Specific Context Improves Zero-Shot Generalization in\\n  Reinforcement Learning',\n",
       "  'Meta Reinforcement Learning with Finite Training Tasks -- a Density Estimation Approach',\n",
       "  'Offline Trajectory Generalization for Offline Reinforcement Learning',\n",
       "  'Single-Task Continual Offline Reinforcement Learning'},\n",
       " 'Human-Timescale Adaptation in an Open-Ended Task Space': {'ASID: Active Exploration for System Identification in Robotic\\n  Manipulation',\n",
       "  'Adversarial Imitation Learning via Boosting',\n",
       "  'Goal Exploration via Adaptive Skill Distribution for Goal-Conditioned\\n  Reinforcement Learning',\n",
       "  'Inferring Behavior-Specific Context Improves Zero-Shot Generalization in\\n  Reinforcement Learning',\n",
       "  'MAexp: A Generic Platform for RL-based Multi-Agent Exploration',\n",
       "  'Meta Reinforcement Learning with Finite Training Tasks -- a Density Estimation Approach',\n",
       "  'Rapid Motor Adaptation for Robotic Manipulator Arms',\n",
       "  'Zero-Shot Stitching in Reinforcement Learning using Relative\\n  Representations'},\n",
       " 'Language Is Not All You Need: Aligning Perception with Language Models': {'Bridging Vision and Language Spaces with Assignment Prediction',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Cross-Modal Adapter: Parameter-Efficient Transfer Learning Approach for\\n  Vision-Language Models',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Find The Gap: Knowledge Base Reasoning For Visual Question Answering',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'RankCLIP: Ranking-Consistent Language-Image Pretraining',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback': {'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'A study on the plasticity of neural networks': {'Continual Learning on a Diet: Learning from Sparsely Labeled Streams\\n  Under Constrained Computation',\n",
       "  'Continual Offline Reinforcement Learning via Diffusion-based Dual\\n  Generative Replay',\n",
       "  'Delayed Bottlenecking: Alleviating Forgetting in Pre-trained Graph\\n  Neural Networks',\n",
       "  'Dynamically Anchored Prompting for Task-Imbalanced Continual Learning',\n",
       "  'FedMeS: Personalized Federated Continual Learning Leveraging Local\\n  Memory',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'NC-TTT: A Noise Contrastive Approach for Test-Time Training',\n",
       "  'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective',\n",
       "  'Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer',\n",
       "  'Single-Task Continual Offline Reinforcement Learning'},\n",
       " 'Consistency Models': {'Balanced Mixed-Type Tabular Data Synthesis with Diffusion Models',\n",
       "  'EdgeFusion: On-Device Text-to-Image Generation',\n",
       "  'Efficient Conditional Diffusion Model with Probability Flow Sampling for\\n  Image Super-resolution',\n",
       "  'LD-Pruner: Efficient Pruning of Latent Diffusion Models using\\n  Task-Agnostic Insights',\n",
       "  'SparseDM: Toward Sparse Efficient Diffusion Models',\n",
       "  'TMPQ-DM: Joint Timestep Reduction and Quantization Precision Selection\\n  for Efficient Diffusion Models'},\n",
       " 'Larger language models do in-context learning differently': {'Aligning language models with human preferences',\n",
       "  'Localizing Paragraph Memorization in Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy\\n  Data',\n",
       "  'Stronger Random Baselines for In-Context Learning',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Permutation Equivariant Neural Functionals': {'A multiobjective continuation method to compute the regularization path of deep neural networks',\n",
       "  'Can We Break Free from Strong Data Augmentations in Self-Supervised\\n  Learning?',\n",
       "  'Deep Learning as Ricci Flow',\n",
       "  'Deep Neural Networks via Complex Network Theory: a Perspective',\n",
       "  'FR-NAS: Forward-and-Reverse Graph Predictor for Efficient Neural\\n  Architecture Search',\n",
       "  'HOIN: High-Order Implicit Neural Representations',\n",
       "  'LipSim: A Provably Robust Perceptual Similarity Metric',\n",
       "  'On permutation-invariant neural networks',\n",
       "  'Reactive Model Correction: Mitigating Harm to Task-Relevant Features via\\n  Conditional Bias Suppression',\n",
       "  'Strong Transferable Adversarial Attacks via Ensembled Asymptotically Normal Distribution Learning'},\n",
       " 'The Prompt Artists': {'An Economic Solution to Copyright Challenges of Generative AI',\n",
       "  'Ethical-Lens: Curbing Malicious Usages of Open-Source Text-to-Image\\n  Models',\n",
       "  'Exploring Text-to-Motion Generation with Human Preference',\n",
       "  'Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept\\n  Understanding',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  '©Plug-in Authorization for Human Content Copyright Protection\\n  in Text-to-Image Model'},\n",
       " 'Sparks of Artificial General Intelligence: Early experiments with GPT-4': {'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Online Deep Learning: Learning Deep Neural Networks on the Fly': {'A Unified Replay-based Continuous Learning Framework for Spatio-Temporal\\n  Prediction on Streaming Data',\n",
       "  'Actor-Critic Reinforcement Learning with Phased Actor',\n",
       "  'AntBatchInfer: Elastic Batch Inference in the Kubernetes Cluster',\n",
       "  'Employing Layerwised Unsupervised Learning to Lessen Data and Loss\\n  Requirements in Forward-Forward Algorithms',\n",
       "  'FedMeS: Personalized Federated Continual Learning Leveraging Local\\n  Memory',\n",
       "  'Online Algorithms with Limited Data Retention',\n",
       "  'QCore: Data-Efficient, On-Device Continual Calibration for Quantized\\n  Models -- Extended Version',\n",
       "  'Single-Task Continual Offline Reinforcement Learning'},\n",
       " 'Dynamically Modular and Sparse General Continual Learning': {'A Unified Replay-based Continuous Learning Framework for Spatio-Temporal\\n  Prediction on Streaming Data',\n",
       "  'Continual Learning on a Diet: Learning from Sparsely Labeled Streams\\n  Under Constrained Computation',\n",
       "  'Continual Offline Reinforcement Learning via Diffusion-based Dual\\n  Generative Replay',\n",
       "  'Delayed Bottlenecking: Alleviating Forgetting in Pre-trained Graph\\n  Neural Networks',\n",
       "  'Dynamically Anchored Prompting for Task-Imbalanced Continual Learning',\n",
       "  'Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World',\n",
       "  'FedMeS: Personalized Federated Continual Learning Leveraging Local\\n  Memory',\n",
       "  'Graph Continual Learning with Debiased Lossless Memory Replay',\n",
       "  'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective',\n",
       "  'Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer'},\n",
       " 'Meta-learning via Language Model In-context Tuning': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'Improving Generalization via Meta-Learning on Hard Samples',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'OTTER: Improving Zero-Shot Classification via Optimal Transport',\n",
       "  'Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy\\n  Data',\n",
       "  'Stronger Random Baselines for In-Context Learning',\n",
       "  'When LLMs are Unfit Use FastFit: Fast and Effective Text Classification\\n  with Many Classes'},\n",
       " 'SemSup: Semantic Supervision for Simple and Scalable Zero-shot Generalization': {'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Data Alignment for Zero-Shot Concept Generation in Dermatology AI',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'OTTER: Improving Zero-Shot Classification via Optimal Transport',\n",
       "  'RankCLIP: Ranking-Consistent Language-Image Pretraining',\n",
       "  'When LLMs are Unfit Use FastFit: Fast and Effective Text Classification\\n  with Many Classes',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " \"A Neuro-vector-symbolic Architecture for Solving Raven's Progressive Matrices\": {'Complexity of Probabilistic Reasoning for Neurosymbolic Classification\\n  Techniques',\n",
       "  'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'FR-NAS: Forward-and-Reverse Graph Predictor for Efficient Neural\\n  Architecture Search',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Neuro-Inspired Information-Theoretic Hierarchical Perception for\\n  Multimodal Learning',\n",
       "  'On the Independence Assumption in Neurosymbolic Learning',\n",
       "  'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective',\n",
       "  'Symbolic Integration Algorithm Selection with Machine Learning: LSTMs vs\\n  Tree LSTMs'},\n",
       " 'Transfer Learning with Deep Tabular Models': {'An improved tabular data generator with VAE-GMM integration',\n",
       "  'Explainable Lung Disease Classification from Chest X-Ray Images\\n  Utilizing Deep Learning and XAI',\n",
       "  'Joint chest X-ray diagnosis and clinical visual attention prediction with multi-stage cooperative learning: enhancing interpretability',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'Machine Learning Techniques for MRI Data Processing at Expanding Scale',\n",
       "  'Measuring Feature Dependency of Neural Networks by Collapsing Feature\\n  Dimensions in the Data Manifold',\n",
       "  'Time-aware Heterogeneous Graph Transformer with Adaptive Attention\\n  Merging for Health Event Prediction',\n",
       "  'Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science'},\n",
       " 'Improving Code Generation by Training with Natural Language Feedback': {'Aligning language models with human preferences',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model',\n",
       "  'XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\\n  Upcycled Mixture-of-Experts'},\n",
       " 'Self-Refine: Iterative Refinement with Self-Feedback': {'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level\\n  Knowledge Distillation',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning',\n",
       "  'Towards a Robust Retrieval-Based Summarization System',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'A Comparison of Reinforcement Learning and Deep Trajectory Based Stochastic Control Agents for Stepwise Mean-Variance Hedging': {'A Mean-Field Analysis of Neural Gradient Descent-Ascent: Applications to\\n  Functional Conditional Moment Equations',\n",
       "  'Actor-Critic Reinforcement Learning with Phased Actor',\n",
       "  'Continuous-time Risk-sensitive Reinforcement Learning via Quadratic\\n  Variation Penalty',\n",
       "  'DPO: Differential reinforcement learning with application to optimal\\n  configuration search',\n",
       "  'Reinforcement Learning in Agent-Based Market Simulation: Unveiling Realistic Stylized Facts and Behavior',\n",
       "  'VC Theory for Inventory Policies'},\n",
       " 'Language Models can Solve Computer Tasks': {'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing'},\n",
       " 'HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face': {'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions',\n",
       "  'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing'},\n",
       " 'Better Language Models of Code through Self-Improvement': {'Automated Multi-Language to English Machine Translation Using Generative\\n  Pre-Trained Transformers',\n",
       "  'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level\\n  Knowledge Distillation',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\\n  Upcycled Mixture-of-Experts'},\n",
       " 'Generative Agents: Interactive Simulacra of Human Behavior': {'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions',\n",
       "  'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Decoding AI: The inside story of data analysis in ChatGPT',\n",
       "  'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'PhyScene: Physically Interactable 3D Scene Synthesis for Embodied AI',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'iRAG: An Incremental Retrieval Augmented Generation System for Videos',\n",
       "  'iTBLS: A Dataset of Interactive Conversations Over Tabular Information'},\n",
       " 'Evaluating Verifiability in Generative Search Engines': {'AI-Generated Faces in the Real World: A Large-Scale Case Study of\\n  Twitter Profile Images',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'MUGC: Machine Generated versus User Generated Content Detection',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Reliability Estimation of News Media Sources: Birds of a Feather Flock\\n  Together',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'STRUM-LLM: Attributed and Structured Contrastive Summarization',\n",
       "  'Towards a Robust Retrieval-Based Summarization System',\n",
       "  'iRAG: An Incremental Retrieval Augmented Generation System for Videos'},\n",
       " 'Prompting Is Programming: A Query Language for Large Language Models': {'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving',\n",
       "  'Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\\n  Prediction',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction'},\n",
       " 'Scaling Transformer to 1M tokens and beyond with RMT': {'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving',\n",
       "  'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling'},\n",
       " 'Repository-Level Prompt Generation for Large Language Models of Code': {'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Beyond Code Generation: An Observational Study of ChatGPT Usage in\\n  Software Engineering Practice',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'LLM-Powered Test Case Generation for Detecting Tricky Bugs',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\\n  Upcycled Mixture-of-Experts'},\n",
       " 'Self-Instruct: Aligning Language Models with Self-Generated Instructions': {'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'Unlimiformer: Long-Range Transformers with Unlimited Length Input': {'Automated Multi-Language to English Machine Translation Using Generative\\n  Pre-Trained Transformers',\n",
       "  'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey'},\n",
       " 'Causal Reasoning and Large Language Models: Opening a New Frontier for Causality': {'Benchmarking Counterfactual Image Generation',\n",
       "  'Bias patterns in the application of LLMs for clinical decision support:\\n  A comprehensive study',\n",
       "  'CT-Agent: Clinical Trial Multi-Agent with Large Language Model-based\\n  Reasoning',\n",
       "  'Enhancing Counterfactual Explanation Search with Diffusion Distance and\\n  Directional Coherence',\n",
       "  'LLM-Enhanced Causal Discovery in Temporal Domain from Interventional\\n  Data',\n",
       "  'Neural Networks with Causal Graph Constraints: A New Approach for\\n  Treatment Effects Estimation',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'Redefining the Shortest Path Problem Formulation of the Linear\\n  Non-Gaussian Acyclic Model: Pairwise Likelihood Ratios, Prior Knowledge, and\\n  Path Enumeration'},\n",
       " 'Masked Trajectory Models for Prediction, Representation, and Control': {'Actor-Critic Reinforcement Learning with Phased Actor',\n",
       "  'Adaptive Regularization of Representation Rank as an Implicit Constraint\\n  of Bellman Equation',\n",
       "  'Adversarial Imitation Learning via Boosting',\n",
       "  'Continual Offline Reinforcement Learning via Diffusion-based Dual\\n  Generative Replay',\n",
       "  'CtRL-Sim: Reactive and Controllable Driving Agents with Offline Reinforcement Learning',\n",
       "  'DPO: Differential reinforcement learning with application to optimal\\n  configuration search',\n",
       "  'Deep Policy Optimization with Temporal Logic Constraints',\n",
       "  'Offline Trajectory Generalization for Offline Reinforcement Learning'},\n",
       " 'Replacing Rewards with Examples: Example-Based Policy Search via Recursive Classification': {'Actor-Critic Reinforcement Learning with Phased Actor',\n",
       "  'Adaptive Regularization of Representation Rank as an Implicit Constraint\\n  of Bellman Equation',\n",
       "  'DPO: Differential reinforcement learning with application to optimal\\n  configuration search',\n",
       "  'Dataset Reset Policy Optimization for RLHF',\n",
       "  'Deep Policy Optimization with Temporal Logic Constraints',\n",
       "  'From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function',\n",
       "  'Generalized Population-Based Training for Hyperparameter Optimization in\\n  Reinforcement Learning',\n",
       "  'Meta Reinforcement Learning with Finite Training Tasks -- a Density Estimation Approach'},\n",
       " 'Pretraining Without Attention': {'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LayerNorm: A key component in parameter-efficient fine-tuning',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey',\n",
       "  'Toward a Theory of Tokenization in LLMs'},\n",
       " 'Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision': {'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Aligning language models with human preferences',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Stepwise Alignment for Constrained Language Model Policy Optimization',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers': {'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LD-Pruner: Efficient Pruning of Latent Diffusion Models using\\n  Task-Agnostic Insights',\n",
       "  'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling'},\n",
       " 'The ConceptARC Benchmark: Evaluating Understanding and Generalization in the ARC Domain': {'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Decoding AI: The inside story of data analysis in ChatGPT',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'Pre-trained Vision-Language Models Learn Discoverable Visual Concepts',\n",
       "  'Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept\\n  Understanding',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning'},\n",
       " 'Symbol tuning improves in-context learning in language models': {'Aligning language models with human preferences',\n",
       "  'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'LayerNorm: A key component in parameter-efficient fine-tuning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy\\n  Data',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Online Continual Learning Without the Storage Constraint': {'A Unified Replay-based Continuous Learning Framework for Spatio-Temporal\\n  Prediction on Streaming Data',\n",
       "  'Continual Learning on a Diet: Learning from Sparsely Labeled Streams\\n  Under Constrained Computation',\n",
       "  'Continual Offline Reinforcement Learning via Diffusion-based Dual\\n  Generative Replay',\n",
       "  'Dynamically Anchored Prompting for Task-Imbalanced Continual Learning',\n",
       "  'FedMeS: Personalized Federated Continual Learning Leveraging Local\\n  Memory',\n",
       "  'QCore: Data-Efficient, On-Device Continual Calibration for Quantized\\n  Models -- Extended Version',\n",
       "  'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective',\n",
       "  'Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer',\n",
       "  'Single-Task Continual Offline Reinforcement Learning',\n",
       "  'kNN-CLIP: Retrieval Enables Training-Free Segmentation on Continually\\n  Expanding Large Vocabularies'},\n",
       " 'TinyStories: How Small Can Language Models Be and Still Speak Coherent English?': {'Automated Multi-Language to English Machine Translation Using Generative\\n  Pre-Trained Transformers',\n",
       "  'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level\\n  Knowledge Distillation',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning'},\n",
       " 'Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task': {'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Localizing Paragraph Memorization in Language Models',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  \"On Large Language Models' Hallucination with Regard to Known Facts\",\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Self-playing Adversarial Language Game Enhances LLM Reasoning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Transformers Can Represent $n$-gram Language Models',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models': {'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing'},\n",
       " 'Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions': {'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Self-playing Adversarial Language Game Enhances LLM Reasoning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction'},\n",
       " 'Has It All Been Solved? Open NLP Research Questions Not Solved by Large Language Models': {'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'Improving Factuality and Reasoning in Language Models through Multiagent Debate': {'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  \"On Large Language Models' Hallucination with Regard to Known Facts\",\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Meta-Learning Online Adaptation of Language Models': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'LayerNorm: A key component in parameter-efficient fine-tuning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy\\n  Data',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'Stronger Random Baselines for In-Context Learning',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback': {'Aligning language models with human preferences',\n",
       "  'FedEval-LLM: Federated Evaluation of Large Language Models on Downstream\\n  Tasks with Collective Wisdom',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Stronger Random Baselines for In-Context Learning',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Direct Preference Optimization: Your Language Model is Secretly a Reward Model': {'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Aligning language models with human preferences',\n",
       "  'Dataset Reset Policy Optimization for RLHF',\n",
       "  'Filtered Direct Preference Optimization',\n",
       "  'From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function',\n",
       "  'Learn Your Reference Model for Real Good Alignment',\n",
       "  'Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy\\n  Data',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs'},\n",
       " 'Thought Cloning: Learning to Think while Acting by Imitating Human Thinking': {'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions',\n",
       "  'Adversarial Imitation Learning via Boosting',\n",
       "  'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'LIMA: Less Is More for Alignment': {'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Aligning language models with human preferences',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Filtered Direct Preference Optimization',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy\\n  Data',\n",
       "  'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Gorilla: Large Language Model Connected with Massive APIs': {'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving',\n",
       "  'Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\\n  Prediction',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'Deductive Verification of Chain-of-Thought Reasoning': {'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  \"On Large Language Models' Hallucination with Regard to Known Facts\",\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning'},\n",
       " 'INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models': {'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'LayerNorm: A key component in parameter-efficient fine-tuning',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'PaLI-X: On Scaling up a Multilingual Vision and Language Model': {'Bridging Vision and Language Spaces with Assignment Prediction',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Cross-Modal Adapter: Parameter-Efficient Transfer Learning Approach for\\n  Vision-Language Models',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Find The Gap: Knowledge Base Reasoning For Visual Question Answering',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'RankCLIP: Ranking-Consistent Language-Image Pretraining',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'Passive learning of active causal strategies in agents and language models': {'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Aligning language models with human preferences',\n",
       "  'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'LLM-Enhanced Causal Discovery in Temporal Domain from Interventional\\n  Data',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Augmenting Language Models with Long-Term Memory': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'Localizing Paragraph Memorization in Language Models',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models'},\n",
       " 'V-LoL: A Diagnostic Dataset for Visual Logical Learning': {'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Concept-based Analysis of Neural Networks via Vision-Language Models',\n",
       "  'Deep Policy Optimization with Temporal Logic Constraints',\n",
       "  'Hyper Evidential Deep Learning to Quantify Composite Classification\\n  Uncertainty',\n",
       "  'LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos',\n",
       "  'Pre-trained Vision-Language Models Learn Discoverable Visual Concepts',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Zero-Shot Stitching in Reinforcement Learning using Relative\\n  Representations'},\n",
       " 'Fine-Tuning Language Models with Just Forward Passes': {'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'TriForce: Lossless Acceleration of Long Sequence Generation with\\n  Hierarchical Speculative Decoding'},\n",
       " 'The Curse of Recursion: Training on Generated Data Makes Models Forget': {'EdgeFusion: On-Device Text-to-Image Generation',\n",
       "  'LD-Pruner: Efficient Pruning of Latent Diffusion Models using\\n  Task-Agnostic Insights',\n",
       "  'Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging\\n  Perturbations That Efficiently Fool Customized Diffusion Models',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept\\n  Understanding',\n",
       "  'Residual-based Language Models are Free Boosters for Biomedical Imaging'},\n",
       " 'System-Level Natural Language Feedback': {'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Beyond Code Generation: An Observational Study of ChatGPT Usage in\\n  Software Engineering Practice',\n",
       "  'Filtered Direct Preference Optimization',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'ReALM: Reference Resolution As Language Modeling',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing'},\n",
       " 'Supervised Pretraining Can Learn In-Context Reinforcement Learning': {'CAESAR: Enhancing Federated RL in Heterogeneous MDPs through Convergence-Aware Sampling with Screening',\n",
       "  'Dataset Reset Policy Optimization for RLHF',\n",
       "  'Decision Mamba: Reinforcement Learning via Sequence Modeling with Selective State Spaces',\n",
       "  'Deep Policy Optimization with Temporal Logic Constraints',\n",
       "  'Goal Exploration via Adaptive Skill Distribution for Goal-Conditioned\\n  Reinforcement Learning',\n",
       "  'Meta Reinforcement Learning with Finite Training Tasks -- a Density Estimation Approach',\n",
       "  'Offline Trajectory Generalization for Offline Reinforcement Learning',\n",
       "  'Randomized Exploration in Cooperative Multi-Agent Reinforcement Learning',\n",
       "  'Settling Constant Regrets in Linear Markov Decision Processes',\n",
       "  'Tree Bandits for Generative Bayes'},\n",
       " 'ChemCrow: Augmenting large-language models with chemistry tools': {'$\\\\texttt{MiniMol}$: A Parameter-Efficient Foundation Model for Molecular\\n  Learning',\n",
       "  'A Self-feedback Knowledge Elicitation Approach for Chemical Reaction\\n  Predictions',\n",
       "  'Mol-AIR: Molecular Reinforcement Learning with Adaptive Intrinsic Rewards for Goal-directed Molecular Generation',\n",
       "  'On the Scalability of GNNs for Molecular Graphs',\n",
       "  'Physical formula enhanced multi-task learning for pharmacokinetics\\n  prediction',\n",
       "  'Physics-informed active learning for accelerating quantum chemical\\n  simulations',\n",
       "  'RLSynC: Offline-Online Reinforcement Learning for Synthon Completion',\n",
       "  'TacoGFN: Target-conditioned GFlowNet for Structure-based Drug Design'},\n",
       " \"Scientists' Perspectives on the Potential for Generative AI in their Fields\": {'AI-Generated Faces in the Real World: A Large-Scale Case Study of\\n  Twitter Profile Images',\n",
       "  'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions',\n",
       "  'An Economic Solution to Copyright Challenges of Generative AI',\n",
       "  'Decoding AI: The inside story of data analysis in ChatGPT',\n",
       "  'Ethical-Lens: Curbing Malicious Usages of Open-Source Text-to-Image\\n  Models',\n",
       "  'Language Model Prompt Selection via Simulation Optimization',\n",
       "  'MUGC: Machine Generated versus User Generated Content Detection',\n",
       "  'iTBLS: A Dataset of Interactive Conversations Over Tabular Information'},\n",
       " 'Validating Large Language Models with ReLM': {'Aligning language models with human preferences',\n",
       "  'Bias patterns in the application of LLMs for clinical decision support:\\n  A comprehensive study',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Learning representations of learning representations',\n",
       "  'Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path\\n  Forward',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'RTP-LX: Can LLMs Evaluate Toxicity in Multilingual Scenarios?',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression'},\n",
       " 'LongNet: Scaling Transformers to 1,000,000,000 Tokens': {'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving',\n",
       "  'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey'},\n",
       " 'Retentive Network: A Successor to Transformer for Large Language Models': {'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'TriForce: Lossless Acceleration of Long Sequence Generation with\\n  Hierarchical Speculative Decoding'},\n",
       " 'Conservative Prediction via Data-Driven Confidence Minimization': {'Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World',\n",
       "  'Explainable Machine Learning System for Predicting Chronic Kidney\\n  Disease in High-Risk Cardiovascular Patients',\n",
       "  'Gradient-Regularized Out-of-Distribution Detection',\n",
       "  'Toward a Realistic Benchmark for Out-of-Distribution Detection',\n",
       "  'Trusted Multi-view Learning with Label Noise',\n",
       "  'Would You Trust an AI Doctor? Building Reliable Medical Predictions with\\n  Kernel Dropout Uncertainty'},\n",
       " 'Learning to Model the World with Language': {'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'Inferring Behavior-Specific Context Improves Zero-Shot Generalization in\\n  Reinforcement Learning',\n",
       "  'LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'Zero-Shot Stitching in Reinforcement Learning using Relative\\n  Representations',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'Path Shadowing Monte-Carlo': {'Estimating the Distribution of Parameters in Differential Equations with\\n  Repeated Cross-Sectional Data',\n",
       "  'Multi-fidelity Gaussian process surrogate modeling for regression\\n  problems in physics',\n",
       "  'Multidimensional Interpolants',\n",
       "  'Neural Flow Diffusion Models: Learnable Forward Process for Improved\\n  Diffusion Modelling',\n",
       "  'Neural McKean-Vlasov Processes: Distributional Dependence in Diffusion\\n  Processes',\n",
       "  'Reinforcement Learning in Agent-Based Market Simulation: Unveiling Realistic Stylized Facts and Behavior',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'Variational Flow Models: Flowing in Your Style'},\n",
       " 'Self-Alignment with Instruction Backtranslation': {'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Aligning language models with human preferences',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Filtered Direct Preference Optimization',\n",
       "  'Learn Your Reference Model for Real Good Alignment',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Bayesian Flow Networks': {'All-in-one simulation-based inference',\n",
       "  'LD-Pruner: Efficient Pruning of Latent Diffusion Models using\\n  Task-Agnostic Insights',\n",
       "  'Neural Flow Diffusion Models: Learnable Forward Process for Improved\\n  Diffusion Modelling',\n",
       "  'Physics-integrated generative modeling using attentive planar\\n  normalizing flow based variational autoencoder',\n",
       "  'SparseDM: Toward Sparse Efficient Diffusion Models',\n",
       "  'TMPQ-DM: Joint Timestep Reduction and Quantization Precision Selection\\n  for Efficient Diffusion Models'},\n",
       " 'Evaluating Embedding APIs for Information Retrieval': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'VDTuner: Automated Performance Tuning for Vector Data Management Systems'},\n",
       " 'Learning One Abstract Bit at a Time Through Self-Invented Experiments Encoded as Neural Networks': {'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Decoding AI: The inside story of data analysis in ChatGPT',\n",
       "  'Deep Policy Optimization with Temporal Logic Constraints',\n",
       "  'From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function',\n",
       "  'Mol-AIR: Molecular Reinforcement Learning with Adaptive Intrinsic Rewards for Goal-directed Molecular Generation',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'The Causal Chambers: Real Physical Systems as a Testbed for AI\\n  Methodology',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing'},\n",
       " 'Teach LLMs to Personalize -- An Approach inspired by Writing Education': {'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Automated Multi-Language to English Machine Translation Using Generative\\n  Pre-Trained Transformers',\n",
       "  'Exploring Text-to-Motion Generation with Human Preference',\n",
       "  'Filtered Direct Preference Optimization',\n",
       "  'Language Model Prompt Selection via Simulation Optimization',\n",
       "  'MUGC: Machine Generated versus User Generated Content Detection',\n",
       "  'Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level\\n  Knowledge Distillation',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Towards a Robust Retrieval-Based Summarization System'},\n",
       " 'Efficient Guided Generation for Large Language Models': {'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Language Model Prompt Selection via Simulation Optimization',\n",
       "  'Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level\\n  Knowledge Distillation',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Toward a Theory of Tokenization in LLMs',\n",
       "  'TriForce: Lossless Acceleration of Long Sequence Generation with\\n  Hierarchical Speculative Decoding',\n",
       "  'σ-GPTs: A New Approach to Autoregressive Models'},\n",
       " 'Visual Instruction Tuning': {'Bridging Vision and Language Spaces with Assignment Prediction',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Cross-Modal Adapter: Parameter-Efficient Transfer Learning Approach for\\n  Vision-Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'RankCLIP: Ranking-Consistent Language-Image Pretraining',\n",
       "  'Self-Supervised Visual Preference Alignment',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'Giraffe: Adventures in Expanding Context Lengths in LLMs': {'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models'},\n",
       " 'Large Language Models as General Pattern Machines': {'Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Toward a Theory of Tokenization in LLMs',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'σ-GPTs: A New Approach to Autoregressive Models'},\n",
       " 'A Neurodiversity-Inspired Solver for the Abstraction \\\\& Reasoning Corpus (ARC) Using Visual Imagery and Program Synthesis': {'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Concept-based Analysis of Neural Networks via Vision-Language Models',\n",
       "  'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'Find The Gap: Knowledge Base Reasoning For Visual Question Answering',\n",
       "  'GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers',\n",
       "  'LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos',\n",
       "  'Pre-trained Vision-Language Models Learn Discoverable Visual Concepts',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing'},\n",
       " 'Learning to Learn Financial Networks for Optimising Momentum Strategies': {'A Survey of Deep Long-Tail Classification Advancements',\n",
       "  'A novel decision fusion approach for sale price prediction using Elastic Net and MOPSO',\n",
       "  'DFWLayer: Differentiable Frank-Wolfe Optimization Layer',\n",
       "  'FR-NAS: Forward-and-Reverse Graph Predictor for Efficient Neural\\n  Architecture Search',\n",
       "  'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'OptiGrad: A Fair and more Efficient Price Elasticity Optimization via a\\n  Gradient Based Learning',\n",
       "  'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'TSLANet: Rethinking Transformers for Time Series Representation Learning'},\n",
       " 'Network Momentum across Asset Classes': {'A novel decision fusion approach for sale price prediction using Elastic Net and MOPSO',\n",
       "  'AGHINT: Attribute-Guided Representation Learning on Heterogeneous\\n  Information Networks with Transformer',\n",
       "  'Conformal Prediction for Stochastic Decision-Making of PV Power in Electricity Markets',\n",
       "  'Conformal Predictive Systems Under Covariate Shift',\n",
       "  'Reinforcement Learning in Agent-Based Market Simulation: Unveiling Realistic Stylized Facts and Behavior',\n",
       "  'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'TSLANet: Rethinking Transformers for Time Series Representation Learning',\n",
       "  'Time-aware Heterogeneous Graph Transformer with Adaptive Attention\\n  Merging for Health Event Prediction',\n",
       "  'Using ARIMA to Predict the Expansion of Subscriber Data Consumption'},\n",
       " 'VolTS: A Volatility-based Trading System to forecast Stock Markets Trend using Statistics and Machine Learning': {'A Guide to Feature Importance Methods for Scientific Inference',\n",
       "  'A novel decision fusion approach for sale price prediction using Elastic Net and MOPSO',\n",
       "  'Conformal Prediction for Stochastic Decision-Making of PV Power in Electricity Markets',\n",
       "  'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting',\n",
       "  'Integrating Marketing Channels into Quantile Transformation and Bayesian\\n  Optimization of Ensemble Kernels for Sales Prediction with Gaussian Process\\n  Models',\n",
       "  'Reinforcement Learning in Agent-Based Market Simulation: Unveiling Realistic Stylized Facts and Behavior',\n",
       "  'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'TSLANet: Rethinking Transformers for Time Series Representation Learning',\n",
       "  'Using ARIMA to Predict the Expansion of Subscriber Data Consumption'},\n",
       " 'Graph of Thoughts: Solving Elaborate Problems with Large Language Models': {'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies': {'Aligning language models with human preferences',\n",
       "  'Bias patterns in the application of LLMs for clinical decision support:\\n  A comprehensive study',\n",
       "  'Filtered Direct Preference Optimization',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'RTP-LX: Can LLMs Evaluate Toxicity in Multilingual Scenarios?',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model',\n",
       "  'Using LLMs to Model the Beliefs and Preferences of Targeted Populations'},\n",
       " 'Gated recurrent neural networks discover attention': {'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey',\n",
       "  'Toward a Theory of Tokenization in LLMs',\n",
       "  'Training neural networks with structured noise improves classification and generalization'},\n",
       " 'Code Llama: Open Foundation Models for Code': {'Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\\n  Prediction',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'TriForce: Lossless Acceleration of Long Sequence Generation with\\n  Hierarchical Speculative Decoding',\n",
       "  'XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\\n  Upcycled Mixture-of-Experts'},\n",
       " 'One Wide Feedforward is All You Need': {'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey'},\n",
       " 'Nougat: Neural Optical Understanding for Academic Documents': {'AlloyBERT: Alloy Property Prediction with Large Language Models',\n",
       "  'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers',\n",
       "  'GatedLexiconNet: A Comprehensive End-to-End Handwritten Paragraph Text\\n  Recognition System',\n",
       "  'MathWriting: A Dataset For Handwritten Mathematical Expression\\n  Recognition',\n",
       "  'PARAMANU-GANITA: Language Model with Mathematical Capabilities',\n",
       "  'Residual-based Language Models are Free Boosters for Biomedical Imaging',\n",
       "  'Towards a Foundation Model for Partial Differential Equation:\\n  Multi-Operator Learning and Extrapolation'},\n",
       " 'Prompt2Model: Generating Deployable Models from Natural Language Instructions': {'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'Stronger Random Baselines for In-Context Learning'},\n",
       " 'Trading via Selective Classification': {'A novel decision fusion approach for sale price prediction using Elastic Net and MOPSO',\n",
       "  'Conformal Prediction for Stochastic Decision-Making of PV Power in Electricity Markets',\n",
       "  'DeepHeteroIoT: Deep Local and Global Learning over Heterogeneous IoT Sensor Data',\n",
       "  'Learning using granularity statistical invariants for classification',\n",
       "  'Mining Invariance from Nonlinear Multi-Environment Data: Binary\\n  Classification',\n",
       "  'OptiGrad: A Fair and more Efficient Price Elasticity Optimization via a\\n  Gradient Based Learning',\n",
       "  'Reinforcement Learning in Agent-Based Market Simulation: Unveiling Realistic Stylized Facts and Behavior',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'Using ARIMA to Predict the Expansion of Subscriber Data Consumption',\n",
       "  'VC Theory for Inventory Policies'},\n",
       " 'Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning': {'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'Cross-Modal Adapter: Parameter-Efficient Transfer Learning Approach for\\n  Vision-Language Models',\n",
       "  'Data-Efficient Multimodal Fusion on a Single GPU',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept\\n  Understanding',\n",
       "  'RankCLIP: Ranking-Consistent Language-Image Pretraining',\n",
       "  'Residual-based Language Models are Free Boosters for Biomedical Imaging',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'Are Emergent Abilities in Large Language Models just In-Context Learning?': {'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Large Language Models as Optimizers': {'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\\n  Prediction',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " \"Fluid Transformers and Creative Analogies: Exploring Large Language Models' Capacity for Augmenting Cross-Domain Analogical Creativity\": {'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level\\n  Knowledge Distillation',\n",
       "  'Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept\\n  Understanding',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'NExT-GPT: Any-to-Any Multimodal LLM': {'Bridging Vision and Language Spaces with Assignment Prediction',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Cross-Modal Adapter: Parameter-Efficient Transfer Learning Approach for\\n  Vision-Language Models',\n",
       "  'Data-Efficient Multimodal Fusion on a Single GPU',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Sparse multimodal fusion with modal channel attention',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'Cognitive Architectures for Language Agents': {'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions',\n",
       "  'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'RAIN: Your Language Models Can Align Themselves without Finetuning': {'Aligning language models with human preferences',\n",
       "  'Filtered Direct Preference Optimization',\n",
       "  'Learn Your Reference Model for Real Good Alignment',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Stepwise Alignment for Constrained Language Model Policy Optimization',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'RWKV: Reinventing RNNs for the Transformer Era': {'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey'},\n",
       " 'Large Language Model for Science: A Study on P vs. NP': {'CT-Agent: Clinical Trial Multi-Agent with Large Language Model-based\\n  Reasoning',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'Self-playing Adversarial Language Game Enhances LLM Reasoning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'FedEval-LLM: Federated Evaluation of Large Language Models on Downstream\\n  Tasks with Collective Wisdom',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'Stronger Random Baselines for In-Context Learning',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Agents: An Open-source Framework for Autonomous Language Agents': {'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions',\n",
       "  'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models',\n",
       "  'Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\\n  Prediction',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing'},\n",
       " 'Test-Time Training with Self-Supervision for Generalization under Distribution Shifts': {'A Survey of Deep Long-Tail Classification Advancements',\n",
       "  'Distilled Datamodel with Reverse Gradient Matching',\n",
       "  'Gradient-Regularized Out-of-Distribution Detection',\n",
       "  'Mining Invariance from Nonlinear Multi-Environment Data: Binary\\n  Classification',\n",
       "  'NC-TTT: A Noise Contrastive Approach for Test-Time Training',\n",
       "  'OTTER: Improving Zero-Shot Classification via Optimal Transport',\n",
       "  'Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer',\n",
       "  'Toward a Realistic Benchmark for Out-of-Distribution Detection'},\n",
       " 'The Rise and Potential of Large Language Model Based Agents: A Survey': {'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions',\n",
       "  'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Aligning language models with human preferences',\n",
       "  'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Self-playing Adversarial Language Game Enhances LLM Reasoning',\n",
       "  'Stepwise Alignment for Constrained Language Model Policy Optimization'},\n",
       " 'Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers': {'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Language Model Prompt Selection via Simulation Optimization',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing'},\n",
       " 'Textbooks Are All You Need II: phi-1.5 technical report': {'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'Transformers Can Represent $n$-gram Language Models',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'PDFTriage: Question Answering over Long, Structured Documents': {'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Latxa: An Open Language Model and Evaluation Suite for Basque',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Towards a Robust Retrieval-Based Summarization System'},\n",
       " 'Training Convolutional Networks with Noisy Labels': {'Can We Break Free from Strong Data Augmentations in Self-Supervised\\n  Learning?',\n",
       "  'DeNetDM: Debiasing by Network Depth Modulation',\n",
       "  'Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World',\n",
       "  'Gradient-Regularized Out-of-Distribution Detection',\n",
       "  'NC-TTT: A Noise Contrastive Approach for Test-Time Training',\n",
       "  'Noise contrastive estimation with soft targets for conditional models',\n",
       "  'OTTER: Improving Zero-Shot Classification via Optimal Transport',\n",
       "  'Toward a Realistic Benchmark for Out-of-Distribution Detection'},\n",
       " 'Understanding and Utilizing Deep Neural Networks Trained with Noisy Labels': {'Can We Break Free from Strong Data Augmentations in Self-Supervised\\n  Learning?',\n",
       "  'DeNetDM: Debiasing by Network Depth Modulation',\n",
       "  'NC-TTT: A Noise Contrastive Approach for Test-Time Training',\n",
       "  'Noise contrastive estimation with soft targets for conditional models',\n",
       "  'Optimizing Calibration by Gaining Aware of Prediction Correctness',\n",
       "  'Positive Unlabeled Contrastive Learning',\n",
       "  'Reactive Model Correction: Mitigating Harm to Task-Relevant Features via\\n  Conditional Bias Suppression',\n",
       "  'Trusted Multi-view Learning with Label Noise'},\n",
       " 'Generating Images with Multimodal Language Models': {'Bridging Vision and Language Spaces with Assignment Prediction',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Cross-Modal Adapter: Parameter-Efficient Transfer Learning Approach for\\n  Vision-Language Models',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept\\n  Understanding',\n",
       "  'Residual-based Language Models are Free Boosters for Biomedical Imaging',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'iRAG: An Incremental Retrieval Augmented Generation System for Videos',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'Selective Mixup Helps with Distribution Shifts, But Not (Only) because of Mixup': {'A Survey of Deep Long-Tail Classification Advancements',\n",
       "  'DeNetDM: Debiasing by Network Depth Modulation',\n",
       "  'Gradient-Regularized Out-of-Distribution Detection',\n",
       "  'Improving Generalization via Meta-Learning on Hard Samples',\n",
       "  'Mining Invariance from Nonlinear Multi-Environment Data: Binary\\n  Classification',\n",
       "  'NC-TTT: A Noise Contrastive Approach for Test-Time Training',\n",
       "  'Noise contrastive estimation with soft targets for conditional models',\n",
       "  'OTTER: Improving Zero-Shot Classification via Optimal Transport'},\n",
       " 'The Future of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence': {'A Self-feedback Knowledge Elicitation Approach for Chemical Reaction\\n  Predictions',\n",
       "  'Decoding AI: The inside story of data analysis in ChatGPT',\n",
       "  'Enhancing Counterfactual Explanation Search with Diffusion Distance and\\n  Directional Coherence',\n",
       "  'Explainable Artificial Intelligence Techniques for Accurate Fault\\n  Detection and Diagnosis: A Review',\n",
       "  'FMint: Bridging Human Designed and Data Pretrained Models for\\n  Differential Equation Foundation Model',\n",
       "  'LLM-Enhanced Causal Discovery in Temporal Domain from Interventional\\n  Data',\n",
       "  'Learning to Solve the Constrained Most Probable Explanation Task in\\n  Probabilistic Graphical Models',\n",
       "  'Mol-AIR: Molecular Reinforcement Learning with Adaptive Intrinsic Rewards for Goal-directed Molecular Generation',\n",
       "  'Sparse Explanations of Neural Networks Using Pruned Layer-Wise Relevance\\n  Propagation',\n",
       "  'The Causal Chambers: Real Physical Systems as a Testbed for AI\\n  Methodology'},\n",
       " 'Auto-Regressive Next-Token Predictors are Universal Learners': {'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'Toward a Theory of Tokenization in LLMs',\n",
       "  'Transformers Can Represent $n$-gram Language Models',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'Neural Graphical Models': {'A Review of Graph Neural Networks in Epidemic Modeling',\n",
       "  'Graph Machine Learning in the Era of Large Language Models (LLMs)',\n",
       "  'Graph Neural Aggregation-diffusion with Metastability',\n",
       "  'Graph Neural Networks for Protein-Protein Interactions - A Short Survey',\n",
       "  'Improving the interpretability of GNN predictions through\\n  conformal-based graph sparsification',\n",
       "  'Multi-View Subgraph Neural Networks: Self-Supervised Learning with\\n  Scarce Labeled Data',\n",
       "  'Time-aware Heterogeneous Graph Transformer with Adaptive Attention\\n  Merging for Health Event Prediction',\n",
       "  'Uncertainty Quantification on Graph Learning: A Survey'},\n",
       " 'TSMixer: An All-MLP Architecture for Time Series Forecasting': {'Computationally and Memory-Efficient Robust Predictive Analytics Using Big Data',\n",
       "  'DeepHeteroIoT: Deep Local and Global Learning over Heterogeneous IoT Sensor Data',\n",
       "  'Feature-Based Echo-State Networks: A Step Towards Interpretability and Minimalism in Reservoir Computer',\n",
       "  'Generating Synthetic Time Series Data for Cyber-Physical Systems',\n",
       "  'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'TSLANet: Rethinking Transformers for Time Series Representation Learning',\n",
       "  'Variational quantization for state space models'},\n",
       " 'Physics of Language Models: Part 1, Context-Free Grammar': {'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'Localizing Paragraph Memorization in Language Models',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Toward a Theory of Tokenization in LLMs',\n",
       "  'Transformers Can Represent $n$-gram Language Models',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model',\n",
       "  'What do Transformers Know about Government?',\n",
       "  'σ-GPTs: A New Approach to Autoregressive Models'},\n",
       " 'Directly Fine-Tuning Diffusion Models on Differentiable Rewards': {'Balanced Mixed-Type Tabular Data Synthesis with Diffusion Models',\n",
       "  'Dataset Reset Policy Optimization for RLHF',\n",
       "  'EdgeFusion: On-Device Text-to-Image Generation',\n",
       "  'Filtered Direct Preference Optimization',\n",
       "  'Gradient Guidance for Diffusion Models: An Optimization Perspective',\n",
       "  'Neural Flow Diffusion Models: Learnable Forward Process for Improved\\n  Diffusion Modelling',\n",
       "  'Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging\\n  Perturbations That Efficiently Fool Customized Diffusion Models',\n",
       "  'Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept\\n  Understanding',\n",
       "  'SparseDM: Toward Sparse Efficient Diffusion Models',\n",
       "  'TMPQ-DM: Joint Timestep Reduction and Quantization Precision Selection\\n  for Efficient Diffusion Models'},\n",
       " 'The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)': {'Bridging Vision and Language Spaces with Assignment Prediction',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'Harnessing GPT-4V(ision) for Insurance: A Preliminary Exploration',\n",
       "  'Pre-trained Vision-Language Models Learn Discoverable Visual Concepts',\n",
       "  'Self-Supervised Visual Preference Alignment',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution': {'Aligning language models with human preferences',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Self-playing Adversarial Language Game Enhances LLM Reasoning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Hypothesis Search: Inductive Reasoning with Language Models': {'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'Pre-trained Vision-Language Models Learn Discoverable Visual Concepts',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning'},\n",
       " 'Representation Engineering: A Top-Down Approach to AI Transparency': {'Closing the Gap in the Trade-off between Fair Representations and\\n  Accuracy',\n",
       "  'Decomposing and Editing Predictions by Modeling Model Computation',\n",
       "  'GenFighter: A Generative and Evolutive Textual Attack Removal',\n",
       "  'Holistic Safety and Responsibility Evaluations of Advanced AI Models',\n",
       "  'Private Attribute Inference from Images with Vision-Language Models',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'RTP-LX: Can LLMs Evaluate Toxicity in Multilingual Scenarios?',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'Self-playing Adversarial Language Game Enhances LLM Reasoning',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Can large language models provide useful feedback on research papers? A large-scale empirical analysis': {'Beyond Code Generation: An Observational Study of ChatGPT Usage in\\n  Software Engineering Practice',\n",
       "  'Bias patterns in the application of LLMs for clinical decision support:\\n  A comprehensive study',\n",
       "  'CT-Agent: Clinical Trial Multi-Agent with Large Language Model-based\\n  Reasoning',\n",
       "  'FedEval-LLM: Federated Evaluation of Large Language Models on Downstream\\n  Tasks with Collective Wisdom',\n",
       "  \"Is ChatGPT Transforming Academics' Writing Style?\",\n",
       "  'Learning representations of learning representations',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'STRUM-LLM: Attributed and Structured Contrastive Summarization'},\n",
       " 'Borges and AI': {'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions',\n",
       "  'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'MUGC: Machine Generated versus User Generated Content Detection',\n",
       "  \"On Large Language Models' Hallucination with Regard to Known Facts\",\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Taxonomy to Regulation: A (Geo)Political Taxonomy for AI Risks and\\n  Regulatory Measures in the EU AI Act',\n",
       "  'Transformers Can Represent $n$-gram Language Models',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation': {'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Automating REST API Postman Test Cases Using LLM',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'LLM-Powered Test Case Generation for Detecting Tricky Bugs',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\\n  Upcycled Mixture-of-Experts'},\n",
       " 'Deep Neural Networks Tend To Extrapolate Predictably': {'A Survey of Deep Long-Tail Classification Advancements',\n",
       "  'Combining Statistical Depth and Fermat Distance for Uncertainty\\n  Quantification',\n",
       "  'Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World',\n",
       "  'Gradient-Regularized Out-of-Distribution Detection',\n",
       "  'Noise contrastive estimation with soft targets for conditional models',\n",
       "  'Reactive Model Correction: Mitigating Harm to Task-Relevant Features via\\n  Conditional Bias Suppression',\n",
       "  'Strong Transferable Adversarial Attacks via Ensembled Asymptotically Normal Distribution Learning',\n",
       "  'Toward a Realistic Benchmark for Out-of-Distribution Detection'},\n",
       " 'How Two-Layer Neural Networks Learn, One (Giant) Step at a Time': {'Deep Neural Networks via Complex Network Theory: a Perspective',\n",
       "  'How Deep Networks Learn Sparse and Hierarchical Data: the Sparse Random\\n  Hierarchy Model',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'Matching the Statistical Query Lower Bound for k-sparse Parity Problems\\n  with Stochastic Gradient Descent',\n",
       "  'Noise contrastive estimation with soft targets for conditional models',\n",
       "  'Sliding down the stairs: how correlated latent variables accelerate\\n  learning with neural networks',\n",
       "  'Strong Transferable Adversarial Attacks via Ensembled Asymptotically Normal Distribution Learning',\n",
       "  'Training neural networks with structured noise improves classification and generalization'},\n",
       " 'Explaining grokking through circuit efficiency': {'Can We Break Free from Strong Data Augmentations in Self-Supervised\\n  Learning?',\n",
       "  'Deep Neural Networks via Complex Network Theory: a Perspective',\n",
       "  'Deep Regression Representation Learning with Topology',\n",
       "  'Delayed Bottlenecking: Alleviating Forgetting in Pre-trained Graph\\n  Neural Networks',\n",
       "  'Localizing Paragraph Memorization in Language Models',\n",
       "  'Mining Invariance from Nonlinear Multi-Environment Data: Binary\\n  Classification',\n",
       "  'On Size and Hardness Generalization in Unsupervised Learning for the Travelling Salesman Problem',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective',\n",
       "  'Training neural networks with structured noise improves classification and generalization'},\n",
       " 'MAPTree: Beating \"Optimal\" Decision Trees with Bayesian Decision Trees': {'AI Competitions and Benchmarks: Dataset Development',\n",
       "  'Application of the representative measure approach to assess the\\n  reliability of decision trees in dealing with unseen vehicle collision data',\n",
       "  'BayesJudge: Bayesian Kernel Language Modelling with Confidence\\n  Uncertainty in Legal Judgment Prediction',\n",
       "  'Explainable LightGBM Approach for Predicting Myocardial Infarction\\n  Mortality',\n",
       "  'Finding Decision Tree Splits in Streaming and Massively Parallel Models',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Tree Bandits for Generative Bayes',\n",
       "  'floZ: Evidence estimation from posterior samples with normalizing flows'},\n",
       " 'Neuro-Modulated Hebbian Learning for Fully Test-Time Adaptation': {'Actor-Critic Reinforcement Learning with Phased Actor',\n",
       "  'ColA: Collaborative Adaptation with Gradient Learning',\n",
       "  'Employing Layerwised Unsupervised Learning to Lessen Data and Loss\\n  Requirements in Forward-Forward Algorithms',\n",
       "  'Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'NC-TTT: A Noise Contrastive Approach for Test-Time Training',\n",
       "  'Reactive Model Correction: Mitigating Harm to Task-Relevant Features via\\n  Conditional Bias Suppression',\n",
       "  'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective'},\n",
       " 'DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines': {'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving',\n",
       "  'Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\\n  Prediction',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'Large Language Models can Learn Rules': {'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  \"On Large Language Models' Hallucination with Regard to Known Facts\",\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction'},\n",
       " 'MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens': {'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Data-Efficient Multimodal Fusion on a Single GPU',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept\\n  Understanding',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'iRAG: An Incremental Retrieval Augmented Generation System for Videos',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'VeRA: Vector-based Random Matrix Adaptation': {'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LoRA Dropout as a Sparsity Regularizer for Overfitting Control',\n",
       "  'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'Personalized Collaborative Fine-Tuning for On-Device Large Language\\n  Models',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages'},\n",
       " 'Context-Aware Meta-Learning': {'Bridging Vision and Language Spaces with Assignment Prediction',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Cross-Modal Adapter: Parameter-Efficient Transfer Learning Approach for\\n  Vision-Language Models',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'RankCLIP: Ranking-Consistent Language-Image Pretraining',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'In-Context Learning for Few-Shot Molecular Property Prediction': {'$\\\\texttt{MiniMol}$: A Parameter-Efficient Foundation Model for Molecular\\n  Learning',\n",
       "  'A Python library for efficient computation of molecular fingerprints',\n",
       "  'A Self-feedback Knowledge Elicitation Approach for Chemical Reaction\\n  Predictions',\n",
       "  'HiGraphDTI: Hierarchical Graph Representation Learning for Drug-Target\\n  Interaction Prediction',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Simple, Efficient and Scalable Structure-aware Adapter Boosts Protein\\n  Language Models',\n",
       "  'When LLMs are Unfit Use FastFit: Fast and Effective Text Classification\\n  with Many Classes'},\n",
       " 'Reset It and Forget It: Relearning Last-Layer Weights Improves Continual and Transfer Learning': {'Can We Break Free from Strong Data Augmentations in Self-Supervised\\n  Learning?',\n",
       "  'ColA: Collaborative Adaptation with Gradient Learning',\n",
       "  'Continual Learning on a Diet: Learning from Sparsely Labeled Streams\\n  Under Constrained Computation',\n",
       "  'Dynamically Anchored Prompting for Task-Imbalanced Continual Learning',\n",
       "  'Improving Generalization via Meta-Learning on Hard Samples',\n",
       "  'NC-TTT: A Noise Contrastive Approach for Test-Time Training',\n",
       "  'OTTER: Improving Zero-Shot Classification via Optimal Transport',\n",
       "  'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective',\n",
       "  'SOPHON: Non-Fine-Tunable Learning to Restrain Task Transferability For\\n  Pre-trained Models',\n",
       "  'Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer'},\n",
       " 'Few-Shot Learning Patterns in Financial Time-Series for Trend-Following Strategies': {'A Comparison of Traditional and Deep Learning Methods for Parameter\\n  Estimation of the Ornstein-Uhlenbeck Process',\n",
       "  'DeepHeteroIoT: Deep Local and Global Learning over Heterogeneous IoT Sensor Data',\n",
       "  'Generating Synthetic Time Series Data for Cyber-Physical Systems',\n",
       "  'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'Reinforcement Learning in Agent-Based Market Simulation: Unveiling Realistic Stylized Facts and Behavior',\n",
       "  'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'TSLANet: Rethinking Transformers for Time Series Representation Learning',\n",
       "  'Variational quantization for state space models'},\n",
       " 'Pseudointelligence: A Unifying Framework for Language Model Evaluation': {'FedEval-LLM: Federated Evaluation of Large Language Models on Downstream\\n  Tasks with Collective Wisdom',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'Stronger Random Baselines for In-Context Learning',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'Transformers Can Represent $n$-gram Language Models',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Retrieving Texts based on Abstract Descriptions': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'Conversational Financial Information Retrieval Model (ConFIRM)',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering'},\n",
       " 'Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence': {'GenFighter: A Generative and Evolutive Textual Attack Removal',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging\\n  Perturbations That Efficiently Fool Customized Diffusion Models',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'Talk Too Much: Poisoning Large Language Models under Token Limit',\n",
       "  'Toward a Theory of Tokenization in LLMs'},\n",
       " 'Attention over pre-trained Sentence Embeddings for Long Document Classification': {'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'LayerNorm: A key component in parameter-efficient fine-tuning',\n",
       "  'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey'},\n",
       " 'The Consensus Game: Language Model Generation via Equilibrium Search': {'Aligning language models with human preferences',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning',\n",
       "  'Towards a Robust Retrieval-Based Summarization System',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Branch-Solve-Merge Improves Large Language Model Evaluation and Generation': {'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Language Model Prompt Selection via Simulation Optimization',\n",
       "  'Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level\\n  Knowledge Distillation',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning',\n",
       "  'Towards a Robust Retrieval-Based Summarization System'},\n",
       " 'What Algorithms can Transformers Learn? A Study in Length Generalization': {'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'Toward a Theory of Tokenization in LLMs',\n",
       "  'Transformers Can Represent $n$-gram Language Models',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'What do Transformers Know about Government?'},\n",
       " 'In-Context Learning Creates Task Vectors': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy\\n  Data',\n",
       "  'Stronger Random Baselines for In-Context Learning',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'PaRaDe: Passage Ranking using Demonstrations with Large Language Models': {'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Language Model Prompt Selection via Simulation Optimization',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'Stronger Random Baselines for In-Context Learning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards a Robust Retrieval-Based Summarization System',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions': {'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  \"On Large Language Models' Hallucination with Regard to Known Facts\",\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents': {'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards a Robust Retrieval-Based Summarization System'},\n",
       " 'Papeos: Augmenting Research Papers with Talk Videos': {'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Beyond Code Generation: An Observational Study of ChatGPT Usage in\\n  Software Engineering Practice',\n",
       "  'Exploring Text-to-Motion Generation with Human Preference',\n",
       "  \"Is ChatGPT Transforming Academics' Writing Style?\",\n",
       "  'STRUM-LLM: Attributed and Structured Contrastive Summarization',\n",
       "  'SuperNOVA: Design Strategies and Opportunities for Interactive Visualization in Computational Notebooks',\n",
       "  'iRAG: An Incremental Retrieval Augmented Generation System for Videos',\n",
       "  'iTBLS: A Dataset of Interactive Conversations Over Tabular Information'},\n",
       " 'Codebook Features: Sparse and Discrete Interpretability for Neural Networks': {'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'Neuro-Inspired Information-Theoretic Hierarchical Perception for\\n  Multimodal Learning',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'Tripod: Three Complementary Inductive Biases for Disentangled\\n  Representation Learning'},\n",
       " 'How do Language Models Bind Entities in Context?': {'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Localizing Paragraph Memorization in Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'Semantic Cells: Evolutional Process to Acquire Sense Diversity of Items',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'Transformers Can Represent $n$-gram Language Models',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'CodeFusion: A Pre-trained Diffusion Model for Code Generation': {'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Automated Multi-Language to English Machine Translation Using Generative\\n  Pre-Trained Transformers',\n",
       "  'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level\\n  Knowledge Distillation',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\\n  Upcycled Mixture-of-Experts',\n",
       "  'σ-GPTs: A New Approach to Autoregressive Models'},\n",
       " 'AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation': {'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Automating REST API Postman Test Cases Using LLM',\n",
       "  'Beyond Code Generation: An Observational Study of ChatGPT Usage in\\n  Software Engineering Practice',\n",
       "  'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'ReALM: Reference Resolution As Language Modeling',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'iTBLS: A Dataset of Interactive Conversations Over Tabular Information'},\n",
       " 'Universal Self-Adaptive Prompting': {'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'Stronger Random Baselines for In-Context Learning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'Beyond Words: A Mathematical Framework for Interpreting Large Language Models': {'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Detecting and Mitigating Hallucination in Large Vision Language Models\\n  via Fine-Grained AI Feedback',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  \"On Large Language Models' Hallucination with Regard to Known Facts\",\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling': {'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey',\n",
       "  'TriForce: Lossless Acceleration of Long Sequence Generation with\\n  Hierarchical Speculative Decoding'},\n",
       " 'Unraveling the ARC Puzzle: Mimicking Human Solutions with Object-Centric Decision Transformer': {'ASID: Active Exploration for System Identification in Robotic\\n  Manipulation',\n",
       "  'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V',\n",
       "  'Decoding AI: The inside story of data analysis in ChatGPT',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function'},\n",
       " 'LILO: Learning Interpretable Libraries by Compressing and Documenting Code': {'Automated Multi-Language to English Machine Translation Using Generative\\n  Pre-Trained Transformers',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\\n  Upcycled Mixture-of-Experts'},\n",
       " 'Meta-learning of semi-supervised learning from tasks with heterogeneous attribute spaces': {'Beyond the Known: Novel Class Discovery for Open-world Graph Learning',\n",
       "  'DeNetDM: Debiasing by Network Depth Modulation',\n",
       "  'Improving Generalization via Meta-Learning on Hard Samples',\n",
       "  'Integration of Self-Supervised BYOL in Semi-Supervised Medical Image\\n  Recognition',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'Meta-Learning with Generalized Ridge Regression: High-dimensional Asymptotics, Optimality and Hyper-covariance Estimation',\n",
       "  'OTTER: Improving Zero-Shot Classification via Optimal Transport',\n",
       "  'VCC-INFUSE: Towards Accurate and Efficient Selection of Unlabeled\\n  Examples in Semi-supervised Learning'},\n",
       " 'Dense Passage Retrieval for Open-Domain Question Answering': {'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Find The Gap: Knowledge Base Reasoning For Visual Question Answering',\n",
       "  'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking',\n",
       "  'Latxa: An Open Language Model and Evaluation Suite for Basque',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'QAGCN: Answering Multi-Relation Questions via Single-Step Implicit Reasoning over Knowledge Graphs',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'Small Models Are (Still) Effective Cross-Domain Argument Extractors'},\n",
       " 'Agent Lumos: Unified and Modular Training for Open-Source Language Agents': {'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Inferring Behavior-Specific Context Improves Zero-Shot Generalization in\\n  Reinforcement Learning',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'Evaluation of Neural Architectures Trained with Square Loss vs Cross-Entropy in Classification Tasks': {'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'NC-TTT: A Noise Contrastive Approach for Test-Time Training',\n",
       "  'Neuro-Inspired Information-Theoretic Hierarchical Perception for\\n  Multimodal Learning',\n",
       "  'Next Generation Loss Function for Image Classification',\n",
       "  'Noise contrastive estimation with soft targets for conditional models',\n",
       "  'Nonlinearity Enhanced Adaptive Activation Function',\n",
       "  'PQA: Exploring the Potential of Product Quantization in DNN Hardware Acceleration',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'Toward a Theory of Tokenization in LLMs'},\n",
       " 'The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4': {'A Self-feedback Knowledge Elicitation Approach for Chemical Reaction\\n  Predictions',\n",
       "  'AlloyBERT: Alloy Property Prediction with Large Language Models',\n",
       "  'CT-Agent: Clinical Trial Multi-Agent with Large Language Model-based\\n  Reasoning',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'On the Scalability of GNNs for Molecular Graphs',\n",
       "  'Simple, Efficient and Scalable Structure-aware Adapter Boosts Protein\\n  Language Models',\n",
       "  'Towards a Foundation Model for Partial Differential Equation:\\n  Multi-Operator Learning and Extrapolation'},\n",
       " 'Cappy: Outperforming and Boosting Large Multi-Task LMs with a Small Scorer': {'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models'},\n",
       " 'Ghostbuster: Detecting Text Ghostwritten by Large Language Models': {'AI-Generated Faces in the Real World: A Large-Scale Case Study of\\n  Twitter Profile Images',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Does It Make Sense to Explain a Black Box With Another Black Box?',\n",
       "  'GenFighter: A Generative and Evolutive Textual Attack Removal',\n",
       "  'Learning representations of learning representations',\n",
       "  'MUGC: Machine Generated versus User Generated Content Detection',\n",
       "  'ParaFusion: A Large-Scale LLM-Driven English Paraphrase Dataset Infused\\n  with High-Quality Lexical and Syntactic Diversity',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'Robust CLIP-Based Detector for Exposing Diffusion Model-Generated Images',\n",
       "  'Talk Too Much: Poisoning Large Language Models under Token Limit'},\n",
       " 'Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks': {'Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second': {'Decomposing and Editing Predictions by Modeling Model Computation',\n",
       "  'Distilled Datamodel with Reverse Gradient Matching',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Learning to Solve the Constrained Most Probable Explanation Task in\\n  Probabilistic Graphical Models',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'Measuring Feature Dependency of Neural Networks by Collapsing Feature\\n  Dimensions in the Data Manifold',\n",
       "  'OTTER: Improving Zero-Shot Classification via Optimal Transport',\n",
       "  'Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science'},\n",
       " 'When Do Neural Nets Outperform Boosted Trees on Tabular Data?': {'A Survey of Deep Long-Tail Classification Advancements',\n",
       "  'AI Competitions and Benchmarks: Dataset Development',\n",
       "  'Application of the representative measure approach to assess the\\n  reliability of decision trees in dealing with unseen vehicle collision data',\n",
       "  'DeNetDM: Debiasing by Network Depth Modulation',\n",
       "  'Distilled Datamodel with Reverse Gradient Matching',\n",
       "  'Improving Generalization via Meta-Learning on Hard Samples',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Structured Evaluation of Synthetic Tabular Data',\n",
       "  'Towards Stable Machine Learning Model Retraining via Slowly Varying Sequences',\n",
       "  'Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science'},\n",
       " 'Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models': {'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing'},\n",
       " 'Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as an Alternative to Attention Layers in Transformers': {'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Enhancing Efficiency in Vision Transformer Networks: Design Techniques and Insights',\n",
       "  'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers',\n",
       "  'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey'},\n",
       " 'System 2 Attention (is something you might need too)': {'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning'},\n",
       " 'RA-DIT: Retrieval-Augmented Dual Instruction Tuning': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering'},\n",
       " 'MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning': {'Bias patterns in the application of LLMs for clinical decision support:\\n  A comprehensive study',\n",
       "  'CT-Agent: Clinical Trial Multi-Agent with Large Language Model-based\\n  Reasoning',\n",
       "  'Data Alignment for Zero-Shot Concept Generation in Dermatology AI',\n",
       "  'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'FedEval-LLM: Federated Evaluation of Large Language Models on Downstream\\n  Tasks with Collective Wisdom',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'Would You Trust an AI Doctor? Building Reliable Medical Predictions with\\n  Kernel Dropout Uncertainty'},\n",
       " 'Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models': {'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LayerNorm: A key component in parameter-efficient fine-tuning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'Stronger Random Baselines for In-Context Learning',\n",
       "  'Toward a Theory of Tokenization in LLMs',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'Text Embeddings Reveal (Almost) As Much As Text': {'Distributional Black-Box Model Inversion Attack with Multi-Agent\\n  Reinforcement Learning',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'PATE-TripleGAN: Privacy-Preserving Image Synthesis with Gaussian\\n  Differential Privacy',\n",
       "  'Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging\\n  Perturbations That Efficiently Fool Customized Diffusion Models',\n",
       "  'Private Attribute Inference from Images with Vision-Language Models',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'VFLGAN: Vertical Federated Learning-based Generative Adversarial Network\\n  for Vertically Partitioned Data Publication'},\n",
       " 'Scaling TabPFN: Sketching and Feature Selection for Tabular Prior-Data Fitted Networks': {'An improved tabular data generator with VAE-GMM integration',\n",
       "  'Application of the representative measure approach to assess the\\n  reliability of decision trees in dealing with unseen vehicle collision data',\n",
       "  'Deep Regression Representation Learning with Topology',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Measuring Feature Dependency of Neural Networks by Collapsing Feature\\n  Dimensions in the Data Manifold',\n",
       "  'Structured Evaluation of Synthetic Tabular Data',\n",
       "  'Synthesizing Realistic Data for Table Recognition',\n",
       "  'Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science'},\n",
       " 'Fine-Tuning the Retrieval Mechanism for Tabular Deep Learning': {'Delayed Bottlenecking: Alleviating Forgetting in Pre-trained Graph\\n  Neural Networks',\n",
       "  'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'FR-NAS: Forward-and-Reverse Graph Predictor for Efficient Neural\\n  Architecture Search',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'Structured Evaluation of Synthetic Tabular Data',\n",
       "  'Synthesizing Realistic Data for Table Recognition',\n",
       "  'Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science'},\n",
       " 'XTab: Cross-table Pretraining for Tabular Transformers': {'An improved tabular data generator with VAE-GMM integration',\n",
       "  'Can We Break Free from Strong Data Augmentations in Self-Supervised\\n  Learning?',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'One-Shot Sequential Federated Learning for Non-IID Data by Enhancing\\n  Local Model Diversity',\n",
       "  'Structured Evaluation of Synthetic Tabular Data',\n",
       "  'Synthesizing Realistic Data for Table Recognition',\n",
       "  'Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science'},\n",
       " 'Deep incremental learning models for financial temporal tabular datasets with distribution shifts': {'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting',\n",
       "  'Integrating Marketing Channels into Quantile Transformation and Bayesian\\n  Optimization of Ensemble Kernels for Sales Prediction with Gaussian Process\\n  Models',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'Mining Invariance from Nonlinear Multi-Environment Data: Binary\\n  Classification',\n",
       "  'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'TSLANet: Rethinking Transformers for Time Series Representation Learning',\n",
       "  'Towards Stable Machine Learning Model Retraining via Slowly Varying Sequences',\n",
       "  'Using ARIMA to Predict the Expansion of Subscriber Data Consumption',\n",
       "  'Variational quantization for state space models'},\n",
       " 'Deep Regression Ensembles': {'Can We Break Free from Strong Data Augmentations in Self-Supervised\\n  Learning?',\n",
       "  'DeNetDM: Debiasing by Network Depth Modulation',\n",
       "  'Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World',\n",
       "  'Gradient-Regularized Out-of-Distribution Detection',\n",
       "  'Measuring Feature Dependency of Neural Networks by Collapsing Feature\\n  Dimensions in the Data Manifold',\n",
       "  'Reactive Model Correction: Mitigating Harm to Task-Relevant Features via\\n  Conditional Bias Suppression'},\n",
       " 'War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars': {'A survey of air combat behavior modeling using machine learning',\n",
       "  'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'LLM-Enhanced Causal Discovery in Temporal Domain from Interventional\\n  Data',\n",
       "  'Self-playing Adversarial Language Game Enhances LLM Reasoning',\n",
       "  'Social Choice for AI Alignment: Dealing with Diverse Human Feedback',\n",
       "  'Taxonomy to Regulation: A (Geo)Political Taxonomy for AI Risks and\\n  Regulatory Measures in the EU AI Act',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'Using LLMs to Model the Beliefs and Preferences of Targeted Populations'},\n",
       " 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces': {'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey'},\n",
       " 'The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning': {'Aligning language models with human preferences',\n",
       "  'Learn Your Reference Model for Real Good Alignment',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy\\n  Data',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'SciRepEval: A Multi-Format Benchmark for Scientific Document Representations': {'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking',\n",
       "  'Learning Word Embedding with Better Distance Weighting and Window Size\\n  Scheduling',\n",
       "  'Learning representations of learning representations',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'STRUM-LLM: Attributed and Structured Contrastive Summarization',\n",
       "  'Simple, Efficient and Scalable Structure-aware Adapter Boosts Protein\\n  Language Models',\n",
       "  'Sparse multimodal fusion with modal channel attention'},\n",
       " 'Generalization to New Sequential Decision Making Tasks with In-Context Learning': {'Adversarial Imitation Learning via Boosting',\n",
       "  'Continual Offline Reinforcement Learning via Diffusion-based Dual\\n  Generative Replay',\n",
       "  'Decision Mamba: Reinforcement Learning via Sequence Modeling with Selective State Spaces',\n",
       "  'Deep Policy Optimization with Temporal Logic Constraints',\n",
       "  'Dynamically Anchored Prompting for Task-Imbalanced Continual Learning',\n",
       "  'Higher Replay Ratio Empowers Sample-Efficient Multi-Agent Reinforcement\\n  Learning',\n",
       "  'Inferring Behavior-Specific Context Improves Zero-Shot Generalization in\\n  Reinforcement Learning',\n",
       "  'Meta Reinforcement Learning with Finite Training Tasks -- a Density Estimation Approach',\n",
       "  'Offline Trajectory Generalization for Offline Reinforcement Learning',\n",
       "  'Zero-Shot Stitching in Reinforcement Learning using Relative\\n  Representations'},\n",
       " 'Errors are Useful Prompts: Instruction Guided Task Programming with Verifier-Assisted Iterative Prompting': {'A Self-feedback Knowledge Elicitation Approach for Chemical Reaction\\n  Predictions',\n",
       "  'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Automating REST API Postman Test Cases Using LLM',\n",
       "  'Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V',\n",
       "  'Concept-based Analysis of Neural Networks via Vision-Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'iTBLS: A Dataset of Interactive Conversations Over Tabular Information'},\n",
       " 'AI and Jobs: Has the Inflection Point Arrived? Evidence from an Online Labor Platform': {'AI Competitions and Benchmarks: Dataset Development',\n",
       "  'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions',\n",
       "  'An Economic Solution to Copyright Challenges of Generative AI',\n",
       "  'Beyond Code Generation: An Observational Study of ChatGPT Usage in\\n  Software Engineering Practice',\n",
       "  'Decoding AI: The inside story of data analysis in ChatGPT',\n",
       "  'Enhancing Fairness and Performance in Machine Learning Models: A\\n  Multi-Task Learning Approach with Monte-Carlo Dropout and Pareto Optimality',\n",
       "  'Fair Concurrent Training of Multiple Models in Federated Learning',\n",
       "  'LLM-Enhanced Causal Discovery in Temporal Domain from Interventional\\n  Data',\n",
       "  'Taxonomy to Regulation: A (Geo)Political Taxonomy for AI Risks and\\n  Regulatory Measures in the EU AI Act',\n",
       "  'The Impact of Machine Learning on Society: An Analysis of Current Trends\\n  and Future Implications'},\n",
       " 'Dense X Retrieval: What Retrieval Granularity Should We Use?': {'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Conversational Financial Information Retrieval Model (ConFIRM)',\n",
       "  'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'Small Models Are (Still) Effective Cross-Domain Argument Extractors',\n",
       "  'Towards a Robust Retrieval-Based Summarization System'},\n",
       " 'MotherNet: A Foundational Hypernetwork for Tabular Classification': {'Distilled Datamodel with Reverse Gradient Matching',\n",
       "  'Employing Layerwised Unsupervised Learning to Lessen Data and Loss\\n  Requirements in Forward-Forward Algorithms',\n",
       "  'Engineering software 2.0 by interpolating neural networks: unifying\\n  training, solving, and calibration',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Improving Generalization via Meta-Learning on Hard Samples',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'Noise contrastive estimation with soft targets for conditional models',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science'},\n",
       " 'Weight subcloning: direct initialization of transformers using larger pretrained ones': {'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Efficient Transformer Encoders for Mask2Former-style models',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LayerNorm: A key component in parameter-efficient fine-tuning',\n",
       "  'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Residual-based Language Models are Free Boosters for Biomedical Imaging',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey'},\n",
       " 'A mathematical perspective on Transformers': {'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'Localizing Paragraph Memorization in Language Models',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey',\n",
       "  'Toward a Theory of Tokenization in LLMs',\n",
       "  'Transformers Can Represent $n$-gram Language Models',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'What do Transformers Know about Government?'},\n",
       " 'Perspectives on the State and Future of Deep Learning - 2023': {'AI Competitions and Benchmarks: Dataset Development',\n",
       "  'Closing the Gap in the Trade-off between Fair Representations and\\n  Accuracy',\n",
       "  'DXAI: Explaining Classification by Image Decomposition',\n",
       "  'Does It Make Sense to Explain a Black Box With Another Black Box?',\n",
       "  'Enhancing Counterfactual Explanation Search with Diffusion Distance and\\n  Directional Coherence',\n",
       "  'Explainable Artificial Intelligence Techniques for Accurate Fault\\n  Detection and Diagnosis: A Review',\n",
       "  'Gradient strikes back: How filtering out high frequencies improves explanations',\n",
       "  'Learning representations of learning representations',\n",
       "  'MUGC: Machine Generated versus User Generated Content Detection',\n",
       "  'Toward Understanding the Disagreement Problem in Neural Network Feature\\n  Attribution'},\n",
       " 'Retrieval-Augmented Generation for Large Language Models: A Survey': {'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'Towards a Robust Retrieval-Based Summarization System',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'Time is Encoded in the Weights of Finetuned Language Models': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'Localizing Paragraph Memorization in Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'Stronger Random Baselines for In-Context Learning',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'Recasting Continual Learning as Sequence Modeling': {'Continual Learning on a Diet: Learning from Sparsely Labeled Streams\\n  Under Constrained Computation',\n",
       "  'Decision Mamba: Reinforcement Learning via Sequence Modeling with Selective State Spaces',\n",
       "  'Dynamically Anchored Prompting for Task-Imbalanced Continual Learning',\n",
       "  'FedMeS: Personalized Federated Continual Learning Leveraging Local\\n  Memory',\n",
       "  'Improving Generalization via Meta-Learning on Hard Samples',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective',\n",
       "  'Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer',\n",
       "  'Toward a Theory of Tokenization in LLMs'},\n",
       " \"An In-depth Look at Gemini's Language Abilities\": {'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning': {'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V',\n",
       "  'Deep Policy Optimization with Temporal Logic Constraints',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning'},\n",
       " 'DBOS: A Proposal for a Data-Centric Operating System': {'AntDT: A Self-Adaptive Distributed Training Framework for Leader and\\n  Straggler Nodes',\n",
       "  'Apodotiko: Enabling Efficient Serverless Federated Learning in\\n  Heterogeneous Environments',\n",
       "  'Confidential Federated Computations',\n",
       "  'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models',\n",
       "  'Dynamic Frequency-Based Fingerprinting Attacks against Modern Sandbox\\n  Environments',\n",
       "  'End-to-End Verifiable Decentralized Federated Learning',\n",
       "  'FedFa: A Fully Asynchronous Training Paradigm for Federated Learning',\n",
       "  'I/O in Machine Learning Applications on HPC Systems: A 360-degree Survey',\n",
       "  'Privacy-Preserving Intrusion Detection using Convolutional Neural\\n  Networks',\n",
       "  'Privacy-Preserving Training-as-a-Service for On-Device Intelligence:\\n  Concept, Architectural Scheme, and Open Problems'},\n",
       " 'Mindstorms in Natural Language-Based Societies of Mind': {'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions',\n",
       "  'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Social Choice for AI Alignment: Dealing with Diverse Human Feedback',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing'},\n",
       " 'TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones': {'Bridging Vision and Language Spaces with Assignment Prediction',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Cross-Modal Adapter: Parameter-Efficient Transfer Learning Approach for\\n  Vision-Language Models',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Find The Gap: Knowledge Base Reasoning For Visual Question Answering',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'RankCLIP: Ranking-Consistent Language-Image Pretraining',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'Fast Inference of Mixture-of-Experts Language Models with Offloading': {'Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\\n  Prediction',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LD-Pruner: Efficient Pruning of Latent Diffusion Models using\\n  Task-Agnostic Insights',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'TriForce: Lossless Acceleration of Long Sequence Generation with\\n  Hierarchical Speculative Decoding'},\n",
       " 'Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language Models': {'Bridging Vision and Language Spaces with Assignment Prediction',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Find The Gap: Knowledge Base Reasoning For Visual Question Answering',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Self-Supervised Visual Preference Alignment',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'LLM Augmented LLMs: Expanding Capabilities through Composition': {'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'Mixtral of Experts': {'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving',\n",
       "  'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'Multi-Head Mixture-of-Experts',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'TriForce: Lossless Acceleration of Long Sequence Generation with\\n  Hierarchical Speculative Decoding'},\n",
       " 'ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'LayerNorm: A key component in parameter-efficient fine-tuning',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering'},\n",
       " 'Transformers are Multi-State RNNs': {'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling'},\n",
       " 'GenCast: Diffusion-based ensemble forecasting for medium-range weather': {'Four-hour thunderstorm nowcasting using deep diffusion models of\\n  satellite',\n",
       "  'Fuxi-DA: A Generalized Deep Learning Data Assimilation Framework for\\n  Assimilating Satellite Observations',\n",
       "  'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting',\n",
       "  'Probabilistic forecasting of power system imbalance using neural\\n  network-based ensembles',\n",
       "  'Recurrent Neural Networks for Modelling Gross Primary Production',\n",
       "  'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion',\n",
       "  'Scalable Data Assimilation with Message Passing',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'Toward Routing River Water in Land Surface Models with Recurrent Neural\\n  Networks',\n",
       "  'Uncertainty Aware Tropical Cyclone Wind Speed Estimation from Satellite\\n  Data'},\n",
       " 'The Unreasonable Effectiveness of Easy Training Data for Hard Tasks': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Latxa: An Open Language Model and Evaluation Suite for Basque',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'Stronger Random Baselines for In-Context Learning',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy': {'A multiobjective continuation method to compute the regularization path of deep neural networks',\n",
       "  'DeNetDM: Debiasing by Network Depth Modulation',\n",
       "  'Delayed Bottlenecking: Alleviating Forgetting in Pre-trained Graph\\n  Neural Networks',\n",
       "  'Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World',\n",
       "  'Graph Neural Aggregation-diffusion with Metastability',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'Multi-Head Mixture-of-Experts',\n",
       "  'Neuro-Inspired Information-Theoretic Hierarchical Perception for\\n  Multimodal Learning',\n",
       "  'Separate, Dynamic and Differentiable (SMART) Pruner for Block/Output Channel Pruning on Computer Vision Tasks'},\n",
       " 'Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering': {'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Automating REST API Postman Test Cases Using LLM',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'LLM-Powered Test Case Generation for Detecting Tricky Bugs',\n",
       "  'Language Model Prompt Selection via Simulation Optimization',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\\n  Upcycled Mixture-of-Experts'},\n",
       " 'Foundations of Vector Retrieval': {'Bridging Vision and Language Spaces with Assignment Prediction',\n",
       "  'GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers',\n",
       "  'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'VDTuner: Automated Performance Tuning for Vector Data Management Systems',\n",
       "  'iRAG: An Incremental Retrieval Augmented Generation System for Videos'},\n",
       " 'An Emulator for Fine-Tuning Large Language Models using Small Language Models': {'Aligning language models with human preferences',\n",
       "  'Filtered Direct Preference Optimization',\n",
       "  'Learn Your Reference Model for Real Good Alignment',\n",
       "  'Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy\\n  Data',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Tuning Language Models by Proxy': {'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LayerNorm: A key component in parameter-efficient fine-tuning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy\\n  Data',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model',\n",
       "  'XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\\n  Upcycled Mixture-of-Experts'},\n",
       " 'Context is Environment': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Stronger Random Baselines for In-Context Learning',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'Self-Rewarding Language Models': {'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Filtered Direct Preference Optimization',\n",
       "  'From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function',\n",
       "  'Generalized Population-Based Training for Hyperparameter Optimization in\\n  Reinforcement Learning',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Self-playing Adversarial Language Game Enhances LLM Reasoning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning': {'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving',\n",
       "  'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models'},\n",
       " 'InRanker: Distilled Rankers for Zero-shot Information Retrieval': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'Estimating the Hessian Matrix of Ranking Objectives for Stochastic\\n  Learning to Rank with Gradient Boosted Trees',\n",
       "  'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering'},\n",
       " 'Explaining Anomalies using Denoising Autoencoders for Financial Tabular Data': {'A Customer Level Fraudulent Activity Detection Benchmark for Enhancing\\n  Machine Learning Model Research and Evaluation',\n",
       "  'An improved tabular data generator with VAE-GMM integration',\n",
       "  'Anomaly Correction of Business Processes Using Transformer Autoencoder',\n",
       "  'BayesJudge: Bayesian Kernel Language Modelling with Confidence\\n  Uncertainty in Legal Judgment Prediction',\n",
       "  'DACAD: Domain Adaptation Contrastive Learning for Anomaly Detection in\\n  Multivariate Time Series',\n",
       "  'Explainable Artificial Intelligence Techniques for Accurate Fault\\n  Detection and Diagnosis: A Review',\n",
       "  'Fin-Fed-OD: Federated Outlier Detection on Financial Tabular Data',\n",
       "  'Gradient strikes back: How filtering out high frequencies improves explanations',\n",
       "  'Sparse Explanations of Neural Networks Using Pruned Layer-Wise Relevance\\n  Propagation',\n",
       "  'Toward Understanding the Disagreement Problem in Neural Network Feature\\n  Attribution'},\n",
       " 'Learning to Reweight Examples for Robust Deep Learning': {'A Survey of Deep Long-Tail Classification Advancements',\n",
       "  'DeNetDM: Debiasing by Network Depth Modulation',\n",
       "  'Distilled Datamodel with Reverse Gradient Matching',\n",
       "  'Gradient-Regularized Out-of-Distribution Detection',\n",
       "  'Improving Generalization via Meta-Learning on Hard Samples',\n",
       "  'NC-TTT: A Noise Contrastive Approach for Test-Time Training',\n",
       "  'OTTER: Improving Zero-Shot Classification via Optimal Transport',\n",
       "  'Positive Unlabeled Contrastive Learning',\n",
       "  'Reactive Model Correction: Mitigating Harm to Task-Relevant Features via\\n  Conditional Bias Suppression',\n",
       "  'SOPHON: Non-Fine-Tunable Learning to Restrain Task Transferability For\\n  Pre-trained Models'},\n",
       " 'In-Context Learning for Extreme Multi-Label Classification': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy\\n  Data',\n",
       "  'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages',\n",
       "  'Stronger Random Baselines for In-Context Learning',\n",
       "  'When LLMs are Unfit Use FastFit: Fast and Effective Text Classification\\n  with Many Classes'},\n",
       " 'Introducing Neural Bag of Whole-Words with ColBERTer: Contextualized Late Interactions using Enhanced Reduction': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'Conversational Financial Information Retrieval Model (ConFIRM)',\n",
       "  'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'VDTuner: Automated Performance Tuning for Vector Data Management Systems'},\n",
       " 'Progress measures for grokking via mechanistic interpretability': {'DXAI: Explaining Classification by Image Decomposition',\n",
       "  'Deep Learning as Ricci Flow',\n",
       "  'Deep Neural Networks via Complex Network Theory: a Perspective',\n",
       "  'Engineering software 2.0 by interpolating neural networks: unifying\\n  training, solving, and calibration',\n",
       "  'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'Listen to the Waves: Using a Neuronal Model of the Human Auditory System\\n  to Predict Ocean Waves',\n",
       "  'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective',\n",
       "  'Training neural networks with structured noise improves classification and generalization'},\n",
       " 'ChatQA: Building GPT-4 Level Conversational QA Models': {'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Towards a Robust Retrieval-Based Summarization System'},\n",
       " 'Qlib: An AI-oriented Quantitative Investment Platform': {'AI Competitions and Benchmarks: Dataset Development',\n",
       "  'Decision Mamba: Reinforcement Learning via Sequence Modeling with Selective State Spaces',\n",
       "  'Digital Twins for forecasting and decision optimisation with machine\\n  learning: applications in wastewater treatment',\n",
       "  'OptiGrad: A Fair and more Efficient Price Elasticity Optimization via a\\n  Gradient Based Learning',\n",
       "  'Reinforcement Learning in Agent-Based Market Simulation: Unveiling Realistic Stylized Facts and Behavior',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'The New Agronomists: Language Models are Experts in Crop Management',\n",
       "  'Towards Stable Machine Learning Model Retraining via Slowly Varying Sequences',\n",
       "  'Using ARIMA to Predict the Expansion of Subscriber Data Consumption',\n",
       "  'VC Theory for Inventory Policies'},\n",
       " 'Matryoshka Representation Learning': {'Bridging Vision and Language Spaces with Assignment Prediction',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'RankCLIP: Ranking-Consistent Language-Image Pretraining',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding': {'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Beyond Code Generation: An Observational Study of ChatGPT Usage in\\n  Software Engineering Practice',\n",
       "  'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'iTBLS: A Dataset of Interactive Conversations Over Tabular Information'},\n",
       " 'MambaTab: A Simple Yet Effective Approach for Handling Tabular Data': {'An improved tabular data generator with VAE-GMM integration',\n",
       "  'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey',\n",
       "  'Structured Evaluation of Synthetic Tabular Data',\n",
       "  'Synthesizing Realistic Data for Table Recognition',\n",
       "  'Towards Universal Performance Modeling for Machine Learning Training on\\n  Multi-GPU Platforms',\n",
       "  'Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science'},\n",
       " 'ReGAL: Refactoring Programs to Discover Generalizable Abstractions': {'Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\\n  Prediction',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LLM-Powered Test Case Generation for Detecting Tricky Bugs',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\\n  Upcycled Mixture-of-Experts'},\n",
       " 'Learning Transformer Programs': {'A Learning Paradigm for Interpretable Gradients',\n",
       "  'AlloyBERT: Alloy Property Prediction with Large Language Models',\n",
       "  'DXAI: Explaining Classification by Image Decomposition',\n",
       "  'Decomposing and Editing Predictions by Modeling Model Computation',\n",
       "  'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'Gradient strikes back: How filtering out high frequencies improves explanations',\n",
       "  'LayerNorm: A key component in parameter-efficient fine-tuning',\n",
       "  'Sparse Explanations of Neural Networks Using Pruned Layer-Wise Relevance\\n  Propagation'},\n",
       " 'RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'Towards a Robust Retrieval-Based Summarization System'},\n",
       " 'Learning Universal Predictors': {'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'Improving Generalization via Meta-Learning on Hard Samples',\n",
       "  'MPC of Uncertain Nonlinear Systems with Meta-Learning for Fast\\n  Adaptation of Neural Predictive Models',\n",
       "  'Meta Reinforcement Learning with Finite Training Tasks -- a Density Estimation Approach',\n",
       "  'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective',\n",
       "  'Training neural networks with structured noise improves classification and generalization'},\n",
       " 'DoubleEnsemble: A New Ensemble Method Based on Sample Reweighting and Feature Selection for Financial Data Analysis': {'A Guide to Feature Importance Methods for Scientific Inference',\n",
       "  'A Survey of Deep Long-Tail Classification Advancements',\n",
       "  'A novel decision fusion approach for sale price prediction using Elastic Net and MOPSO',\n",
       "  'Application of the representative measure approach to assess the\\n  reliability of decision trees in dealing with unseen vehicle collision data',\n",
       "  'Explainable LightGBM Approach for Predicting Myocardial Infarction\\n  Mortality',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'TSLANet: Rethinking Transformers for Time Series Representation Learning',\n",
       "  'Towards Stable Machine Learning Model Retraining via Slowly Varying Sequences'},\n",
       " 'Time-LLM: Time Series Forecasting by Reprogramming Large Language Models': {'DeepHeteroIoT: Deep Local and Global Learning over Heterogeneous IoT Sensor Data',\n",
       "  'Generating Synthetic Time Series Data for Cyber-Physical Systems',\n",
       "  'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'TSLANet: Rethinking Transformers for Time Series Representation Learning',\n",
       "  'Variational quantization for state space models',\n",
       "  'σ-GPTs: A New Approach to Autoregressive Models'},\n",
       " 'DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines': {'Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\\n  Prediction',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'LLM-Powered Test Case Generation for Detecting Tricky Bugs',\n",
       "  'Language Model Prompt Selection via Simulation Optimization',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path\\n  Forward',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards a Robust Retrieval-Based Summarization System'},\n",
       " 'A decoder-only foundation model for time-series forecasting': {'Advancing Long-Term Multi-Energy Load Forecasting with Patchformer: A\\n  Patch and Transformer-Based Approach',\n",
       "  'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting',\n",
       "  'LayerNorm: A key component in parameter-efficient fine-tuning',\n",
       "  'Learning representations of learning representations',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods'},\n",
       " 'A Time Series is Worth 64 Words: Long-term Forecasting with Transformers': {'Advancing Long-Term Multi-Energy Load Forecasting with Patchformer: A\\n  Patch and Transformer-Based Approach',\n",
       "  'DeepHeteroIoT: Deep Local and Global Learning over Heterogeneous IoT Sensor Data',\n",
       "  'Generating Synthetic Time Series Data for Cyber-Physical Systems',\n",
       "  'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'TSLANet: Rethinking Transformers for Time Series Representation Learning',\n",
       "  'Variational quantization for state space models'},\n",
       " 'TransTab: Learning Transferable Tabular Transformers Across Tables': {'An improved tabular data generator with VAE-GMM integration',\n",
       "  'Guided Discrete Diffusion for Electronic Health Record Generation',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'Machine Learning Techniques for MRI Data Processing at Expanding Scale',\n",
       "  'Segmentation, Classification and Interpretation of Breast Cancer Medical Images using Human-in-the-Loop Machine Learning',\n",
       "  'Structured Evaluation of Synthetic Tabular Data',\n",
       "  'Supervised Contrastive Vision Transformer for Breast Histopathological\\n  Image Classification',\n",
       "  'Synthesizing Realistic Data for Table Recognition',\n",
       "  'Time-aware Heterogeneous Graph Transformer with Adaptive Attention\\n  Merging for Health Event Prediction',\n",
       "  'Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science'},\n",
       " 'Spectral State Space Models': {'Computationally and Memory-Efficient Robust Predictive Analytics Using Big Data',\n",
       "  'Feature-Based Echo-State Networks: A Step Towards Interpretability and Minimalism in Reservoir Computer',\n",
       "  'Generating Synthetic Time Series Data for Cyber-Physical Systems',\n",
       "  'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey',\n",
       "  'TSLANet: Rethinking Transformers for Time Series Representation Learning',\n",
       "  'Variational quantization for state space models',\n",
       "  'σ-GPTs: A New Approach to Autoregressive Models'},\n",
       " 'Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks': {'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey'},\n",
       " 'Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models': {'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'In-Context Principle Learning from Mistakes': {'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Find The Gap: Knowledge Base Reasoning For Visual Question Answering',\n",
       "  'Latxa: An Open Language Model and Evaluation Suite for Basque',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'QAGCN: Answering Multi-Relation Questions via Single-Step Implicit Reasoning over Knowledge Graphs',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing'},\n",
       " 'Neural Network Diffusion': {'An improved tabular data generator with VAE-GMM integration',\n",
       "  'Generative Modelling with High-Order Langevin Dynamics',\n",
       "  'LD-Pruner: Efficient Pruning of Latent Diffusion Models using\\n  Task-Agnostic Insights',\n",
       "  'Neural Flow Diffusion Models: Learnable Forward Process for Improved\\n  Diffusion Modelling',\n",
       "  'Physics-integrated generative modeling using attentive planar\\n  normalizing flow based variational autoencoder',\n",
       "  'SparseDM: Toward Sparse Efficient Diffusion Models'},\n",
       " 'RAGIC: Risk-Aware Generative Adversarial Model for Stock Interval Construction': {'An improved tabular data generator with VAE-GMM integration',\n",
       "  'Conformal Prediction for Stochastic Decision-Making of PV Power in Electricity Markets',\n",
       "  'Guided Discrete Diffusion for Electronic Health Record Generation',\n",
       "  'Reinforcement Learning in Agent-Based Market Simulation: Unveiling Realistic Stylized Facts and Behavior',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'TSLANet: Rethinking Transformers for Time Series Representation Learning',\n",
       "  'Variational quantization for state space models',\n",
       "  'σ-GPTs: A New Approach to Autoregressive Models'},\n",
       " 'OmniPred: Language Models as Universal Regressors': {'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'FedEval-LLM: Federated Evaluation of Large Language Models on Downstream\\n  Tasks with Collective Wisdom',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Learning representations of learning representations',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages',\n",
       "  'Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science',\n",
       "  'Using LLMs to Model the Beliefs and Preferences of Targeted Populations'},\n",
       " 'A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'A Comparative Study of AI-Generated (GPT-4) and Human-crafted MCQs in Programming Education': {'Beyond Code Generation: An Observational Study of ChatGPT Usage in\\n  Software Engineering Practice',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'LLM-Powered Test Case Generation for Detecting Tricky Bugs',\n",
       "  'Language Model Prompt Selection via Simulation Optimization',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards a Robust Retrieval-Based Summarization System'},\n",
       " 'Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models': {'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey'},\n",
       " 'Latent Attention for Linear Time Transformers': {'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving',\n",
       "  'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey',\n",
       "  'Toward a Theory of Tokenization in LLMs',\n",
       "  'TriForce: Lossless Acceleration of Long Sequence Generation with\\n  Hierarchical Speculative Decoding'},\n",
       " 'Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution': {'EdgeFusion: On-Device Text-to-Image Generation',\n",
       "  'LD-Pruner: Efficient Pruning of Latent Diffusion Models using\\n  Task-Agnostic Insights',\n",
       "  'Neural Flow Diffusion Models: Learnable Forward Process for Improved\\n  Diffusion Modelling',\n",
       "  'Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging\\n  Perturbations That Efficiently Fool Customized Diffusion Models',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Toward a Theory of Tokenization in LLMs'},\n",
       " 'Repetition Improves Language Model Embeddings': {'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling'},\n",
       " 'Fine-tuning with Very Large Dropout': {'A Survey of Deep Long-Tail Classification Advancements',\n",
       "  'DeNetDM: Debiasing by Network Depth Modulation',\n",
       "  'Distilled Datamodel with Reverse Gradient Matching',\n",
       "  'Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World',\n",
       "  'Gradient-Regularized Out-of-Distribution Detection',\n",
       "  'LoRA Dropout as a Sparsity Regularizer for Overfitting Control',\n",
       "  'Noise contrastive estimation with soft targets for conditional models',\n",
       "  'Toward a Realistic Benchmark for Out-of-Distribution Detection'},\n",
       " 'Is Cosine-Similarity of Embeddings Really About Similarity?': {'A multiobjective continuation method to compute the regularization path of deep neural networks',\n",
       "  'Closing the Gap in the Trade-off between Fair Representations and\\n  Accuracy',\n",
       "  'Deep Regression Representation Learning with Topology',\n",
       "  'How Deep Networks Learn Sparse and Hierarchical Data: the Sparse Random\\n  Hierarchy Model',\n",
       "  'Hyperbolic Delaunay Geometric Alignment',\n",
       "  'LipSim: A Provably Robust Perceptual Similarity Metric',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'Review-Based Cross-Domain Recommendation via Hyperbolic Embedding and Hierarchy-Aware Domain Disentanglement'},\n",
       " 'Dual Operating Modes of In-Context Learning': {'Dynamically Anchored Prompting for Task-Imbalanced Continual Learning',\n",
       "  'Localizing Paragraph Memorization in Language Models',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Meta Reinforcement Learning with Finite Training Tasks -- a Density Estimation Approach',\n",
       "  'Meta-Learning with Generalized Ridge Regression: High-dimensional Asymptotics, Optimality and Hyper-covariance Estimation',\n",
       "  'NC-TTT: A Noise Contrastive Approach for Test-Time Training',\n",
       "  'OTTER: Improving Zero-Shot Classification via Optimal Transport',\n",
       "  'Stronger Random Baselines for In-Context Learning',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'Chronos: Learning the Language of Time Series': {'Advancing Long-Term Multi-Energy Load Forecasting with Patchformer: A\\n  Patch and Transformer-Based Approach',\n",
       "  'DeepHeteroIoT: Deep Local and Global Learning over Heterogeneous IoT Sensor Data',\n",
       "  'Generating Synthetic Time Series Data for Cyber-Physical Systems',\n",
       "  'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'TSLANet: Rethinking Transformers for Time Series Representation Learning',\n",
       "  'Toward a Theory of Tokenization in LLMs',\n",
       "  'Variational quantization for state space models'},\n",
       " 'How Far Are We from Intelligent Visual Deductive Reasoning?': {'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Bridging Vision and Language Spaces with Assignment Prediction',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Concept-based Analysis of Neural Networks via Vision-Language Models',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Find The Gap: Knowledge Base Reasoning For Visual Question Answering',\n",
       "  'Pre-trained Vision-Language Models Learn Discoverable Visual Concepts',\n",
       "  'Self-Supervised Visual Preference Alignment',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models'},\n",
       " 'Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset': {'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Beyond Code Generation: An Observational Study of ChatGPT Usage in\\n  Software Engineering Practice',\n",
       "  'Concept-based Analysis of Neural Networks via Vision-Language Models',\n",
       "  'GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'Pre-trained Vision-Language Models Learn Discoverable Visual Concepts',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Self-Supervised Visual Preference Alignment',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning'},\n",
       " 'Data Interpreter: An LLM Agent For Data Science': {'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Automating REST API Postman Test Cases Using LLM',\n",
       "  'CT-Agent: Clinical Trial Multi-Agent with Large Language Model-based\\n  Reasoning',\n",
       "  'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models',\n",
       "  'Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\\n  Prediction',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing'},\n",
       " 'Mechanics of Next Token Prediction with Self-Attention': {'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models',\n",
       "  'Localizing Paragraph Memorization in Language Models',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey',\n",
       "  'Toward a Theory of Tokenization in LLMs',\n",
       "  'Transformers Can Represent $n$-gram Language Models',\n",
       "  'What do Transformers Know about Government?'},\n",
       " 'Transformers for Supervised Online Continual Learning': {'A Unified Replay-based Continuous Learning Framework for Spatio-Temporal\\n  Prediction on Streaming Data',\n",
       "  'Continual Learning on a Diet: Learning from Sparsely Labeled Streams\\n  Under Constrained Computation',\n",
       "  'Continual Offline Reinforcement Learning via Diffusion-based Dual\\n  Generative Replay',\n",
       "  'Dynamically Anchored Prompting for Task-Imbalanced Continual Learning',\n",
       "  'FedMeS: Personalized Federated Continual Learning Leveraging Local\\n  Memory',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'Offline Trajectory Generalization for Offline Reinforcement Learning',\n",
       "  'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective',\n",
       "  'Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer'},\n",
       " 'Transformer-Based Models Are Not Yet Perfect At Learning to Emulate Structural Recursion': {'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'Symbolic Integration Algorithm Selection with Machine Learning: LSTMs vs\\n  Tree LSTMs',\n",
       "  'Transformers Can Represent $n$-gram Language Models',\n",
       "  'What do Transformers Know about Government?'},\n",
       " 'TnT-LLM: Text Mining at Scale with Large Language Models': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'CT-Agent: Clinical Trial Multi-Agent with Large Language Model-based\\n  Reasoning',\n",
       "  'Conversational Financial Information Retrieval Model (ConFIRM)',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction'},\n",
       " 'FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'Conversational Financial Information Retrieval Model (ConFIRM)',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'Towards a Robust Retrieval-Based Summarization System'},\n",
       " 'Gecko: Versatile Text Embeddings Distilled from Large Language Models': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Learning Word Embedding with Better Distance Weighting and Window Size\\n  Scheduling',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering'},\n",
       " 'Stream of Search (SoS): Learning to Search in Language': {'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Self-playing Adversarial Language Game Enhances LLM Reasoning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'TimeGPT-1': {'A Comparison of Traditional and Deep Learning Methods for Parameter\\n  Estimation of the Ornstein-Uhlenbeck Process',\n",
       "  'Computationally and Memory-Efficient Robust Predictive Analytics Using Big Data',\n",
       "  'DeepHeteroIoT: Deep Local and Global Learning over Heterogeneous IoT Sensor Data',\n",
       "  'FMint: Bridging Human Designed and Data Pretrained Models for\\n  Differential Equation Foundation Model',\n",
       "  'Generating Synthetic Time Series Data for Cyber-Physical Systems',\n",
       "  'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'TSLANet: Rethinking Transformers for Time Series Representation Learning',\n",
       "  'Variational quantization for state space models'},\n",
       " 'Addressing the Abstraction and Reasoning Corpus via Procedural Example Generation': {'ASID: Active Exploration for System Identification in Robotic\\n  Manipulation',\n",
       "  'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers',\n",
       "  'NC-TTT: A Noise Contrastive Approach for Test-Time Training',\n",
       "  'PyTorchGeoNodes: Enabling Differentiable Shape Programs for 3D Shape\\n  Reconstruction',\n",
       "  'Zero-Shot Stitching in Reinforcement Learning using Relative\\n  Representations'},\n",
       " 'From Words to Numbers: Your Large Language Model Is Secretly A Capable Regressor When Given In-Context Examples': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Nearly Optimal Algorithms for Contextual Dueling Bandits from\\n  Adversarial Feedback',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'Stronger Random Baselines for In-Context Learning',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science'},\n",
       " 'The Rise of Diffusion Models in Time-Series Forecasting': {'ICST-DNET: An Interpretable Causal Spatio-Temporal Diffusion Network for\\n  Traffic Speed Prediction',\n",
       "  'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting',\n",
       "  'Integrating Marketing Channels into Quantile Transformation and Bayesian\\n  Optimization of Ensemble Kernels for Sales Prediction with Gaussian Process\\n  Models',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'Neural Flow Diffusion Models: Learnable Forward Process for Improved\\n  Diffusion Modelling',\n",
       "  'Neural McKean-Vlasov Processes: Distributional Dependence in Diffusion\\n  Processes',\n",
       "  'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'Using ARIMA to Predict the Expansion of Subscriber Data Consumption',\n",
       "  'Variational quantization for state space models'},\n",
       " 'The Illusion of State in State-Space Models': {'Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\\n  Prediction',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey',\n",
       "  'Toward a Theory of Tokenization in LLMs',\n",
       "  'Transformers Can Represent $n$-gram Language Models',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'What do Transformers Know about Government?'},\n",
       " 'AgentKit: Flow Engineering with Graphs, not Coding': {'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions',\n",
       "  'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Beyond Code Generation: An Observational Study of ChatGPT Usage in\\n  Software Engineering Practice',\n",
       "  'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'iTBLS: A Dataset of Interactive Conversations Over Tabular Information'},\n",
       " 'GoEX: Perspectives and Designs Towards a Runtime for Autonomous LLM Applications': {'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Automating REST API Postman Test Cases Using LLM',\n",
       "  'Beyond Code Generation: An Observational Study of ChatGPT Usage in\\n  Software Engineering Practice',\n",
       "  'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path\\n  Forward',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation'}}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Papers similar to papers_of_interest recommended via cosine similarity are :\")\n",
    "recom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "recom_euc={}\n",
    "for i in range(len(p_interest_2)):\n",
    "    row_i=set()\n",
    "    for j in recom_by_summary_index_euc[i]:\n",
    "        # print(daily['title'][j])\n",
    "        row_i.add(daily['title'][j])\n",
    "    recom_euc[p_interest_2['title'][i]] = row_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers similar to papers_of_interest recommended via euclidean distance are :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'A Generalist Neural Algorithmic Learner': {'Beyond the Known: Novel Class Discovery for Open-world Graph Learning',\n",
       "  'Delayed Bottlenecking: Alleviating Forgetting in Pre-trained Graph\\n  Neural Networks',\n",
       "  'FR-NAS: Forward-and-Reverse Graph Predictor for Efficient Neural\\n  Architecture Search',\n",
       "  'GNNavigator: Towards Adaptive Training of Graph Neural Networks via\\n  Automatic Guideline Exploration',\n",
       "  'Graph Continual Learning with Debiased Lossless Memory Replay',\n",
       "  'Graph Machine Learning in the Era of Large Language Models (LLMs)',\n",
       "  'Improving the interpretability of GNN predictions through\\n  conformal-based graph sparsification',\n",
       "  'On the Scalability of GNNs for Molecular Graphs'},\n",
       " 'Meta-Learning Fast Weight Language Models': {'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LayerNorm: A key component in parameter-efficient fine-tuning',\n",
       "  'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling'},\n",
       " \"Testing GLOM's ability to infer wholes from ambiguous parts\": {'Bridging Vision and Language Spaces with Assignment Prediction',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Concept-based Analysis of Neural Networks via Vision-Language Models',\n",
       "  'GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Neuro-Inspired Information-Theoretic Hierarchical Perception for\\n  Multimodal Learning',\n",
       "  'Residual-based Language Models are Free Boosters for Biomedical Imaging',\n",
       "  'Sparse multimodal fusion with modal channel attention',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'Seven Sketches in Compositionality: An Invitation to Applied Category Theory': {'Accounting for AI and Users Shaping One Another: The Role of\\n  Mathematical Models',\n",
       "  'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions',\n",
       "  'Complexity of Probabilistic Reasoning for Neurosymbolic Classification\\n  Techniques',\n",
       "  'Generalized Gradient Descent is a Hypergraph Functor',\n",
       "  'On permutation-invariant neural networks',\n",
       "  'Semantic Cells: Evolutional Process to Acquire Sense Diversity of Items',\n",
       "  'Transformers Can Represent $n$-gram Language Models',\n",
       "  'What do Transformers Know about Government?'},\n",
       " 'Evolution through Large Models': {'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'Next Generation Loss Function for Image Classification',\n",
       "  'OpenBezoar: Small, Cost-Effective and Open Models Trained on Mixes of\\n  Instruction Data',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\\n  Upcycled Mixture-of-Experts'},\n",
       " 'Teaching Algorithmic Reasoning via In-context Learning': {'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Emergent Analogical Reasoning in Large Language Models': {'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Transformers Can Represent $n$-gram Language Models'},\n",
       " 'Autoformalization with Large Language Models': {'Automating REST API Postman Test Cases Using LLM',\n",
       "  'Concept-based Analysis of Neural Networks via Vision-Language Models',\n",
       "  'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Learning representations of learning representations',\n",
       "  'PARAMANU-GANITA: Language Model with Mathematical Capabilities',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'Symbolic Integration Algorithm Selection with Machine Learning: LSTMs vs\\n  Tree LSTMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing'},\n",
       " 'In-context Reinforcement Learning with Algorithm Distillation': {'Adaptive Regularization of Representation Rank as an Implicit Constraint\\n  of Bellman Equation',\n",
       "  'Adversarial Imitation Learning via Boosting',\n",
       "  'Continual Offline Reinforcement Learning via Diffusion-based Dual\\n  Generative Replay',\n",
       "  'DPO: Differential reinforcement learning with application to optimal\\n  configuration search',\n",
       "  'Decision Mamba: Reinforcement Learning via Sequence Modeling with Selective State Spaces',\n",
       "  'Deep Policy Optimization with Temporal Logic Constraints',\n",
       "  'Goal Exploration via Adaptive Skill Distribution for Goal-Conditioned\\n  Reinforcement Learning',\n",
       "  'Inferring Behavior-Specific Context Improves Zero-Shot Generalization in\\n  Reinforcement Learning',\n",
       "  'Offline Trajectory Generalization for Offline Reinforcement Learning',\n",
       "  'Zero-Shot Stitching in Reinforcement Learning using Relative\\n  Representations'},\n",
       " 'What learning algorithm is in-context learning? Investigations with linear models': {'Decision Mamba: Reinforcement Learning via Sequence Modeling with Selective State Spaces',\n",
       "  'Decomposing and Editing Predictions by Modeling Model Computation',\n",
       "  'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'Toward a Theory of Tokenization in LLMs',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'Transformers learn in-context by gradient descent': {'Can We Break Free from Strong Data Augmentations in Self-Supervised\\n  Learning?',\n",
       "  'Decomposing and Editing Predictions by Modeling Model Computation',\n",
       "  'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Employing Layerwised Unsupervised Learning to Lessen Data and Loss\\n  Requirements in Forward-Forward Algorithms',\n",
       "  'Enhancing Efficiency in Vision Transformer Networks: Design Techniques and Insights',\n",
       "  'Improving Generalization via Meta-Learning on Hard Samples',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'NC-TTT: A Noise Contrastive Approach for Test-Time Training',\n",
       "  'Reactive Model Correction: Mitigating Harm to Task-Relevant Features via\\n  Conditional Bias Suppression',\n",
       "  'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective'},\n",
       " 'Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers': {'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LayerNorm: A key component in parameter-efficient fine-tuning',\n",
       "  'Localizing Paragraph Memorization in Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models': {'EdgeFusion: On-Device Text-to-Image Generation',\n",
       "  'FlashSpeech: Efficient Zero-Shot Speech Synthesis',\n",
       "  'Guided Discrete Diffusion for Electronic Health Record Generation',\n",
       "  'LD-Pruner: Efficient Pruning of Latent Diffusion Models using\\n  Task-Agnostic Insights',\n",
       "  'Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level\\n  Knowledge Distillation'},\n",
       " 'Tracr: Compiled Transformers as a Laboratory for Interpretability': {'Concept-based Analysis of Neural Networks via Vision-Language Models',\n",
       "  'Decomposing and Editing Predictions by Modeling Model Computation',\n",
       "  'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'LayerNorm: A key component in parameter-efficient fine-tuning',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'Toward a Theory of Tokenization in LLMs',\n",
       "  'Transformers Can Represent $n$-gram Language Models',\n",
       "  'What do Transformers Know about Government?'},\n",
       " 'Learning-Rate-Free Learning by D-Adaptation': {'ColA: Collaborative Adaptation with Gradient Learning',\n",
       "  'DFWLayer: Differentiable Frank-Wolfe Optimization Layer',\n",
       "  'Data-Driven Performance Guarantees for Classical and Learned Optimizers',\n",
       "  'Federated Optimization with Doubly Regularized Drift Correction',\n",
       "  'Generalized Population-Based Training for Hyperparameter Optimization in\\n  Reinforcement Learning',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'Personalized Collaborative Fine-Tuning for On-Device Large Language\\n  Models',\n",
       "  'Regularized Gauss-Newton for Optimizing Overparameterized Neural\\n  Networks'},\n",
       " 'A Watermark for Large Language Models': {'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'Negative Label Guided OOD Detection with Pretrained Vision-Language Models',\n",
       "  'Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path\\n  Forward',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Private Attribute Inference from Images with Vision-Language Models',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'Talk Too Much: Poisoning Large Language Models under Token Limit',\n",
       "  'Toward a Theory of Tokenization in LLMs',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature': {'Does It Make Sense to Explain a Black Box With Another Black Box?',\n",
       "  'MUGC: Machine Generated versus User Generated Content Detection',\n",
       "  'Negative Label Guided OOD Detection with Pretrained Vision-Language Models',\n",
       "  'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'Robust CLIP-Based Detector for Exposing Diffusion Model-Generated Images',\n",
       "  'Talk Too Much: Poisoning Large Language Models under Token Limit',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Grounding Language Models to Images for Multimodal Inputs and Outputs': {'Bridging Vision and Language Spaces with Assignment Prediction',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Cross-Modal Adapter: Parameter-Efficient Transfer Learning Approach for\\n  Vision-Language Models',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Find The Gap: Knowledge Base Reasoning For Visual Question Answering',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept\\n  Understanding',\n",
       "  'RankCLIP: Ranking-Consistent Language-Image Pretraining',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'Stronger Random Baselines for In-Context Learning',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'ReAct: Synergizing Reasoning and Acting in Language Models': {'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction'},\n",
       " 'STaR: Bootstrapping Reasoning With Reasoning': {'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Hungry Hungry Hippos: Towards Language Modeling with State Space Models': {'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey',\n",
       "  'TriForce: Lossless Acceleration of Long Sequence Generation with\\n  Hierarchical Speculative Decoding'},\n",
       " 'Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment': {'Bridging Vision and Language Spaces with Assignment Prediction',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Cross-Modal Adapter: Parameter-Efficient Transfer Learning Approach for\\n  Vision-Language Models',\n",
       "  'Data-Efficient Multimodal Fusion on a Single GPU',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'RankCLIP: Ranking-Consistent Language-Image Pretraining',\n",
       "  'Residual-based Language Models are Free Boosters for Biomedical Imaging',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'Visual Time Series Forecasting: An Image-driven Approach': {'DeepHeteroIoT: Deep Local and Global Learning over Heterogeneous IoT Sensor Data',\n",
       "  'Generating Synthetic Time Series Data for Cyber-Physical Systems',\n",
       "  'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting',\n",
       "  'Integrating Marketing Channels into Quantile Transformation and Bayesian\\n  Optimization of Ensemble Kernels for Sales Prediction with Gaussian Process\\n  Models',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'TSLANet: Rethinking Transformers for Time Series Representation Learning',\n",
       "  'Using ARIMA to Predict the Expansion of Subscriber Data Consumption',\n",
       "  'Variational quantization for state space models'},\n",
       " 'Unveiling Transformers with LEGO: a synthetic reasoning task': {'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'LayerNorm: A key component in parameter-efficient fine-tuning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'What do Transformers Know about Government?'},\n",
       " 'Looped Transformers as Programmable Computers': {'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'Transformer tricks: Removing weights for skipless transformers'},\n",
       " 'Evaluating Large Language Models in Theory of Mind Tasks': {'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  \"On Large Language Models' Hallucination with Regard to Known Facts\",\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'Transformers Can Represent $n$-gram Language Models',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Learning threshold neurons via the \"edge of stability\"': {'A Mean-Field Analysis of Neural Gradient Descent-Ascent: Applications to\\n  Functional Conditional Moment Equations',\n",
       "  'A mean curvature flow arising in adversarial training',\n",
       "  'Regularized Gauss-Newton for Optimizing Overparameterized Neural\\n  Networks',\n",
       "  'Regularized Gradient Clipping Provably Trains Wide and Deep Neural\\n  Networks',\n",
       "  'Singular-limit analysis of gradient descent with noise injection',\n",
       "  'Training neural networks with structured noise improves classification and generalization'},\n",
       " 'Symbolic Discovery of Optimization Algorithms': {'Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World',\n",
       "  'FR-NAS: Forward-and-Reverse Graph Predictor for Efficient Neural\\n  Architecture Search',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LD-Pruner: Efficient Pruning of Latent Diffusion Models using\\n  Task-Agnostic Insights',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'PQA: Exploring the Potential of Product Quantization in DNN Hardware Acceleration',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens'},\n",
       " 'MoËT: Mixture of Expert Trees and its Application to Verifiable Reinforcement Learning': {'Actor-Critic Reinforcement Learning with Phased Actor',\n",
       "  'Adversarial Imitation Learning via Boosting',\n",
       "  'Application of the representative measure approach to assess the\\n  reliability of decision trees in dealing with unseen vehicle collision data',\n",
       "  'BayesJudge: Bayesian Kernel Language Modelling with Confidence\\n  Uncertainty in Legal Judgment Prediction',\n",
       "  'CtRL-Sim: Reactive and Controllable Driving Agents with Offline Reinforcement Learning',\n",
       "  'Enhancing Autonomous Vehicle Training with Language Model Integration\\n  and Critical Scenario Generation',\n",
       "  'Generalized Population-Based Training for Hyperparameter Optimization in\\n  Reinforcement Learning',\n",
       "  'Random Network Distillation Based Deep Reinforcement Learning for AGV\\n  Path Planning'},\n",
       " 'Continual Backprop: Stochastic Gradient Descent with Persistent Randomness': {'Actor-Critic Reinforcement Learning with Phased Actor',\n",
       "  'Continual Offline Reinforcement Learning via Diffusion-based Dual\\n  Generative Replay',\n",
       "  'Dynamically Anchored Prompting for Task-Imbalanced Continual Learning',\n",
       "  'FedMeS: Personalized Federated Continual Learning Leveraging Local\\n  Memory',\n",
       "  'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective',\n",
       "  'Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer',\n",
       "  'Single-Task Continual Offline Reinforcement Learning',\n",
       "  'Training neural networks with structured noise improves classification and generalization'},\n",
       " 'General-Purpose In-Context Learning by Meta-Learning Transformers': {'Distilled Datamodel with Reverse Gradient Matching',\n",
       "  'Improving Generalization via Meta-Learning on Hard Samples',\n",
       "  'MPC of Uncertain Nonlinear Systems with Meta-Learning for Fast\\n  Adaptation of Neural Predictive Models',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'Meta Reinforcement Learning with Finite Training Tasks -- a Density Estimation Approach',\n",
       "  'NC-TTT: A Noise Contrastive Approach for Test-Time Training',\n",
       "  'SOPHON: Non-Fine-Tunable Learning to Restrain Task Transferability For\\n  Pre-trained Models',\n",
       "  'Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer'},\n",
       " 'You Only Live Once: Single-Life Reinforcement Learning': {'Adversarial Imitation Learning via Boosting',\n",
       "  'Continual Offline Reinforcement Learning via Diffusion-based Dual\\n  Generative Replay',\n",
       "  'DPO: Differential reinforcement learning with application to optimal\\n  configuration search',\n",
       "  'Dataset Reset Policy Optimization for RLHF',\n",
       "  'Deep Policy Optimization with Temporal Logic Constraints',\n",
       "  'Goal Exploration via Adaptive Skill Distribution for Goal-Conditioned\\n  Reinforcement Learning',\n",
       "  'Inferring Behavior-Specific Context Improves Zero-Shot Generalization in\\n  Reinforcement Learning',\n",
       "  'Meta Reinforcement Learning with Finite Training Tasks -- a Density Estimation Approach',\n",
       "  'Offline Trajectory Generalization for Offline Reinforcement Learning',\n",
       "  'Single-Task Continual Offline Reinforcement Learning'},\n",
       " 'Human-Timescale Adaptation in an Open-Ended Task Space': {'ASID: Active Exploration for System Identification in Robotic\\n  Manipulation',\n",
       "  'Adversarial Imitation Learning via Boosting',\n",
       "  'Goal Exploration via Adaptive Skill Distribution for Goal-Conditioned\\n  Reinforcement Learning',\n",
       "  'Inferring Behavior-Specific Context Improves Zero-Shot Generalization in\\n  Reinforcement Learning',\n",
       "  'MAexp: A Generic Platform for RL-based Multi-Agent Exploration',\n",
       "  'Meta Reinforcement Learning with Finite Training Tasks -- a Density Estimation Approach',\n",
       "  'Rapid Motor Adaptation for Robotic Manipulator Arms',\n",
       "  'Zero-Shot Stitching in Reinforcement Learning using Relative\\n  Representations'},\n",
       " 'Language Is Not All You Need: Aligning Perception with Language Models': {'Bridging Vision and Language Spaces with Assignment Prediction',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Cross-Modal Adapter: Parameter-Efficient Transfer Learning Approach for\\n  Vision-Language Models',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Find The Gap: Knowledge Base Reasoning For Visual Question Answering',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'RankCLIP: Ranking-Consistent Language-Image Pretraining',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback': {'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'A study on the plasticity of neural networks': {'Continual Learning on a Diet: Learning from Sparsely Labeled Streams\\n  Under Constrained Computation',\n",
       "  'Continual Offline Reinforcement Learning via Diffusion-based Dual\\n  Generative Replay',\n",
       "  'Delayed Bottlenecking: Alleviating Forgetting in Pre-trained Graph\\n  Neural Networks',\n",
       "  'Dynamically Anchored Prompting for Task-Imbalanced Continual Learning',\n",
       "  'FedMeS: Personalized Federated Continual Learning Leveraging Local\\n  Memory',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'NC-TTT: A Noise Contrastive Approach for Test-Time Training',\n",
       "  'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective',\n",
       "  'Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer',\n",
       "  'Single-Task Continual Offline Reinforcement Learning'},\n",
       " 'Consistency Models': {'Balanced Mixed-Type Tabular Data Synthesis with Diffusion Models',\n",
       "  'EdgeFusion: On-Device Text-to-Image Generation',\n",
       "  'Efficient Conditional Diffusion Model with Probability Flow Sampling for\\n  Image Super-resolution',\n",
       "  'LD-Pruner: Efficient Pruning of Latent Diffusion Models using\\n  Task-Agnostic Insights',\n",
       "  'SparseDM: Toward Sparse Efficient Diffusion Models',\n",
       "  'TMPQ-DM: Joint Timestep Reduction and Quantization Precision Selection\\n  for Efficient Diffusion Models'},\n",
       " 'Larger language models do in-context learning differently': {'Aligning language models with human preferences',\n",
       "  'Localizing Paragraph Memorization in Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy\\n  Data',\n",
       "  'Stronger Random Baselines for In-Context Learning',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Permutation Equivariant Neural Functionals': {'A multiobjective continuation method to compute the regularization path of deep neural networks',\n",
       "  'Can We Break Free from Strong Data Augmentations in Self-Supervised\\n  Learning?',\n",
       "  'Deep Learning as Ricci Flow',\n",
       "  'Deep Neural Networks via Complex Network Theory: a Perspective',\n",
       "  'FR-NAS: Forward-and-Reverse Graph Predictor for Efficient Neural\\n  Architecture Search',\n",
       "  'HOIN: High-Order Implicit Neural Representations',\n",
       "  'LipSim: A Provably Robust Perceptual Similarity Metric',\n",
       "  'On permutation-invariant neural networks',\n",
       "  'Reactive Model Correction: Mitigating Harm to Task-Relevant Features via\\n  Conditional Bias Suppression',\n",
       "  'Strong Transferable Adversarial Attacks via Ensembled Asymptotically Normal Distribution Learning'},\n",
       " 'The Prompt Artists': {'An Economic Solution to Copyright Challenges of Generative AI',\n",
       "  'Ethical-Lens: Curbing Malicious Usages of Open-Source Text-to-Image\\n  Models',\n",
       "  'Exploring Text-to-Motion Generation with Human Preference',\n",
       "  'Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept\\n  Understanding',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  '©Plug-in Authorization for Human Content Copyright Protection\\n  in Text-to-Image Model'},\n",
       " 'Sparks of Artificial General Intelligence: Early experiments with GPT-4': {'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Online Deep Learning: Learning Deep Neural Networks on the Fly': {'A Unified Replay-based Continuous Learning Framework for Spatio-Temporal\\n  Prediction on Streaming Data',\n",
       "  'Actor-Critic Reinforcement Learning with Phased Actor',\n",
       "  'AntBatchInfer: Elastic Batch Inference in the Kubernetes Cluster',\n",
       "  'Employing Layerwised Unsupervised Learning to Lessen Data and Loss\\n  Requirements in Forward-Forward Algorithms',\n",
       "  'FedMeS: Personalized Federated Continual Learning Leveraging Local\\n  Memory',\n",
       "  'Online Algorithms with Limited Data Retention',\n",
       "  'QCore: Data-Efficient, On-Device Continual Calibration for Quantized\\n  Models -- Extended Version',\n",
       "  'Single-Task Continual Offline Reinforcement Learning'},\n",
       " 'Dynamically Modular and Sparse General Continual Learning': {'A Unified Replay-based Continuous Learning Framework for Spatio-Temporal\\n  Prediction on Streaming Data',\n",
       "  'Continual Learning on a Diet: Learning from Sparsely Labeled Streams\\n  Under Constrained Computation',\n",
       "  'Continual Offline Reinforcement Learning via Diffusion-based Dual\\n  Generative Replay',\n",
       "  'Delayed Bottlenecking: Alleviating Forgetting in Pre-trained Graph\\n  Neural Networks',\n",
       "  'Dynamically Anchored Prompting for Task-Imbalanced Continual Learning',\n",
       "  'Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World',\n",
       "  'FedMeS: Personalized Federated Continual Learning Leveraging Local\\n  Memory',\n",
       "  'Graph Continual Learning with Debiased Lossless Memory Replay',\n",
       "  'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective',\n",
       "  'Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer'},\n",
       " 'Meta-learning via Language Model In-context Tuning': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'Improving Generalization via Meta-Learning on Hard Samples',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'OTTER: Improving Zero-Shot Classification via Optimal Transport',\n",
       "  'Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy\\n  Data',\n",
       "  'Stronger Random Baselines for In-Context Learning',\n",
       "  'When LLMs are Unfit Use FastFit: Fast and Effective Text Classification\\n  with Many Classes'},\n",
       " 'SemSup: Semantic Supervision for Simple and Scalable Zero-shot Generalization': {'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Data Alignment for Zero-Shot Concept Generation in Dermatology AI',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'OTTER: Improving Zero-Shot Classification via Optimal Transport',\n",
       "  'RankCLIP: Ranking-Consistent Language-Image Pretraining',\n",
       "  'When LLMs are Unfit Use FastFit: Fast and Effective Text Classification\\n  with Many Classes',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " \"A Neuro-vector-symbolic Architecture for Solving Raven's Progressive Matrices\": {'Complexity of Probabilistic Reasoning for Neurosymbolic Classification\\n  Techniques',\n",
       "  'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'FR-NAS: Forward-and-Reverse Graph Predictor for Efficient Neural\\n  Architecture Search',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Neuro-Inspired Information-Theoretic Hierarchical Perception for\\n  Multimodal Learning',\n",
       "  'On the Independence Assumption in Neurosymbolic Learning',\n",
       "  'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective',\n",
       "  'Symbolic Integration Algorithm Selection with Machine Learning: LSTMs vs\\n  Tree LSTMs'},\n",
       " 'Transfer Learning with Deep Tabular Models': {'An improved tabular data generator with VAE-GMM integration',\n",
       "  'Explainable Lung Disease Classification from Chest X-Ray Images\\n  Utilizing Deep Learning and XAI',\n",
       "  'Joint chest X-ray diagnosis and clinical visual attention prediction with multi-stage cooperative learning: enhancing interpretability',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'Machine Learning Techniques for MRI Data Processing at Expanding Scale',\n",
       "  'Measuring Feature Dependency of Neural Networks by Collapsing Feature\\n  Dimensions in the Data Manifold',\n",
       "  'Time-aware Heterogeneous Graph Transformer with Adaptive Attention\\n  Merging for Health Event Prediction',\n",
       "  'Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science'},\n",
       " 'Improving Code Generation by Training with Natural Language Feedback': {'Aligning language models with human preferences',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model',\n",
       "  'XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\\n  Upcycled Mixture-of-Experts'},\n",
       " 'Self-Refine: Iterative Refinement with Self-Feedback': {'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level\\n  Knowledge Distillation',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning',\n",
       "  'Towards a Robust Retrieval-Based Summarization System',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'A Comparison of Reinforcement Learning and Deep Trajectory Based Stochastic Control Agents for Stepwise Mean-Variance Hedging': {'A Mean-Field Analysis of Neural Gradient Descent-Ascent: Applications to\\n  Functional Conditional Moment Equations',\n",
       "  'Actor-Critic Reinforcement Learning with Phased Actor',\n",
       "  'Continuous-time Risk-sensitive Reinforcement Learning via Quadratic\\n  Variation Penalty',\n",
       "  'DPO: Differential reinforcement learning with application to optimal\\n  configuration search',\n",
       "  'Reinforcement Learning in Agent-Based Market Simulation: Unveiling Realistic Stylized Facts and Behavior',\n",
       "  'VC Theory for Inventory Policies'},\n",
       " 'Language Models can Solve Computer Tasks': {'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing'},\n",
       " 'HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face': {'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions',\n",
       "  'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing'},\n",
       " 'Better Language Models of Code through Self-Improvement': {'Automated Multi-Language to English Machine Translation Using Generative\\n  Pre-Trained Transformers',\n",
       "  'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level\\n  Knowledge Distillation',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\\n  Upcycled Mixture-of-Experts'},\n",
       " 'Generative Agents: Interactive Simulacra of Human Behavior': {'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions',\n",
       "  'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Decoding AI: The inside story of data analysis in ChatGPT',\n",
       "  'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'PhyScene: Physically Interactable 3D Scene Synthesis for Embodied AI',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'iRAG: An Incremental Retrieval Augmented Generation System for Videos',\n",
       "  'iTBLS: A Dataset of Interactive Conversations Over Tabular Information'},\n",
       " 'Evaluating Verifiability in Generative Search Engines': {'AI-Generated Faces in the Real World: A Large-Scale Case Study of\\n  Twitter Profile Images',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'MUGC: Machine Generated versus User Generated Content Detection',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Reliability Estimation of News Media Sources: Birds of a Feather Flock\\n  Together',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'STRUM-LLM: Attributed and Structured Contrastive Summarization',\n",
       "  'Towards a Robust Retrieval-Based Summarization System',\n",
       "  'iRAG: An Incremental Retrieval Augmented Generation System for Videos'},\n",
       " 'Prompting Is Programming: A Query Language for Large Language Models': {'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving',\n",
       "  'Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\\n  Prediction',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction'},\n",
       " 'Scaling Transformer to 1M tokens and beyond with RMT': {'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving',\n",
       "  'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling'},\n",
       " 'Repository-Level Prompt Generation for Large Language Models of Code': {'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Beyond Code Generation: An Observational Study of ChatGPT Usage in\\n  Software Engineering Practice',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'LLM-Powered Test Case Generation for Detecting Tricky Bugs',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\\n  Upcycled Mixture-of-Experts'},\n",
       " 'Self-Instruct: Aligning Language Models with Self-Generated Instructions': {'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'Unlimiformer: Long-Range Transformers with Unlimited Length Input': {'Automated Multi-Language to English Machine Translation Using Generative\\n  Pre-Trained Transformers',\n",
       "  'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey'},\n",
       " 'Causal Reasoning and Large Language Models: Opening a New Frontier for Causality': {'Benchmarking Counterfactual Image Generation',\n",
       "  'Bias patterns in the application of LLMs for clinical decision support:\\n  A comprehensive study',\n",
       "  'CT-Agent: Clinical Trial Multi-Agent with Large Language Model-based\\n  Reasoning',\n",
       "  'Enhancing Counterfactual Explanation Search with Diffusion Distance and\\n  Directional Coherence',\n",
       "  'LLM-Enhanced Causal Discovery in Temporal Domain from Interventional\\n  Data',\n",
       "  'Neural Networks with Causal Graph Constraints: A New Approach for\\n  Treatment Effects Estimation',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'Redefining the Shortest Path Problem Formulation of the Linear\\n  Non-Gaussian Acyclic Model: Pairwise Likelihood Ratios, Prior Knowledge, and\\n  Path Enumeration'},\n",
       " 'Masked Trajectory Models for Prediction, Representation, and Control': {'Actor-Critic Reinforcement Learning with Phased Actor',\n",
       "  'Adaptive Regularization of Representation Rank as an Implicit Constraint\\n  of Bellman Equation',\n",
       "  'Adversarial Imitation Learning via Boosting',\n",
       "  'Continual Offline Reinforcement Learning via Diffusion-based Dual\\n  Generative Replay',\n",
       "  'CtRL-Sim: Reactive and Controllable Driving Agents with Offline Reinforcement Learning',\n",
       "  'DPO: Differential reinforcement learning with application to optimal\\n  configuration search',\n",
       "  'Deep Policy Optimization with Temporal Logic Constraints',\n",
       "  'Offline Trajectory Generalization for Offline Reinforcement Learning'},\n",
       " 'Replacing Rewards with Examples: Example-Based Policy Search via Recursive Classification': {'Actor-Critic Reinforcement Learning with Phased Actor',\n",
       "  'Adaptive Regularization of Representation Rank as an Implicit Constraint\\n  of Bellman Equation',\n",
       "  'DPO: Differential reinforcement learning with application to optimal\\n  configuration search',\n",
       "  'Dataset Reset Policy Optimization for RLHF',\n",
       "  'Deep Policy Optimization with Temporal Logic Constraints',\n",
       "  'From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function',\n",
       "  'Generalized Population-Based Training for Hyperparameter Optimization in\\n  Reinforcement Learning',\n",
       "  'Meta Reinforcement Learning with Finite Training Tasks -- a Density Estimation Approach'},\n",
       " 'Pretraining Without Attention': {'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LayerNorm: A key component in parameter-efficient fine-tuning',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey',\n",
       "  'Toward a Theory of Tokenization in LLMs'},\n",
       " 'Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision': {'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Aligning language models with human preferences',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Stepwise Alignment for Constrained Language Model Policy Optimization',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers': {'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LD-Pruner: Efficient Pruning of Latent Diffusion Models using\\n  Task-Agnostic Insights',\n",
       "  'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling'},\n",
       " 'The ConceptARC Benchmark: Evaluating Understanding and Generalization in the ARC Domain': {'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Decoding AI: The inside story of data analysis in ChatGPT',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'Pre-trained Vision-Language Models Learn Discoverable Visual Concepts',\n",
       "  'Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept\\n  Understanding',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning'},\n",
       " 'Symbol tuning improves in-context learning in language models': {'Aligning language models with human preferences',\n",
       "  'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'LayerNorm: A key component in parameter-efficient fine-tuning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy\\n  Data',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Online Continual Learning Without the Storage Constraint': {'A Unified Replay-based Continuous Learning Framework for Spatio-Temporal\\n  Prediction on Streaming Data',\n",
       "  'Continual Learning on a Diet: Learning from Sparsely Labeled Streams\\n  Under Constrained Computation',\n",
       "  'Continual Offline Reinforcement Learning via Diffusion-based Dual\\n  Generative Replay',\n",
       "  'Dynamically Anchored Prompting for Task-Imbalanced Continual Learning',\n",
       "  'FedMeS: Personalized Federated Continual Learning Leveraging Local\\n  Memory',\n",
       "  'QCore: Data-Efficient, On-Device Continual Calibration for Quantized\\n  Models -- Extended Version',\n",
       "  'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective',\n",
       "  'Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer',\n",
       "  'Single-Task Continual Offline Reinforcement Learning',\n",
       "  'kNN-CLIP: Retrieval Enables Training-Free Segmentation on Continually\\n  Expanding Large Vocabularies'},\n",
       " 'TinyStories: How Small Can Language Models Be and Still Speak Coherent English?': {'Automated Multi-Language to English Machine Translation Using Generative\\n  Pre-Trained Transformers',\n",
       "  'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level\\n  Knowledge Distillation',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning'},\n",
       " 'Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task': {'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Localizing Paragraph Memorization in Language Models',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  \"On Large Language Models' Hallucination with Regard to Known Facts\",\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Self-playing Adversarial Language Game Enhances LLM Reasoning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Transformers Can Represent $n$-gram Language Models',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models': {'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing'},\n",
       " 'Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions': {'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Self-playing Adversarial Language Game Enhances LLM Reasoning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction'},\n",
       " 'Has It All Been Solved? Open NLP Research Questions Not Solved by Large Language Models': {'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'Improving Factuality and Reasoning in Language Models through Multiagent Debate': {'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  \"On Large Language Models' Hallucination with Regard to Known Facts\",\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Meta-Learning Online Adaptation of Language Models': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'LayerNorm: A key component in parameter-efficient fine-tuning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy\\n  Data',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'Stronger Random Baselines for In-Context Learning',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback': {'Aligning language models with human preferences',\n",
       "  'FedEval-LLM: Federated Evaluation of Large Language Models on Downstream\\n  Tasks with Collective Wisdom',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Stronger Random Baselines for In-Context Learning',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Direct Preference Optimization: Your Language Model is Secretly a Reward Model': {'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Aligning language models with human preferences',\n",
       "  'Dataset Reset Policy Optimization for RLHF',\n",
       "  'Filtered Direct Preference Optimization',\n",
       "  'From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function',\n",
       "  'Learn Your Reference Model for Real Good Alignment',\n",
       "  'Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy\\n  Data',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs'},\n",
       " 'Thought Cloning: Learning to Think while Acting by Imitating Human Thinking': {'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions',\n",
       "  'Adversarial Imitation Learning via Boosting',\n",
       "  'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'LIMA: Less Is More for Alignment': {'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Aligning language models with human preferences',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Filtered Direct Preference Optimization',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy\\n  Data',\n",
       "  'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Gorilla: Large Language Model Connected with Massive APIs': {'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving',\n",
       "  'Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\\n  Prediction',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'Deductive Verification of Chain-of-Thought Reasoning': {'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  \"On Large Language Models' Hallucination with Regard to Known Facts\",\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning'},\n",
       " 'INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models': {'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'LayerNorm: A key component in parameter-efficient fine-tuning',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'PaLI-X: On Scaling up a Multilingual Vision and Language Model': {'Bridging Vision and Language Spaces with Assignment Prediction',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Cross-Modal Adapter: Parameter-Efficient Transfer Learning Approach for\\n  Vision-Language Models',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Find The Gap: Knowledge Base Reasoning For Visual Question Answering',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'RankCLIP: Ranking-Consistent Language-Image Pretraining',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'Passive learning of active causal strategies in agents and language models': {'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Aligning language models with human preferences',\n",
       "  'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'LLM-Enhanced Causal Discovery in Temporal Domain from Interventional\\n  Data',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Augmenting Language Models with Long-Term Memory': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'Localizing Paragraph Memorization in Language Models',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models'},\n",
       " 'V-LoL: A Diagnostic Dataset for Visual Logical Learning': {'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Concept-based Analysis of Neural Networks via Vision-Language Models',\n",
       "  'Deep Policy Optimization with Temporal Logic Constraints',\n",
       "  'Hyper Evidential Deep Learning to Quantify Composite Classification\\n  Uncertainty',\n",
       "  'LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos',\n",
       "  'Pre-trained Vision-Language Models Learn Discoverable Visual Concepts',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Zero-Shot Stitching in Reinforcement Learning using Relative\\n  Representations'},\n",
       " 'Fine-Tuning Language Models with Just Forward Passes': {'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'TriForce: Lossless Acceleration of Long Sequence Generation with\\n  Hierarchical Speculative Decoding'},\n",
       " 'The Curse of Recursion: Training on Generated Data Makes Models Forget': {'EdgeFusion: On-Device Text-to-Image Generation',\n",
       "  'LD-Pruner: Efficient Pruning of Latent Diffusion Models using\\n  Task-Agnostic Insights',\n",
       "  'Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging\\n  Perturbations That Efficiently Fool Customized Diffusion Models',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept\\n  Understanding',\n",
       "  'Residual-based Language Models are Free Boosters for Biomedical Imaging'},\n",
       " 'System-Level Natural Language Feedback': {'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Beyond Code Generation: An Observational Study of ChatGPT Usage in\\n  Software Engineering Practice',\n",
       "  'Filtered Direct Preference Optimization',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'ReALM: Reference Resolution As Language Modeling',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing'},\n",
       " 'Supervised Pretraining Can Learn In-Context Reinforcement Learning': {'CAESAR: Enhancing Federated RL in Heterogeneous MDPs through Convergence-Aware Sampling with Screening',\n",
       "  'Dataset Reset Policy Optimization for RLHF',\n",
       "  'Decision Mamba: Reinforcement Learning via Sequence Modeling with Selective State Spaces',\n",
       "  'Deep Policy Optimization with Temporal Logic Constraints',\n",
       "  'Goal Exploration via Adaptive Skill Distribution for Goal-Conditioned\\n  Reinforcement Learning',\n",
       "  'Meta Reinforcement Learning with Finite Training Tasks -- a Density Estimation Approach',\n",
       "  'Offline Trajectory Generalization for Offline Reinforcement Learning',\n",
       "  'Randomized Exploration in Cooperative Multi-Agent Reinforcement Learning',\n",
       "  'Settling Constant Regrets in Linear Markov Decision Processes',\n",
       "  'Tree Bandits for Generative Bayes'},\n",
       " 'ChemCrow: Augmenting large-language models with chemistry tools': {'$\\\\texttt{MiniMol}$: A Parameter-Efficient Foundation Model for Molecular\\n  Learning',\n",
       "  'A Self-feedback Knowledge Elicitation Approach for Chemical Reaction\\n  Predictions',\n",
       "  'Mol-AIR: Molecular Reinforcement Learning with Adaptive Intrinsic Rewards for Goal-directed Molecular Generation',\n",
       "  'On the Scalability of GNNs for Molecular Graphs',\n",
       "  'Physical formula enhanced multi-task learning for pharmacokinetics\\n  prediction',\n",
       "  'Physics-informed active learning for accelerating quantum chemical\\n  simulations',\n",
       "  'RLSynC: Offline-Online Reinforcement Learning for Synthon Completion',\n",
       "  'TacoGFN: Target-conditioned GFlowNet for Structure-based Drug Design'},\n",
       " \"Scientists' Perspectives on the Potential for Generative AI in their Fields\": {'AI-Generated Faces in the Real World: A Large-Scale Case Study of\\n  Twitter Profile Images',\n",
       "  'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions',\n",
       "  'An Economic Solution to Copyright Challenges of Generative AI',\n",
       "  'Decoding AI: The inside story of data analysis in ChatGPT',\n",
       "  'Ethical-Lens: Curbing Malicious Usages of Open-Source Text-to-Image\\n  Models',\n",
       "  'Language Model Prompt Selection via Simulation Optimization',\n",
       "  'MUGC: Machine Generated versus User Generated Content Detection',\n",
       "  'iTBLS: A Dataset of Interactive Conversations Over Tabular Information'},\n",
       " 'Validating Large Language Models with ReLM': {'Aligning language models with human preferences',\n",
       "  'Bias patterns in the application of LLMs for clinical decision support:\\n  A comprehensive study',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Learning representations of learning representations',\n",
       "  'Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path\\n  Forward',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'RTP-LX: Can LLMs Evaluate Toxicity in Multilingual Scenarios?',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression'},\n",
       " 'LongNet: Scaling Transformers to 1,000,000,000 Tokens': {'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving',\n",
       "  'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey'},\n",
       " 'Retentive Network: A Successor to Transformer for Large Language Models': {'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'TriForce: Lossless Acceleration of Long Sequence Generation with\\n  Hierarchical Speculative Decoding'},\n",
       " 'Conservative Prediction via Data-Driven Confidence Minimization': {'Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World',\n",
       "  'Explainable Machine Learning System for Predicting Chronic Kidney\\n  Disease in High-Risk Cardiovascular Patients',\n",
       "  'Gradient-Regularized Out-of-Distribution Detection',\n",
       "  'Toward a Realistic Benchmark for Out-of-Distribution Detection',\n",
       "  'Trusted Multi-view Learning with Label Noise',\n",
       "  'Would You Trust an AI Doctor? Building Reliable Medical Predictions with\\n  Kernel Dropout Uncertainty'},\n",
       " 'Learning to Model the World with Language': {'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'Inferring Behavior-Specific Context Improves Zero-Shot Generalization in\\n  Reinforcement Learning',\n",
       "  'LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'Zero-Shot Stitching in Reinforcement Learning using Relative\\n  Representations',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'Path Shadowing Monte-Carlo': {'Estimating the Distribution of Parameters in Differential Equations with\\n  Repeated Cross-Sectional Data',\n",
       "  'Multi-fidelity Gaussian process surrogate modeling for regression\\n  problems in physics',\n",
       "  'Multidimensional Interpolants',\n",
       "  'Neural Flow Diffusion Models: Learnable Forward Process for Improved\\n  Diffusion Modelling',\n",
       "  'Neural McKean-Vlasov Processes: Distributional Dependence in Diffusion\\n  Processes',\n",
       "  'Reinforcement Learning in Agent-Based Market Simulation: Unveiling Realistic Stylized Facts and Behavior',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'Variational Flow Models: Flowing in Your Style'},\n",
       " 'Self-Alignment with Instruction Backtranslation': {'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Aligning language models with human preferences',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Filtered Direct Preference Optimization',\n",
       "  'Learn Your Reference Model for Real Good Alignment',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Bayesian Flow Networks': {'All-in-one simulation-based inference',\n",
       "  'LD-Pruner: Efficient Pruning of Latent Diffusion Models using\\n  Task-Agnostic Insights',\n",
       "  'Neural Flow Diffusion Models: Learnable Forward Process for Improved\\n  Diffusion Modelling',\n",
       "  'Physics-integrated generative modeling using attentive planar\\n  normalizing flow based variational autoencoder',\n",
       "  'SparseDM: Toward Sparse Efficient Diffusion Models',\n",
       "  'TMPQ-DM: Joint Timestep Reduction and Quantization Precision Selection\\n  for Efficient Diffusion Models'},\n",
       " 'Evaluating Embedding APIs for Information Retrieval': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'VDTuner: Automated Performance Tuning for Vector Data Management Systems'},\n",
       " 'Learning One Abstract Bit at a Time Through Self-Invented Experiments Encoded as Neural Networks': {'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Decoding AI: The inside story of data analysis in ChatGPT',\n",
       "  'Deep Policy Optimization with Temporal Logic Constraints',\n",
       "  'From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function',\n",
       "  'Mol-AIR: Molecular Reinforcement Learning with Adaptive Intrinsic Rewards for Goal-directed Molecular Generation',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'The Causal Chambers: Real Physical Systems as a Testbed for AI\\n  Methodology',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing'},\n",
       " 'Teach LLMs to Personalize -- An Approach inspired by Writing Education': {'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Automated Multi-Language to English Machine Translation Using Generative\\n  Pre-Trained Transformers',\n",
       "  'Exploring Text-to-Motion Generation with Human Preference',\n",
       "  'Filtered Direct Preference Optimization',\n",
       "  'Language Model Prompt Selection via Simulation Optimization',\n",
       "  'MUGC: Machine Generated versus User Generated Content Detection',\n",
       "  'Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level\\n  Knowledge Distillation',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Towards a Robust Retrieval-Based Summarization System'},\n",
       " 'Efficient Guided Generation for Large Language Models': {'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Language Model Prompt Selection via Simulation Optimization',\n",
       "  'Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level\\n  Knowledge Distillation',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Toward a Theory of Tokenization in LLMs',\n",
       "  'TriForce: Lossless Acceleration of Long Sequence Generation with\\n  Hierarchical Speculative Decoding',\n",
       "  'σ-GPTs: A New Approach to Autoregressive Models'},\n",
       " 'Visual Instruction Tuning': {'Bridging Vision and Language Spaces with Assignment Prediction',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Cross-Modal Adapter: Parameter-Efficient Transfer Learning Approach for\\n  Vision-Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'RankCLIP: Ranking-Consistent Language-Image Pretraining',\n",
       "  'Self-Supervised Visual Preference Alignment',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'Giraffe: Adventures in Expanding Context Lengths in LLMs': {'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models'},\n",
       " 'Large Language Models as General Pattern Machines': {'Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Toward a Theory of Tokenization in LLMs',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'σ-GPTs: A New Approach to Autoregressive Models'},\n",
       " 'A Neurodiversity-Inspired Solver for the Abstraction \\\\& Reasoning Corpus (ARC) Using Visual Imagery and Program Synthesis': {'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Concept-based Analysis of Neural Networks via Vision-Language Models',\n",
       "  'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'Find The Gap: Knowledge Base Reasoning For Visual Question Answering',\n",
       "  'GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers',\n",
       "  'LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos',\n",
       "  'Pre-trained Vision-Language Models Learn Discoverable Visual Concepts',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing'},\n",
       " 'Learning to Learn Financial Networks for Optimising Momentum Strategies': {'A Survey of Deep Long-Tail Classification Advancements',\n",
       "  'A novel decision fusion approach for sale price prediction using Elastic Net and MOPSO',\n",
       "  'DFWLayer: Differentiable Frank-Wolfe Optimization Layer',\n",
       "  'FR-NAS: Forward-and-Reverse Graph Predictor for Efficient Neural\\n  Architecture Search',\n",
       "  'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'OptiGrad: A Fair and more Efficient Price Elasticity Optimization via a\\n  Gradient Based Learning',\n",
       "  'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'TSLANet: Rethinking Transformers for Time Series Representation Learning'},\n",
       " 'Network Momentum across Asset Classes': {'A novel decision fusion approach for sale price prediction using Elastic Net and MOPSO',\n",
       "  'AGHINT: Attribute-Guided Representation Learning on Heterogeneous\\n  Information Networks with Transformer',\n",
       "  'Conformal Prediction for Stochastic Decision-Making of PV Power in Electricity Markets',\n",
       "  'Conformal Predictive Systems Under Covariate Shift',\n",
       "  'Reinforcement Learning in Agent-Based Market Simulation: Unveiling Realistic Stylized Facts and Behavior',\n",
       "  'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'TSLANet: Rethinking Transformers for Time Series Representation Learning',\n",
       "  'Time-aware Heterogeneous Graph Transformer with Adaptive Attention\\n  Merging for Health Event Prediction',\n",
       "  'Using ARIMA to Predict the Expansion of Subscriber Data Consumption'},\n",
       " 'VolTS: A Volatility-based Trading System to forecast Stock Markets Trend using Statistics and Machine Learning': {'A Guide to Feature Importance Methods for Scientific Inference',\n",
       "  'A novel decision fusion approach for sale price prediction using Elastic Net and MOPSO',\n",
       "  'Conformal Prediction for Stochastic Decision-Making of PV Power in Electricity Markets',\n",
       "  'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting',\n",
       "  'Integrating Marketing Channels into Quantile Transformation and Bayesian\\n  Optimization of Ensemble Kernels for Sales Prediction with Gaussian Process\\n  Models',\n",
       "  'Reinforcement Learning in Agent-Based Market Simulation: Unveiling Realistic Stylized Facts and Behavior',\n",
       "  'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'TSLANet: Rethinking Transformers for Time Series Representation Learning',\n",
       "  'Using ARIMA to Predict the Expansion of Subscriber Data Consumption'},\n",
       " 'Graph of Thoughts: Solving Elaborate Problems with Large Language Models': {'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies': {'Aligning language models with human preferences',\n",
       "  'Bias patterns in the application of LLMs for clinical decision support:\\n  A comprehensive study',\n",
       "  'Filtered Direct Preference Optimization',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'RTP-LX: Can LLMs Evaluate Toxicity in Multilingual Scenarios?',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model',\n",
       "  'Using LLMs to Model the Beliefs and Preferences of Targeted Populations'},\n",
       " 'Gated recurrent neural networks discover attention': {'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey',\n",
       "  'Toward a Theory of Tokenization in LLMs',\n",
       "  'Training neural networks with structured noise improves classification and generalization'},\n",
       " 'Code Llama: Open Foundation Models for Code': {'Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\\n  Prediction',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'TriForce: Lossless Acceleration of Long Sequence Generation with\\n  Hierarchical Speculative Decoding',\n",
       "  'XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\\n  Upcycled Mixture-of-Experts'},\n",
       " 'One Wide Feedforward is All You Need': {'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey'},\n",
       " 'Nougat: Neural Optical Understanding for Academic Documents': {'AlloyBERT: Alloy Property Prediction with Large Language Models',\n",
       "  'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers',\n",
       "  'GatedLexiconNet: A Comprehensive End-to-End Handwritten Paragraph Text\\n  Recognition System',\n",
       "  'MathWriting: A Dataset For Handwritten Mathematical Expression\\n  Recognition',\n",
       "  'PARAMANU-GANITA: Language Model with Mathematical Capabilities',\n",
       "  'Residual-based Language Models are Free Boosters for Biomedical Imaging',\n",
       "  'Towards a Foundation Model for Partial Differential Equation:\\n  Multi-Operator Learning and Extrapolation'},\n",
       " 'Prompt2Model: Generating Deployable Models from Natural Language Instructions': {'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'Stronger Random Baselines for In-Context Learning'},\n",
       " 'Trading via Selective Classification': {'A novel decision fusion approach for sale price prediction using Elastic Net and MOPSO',\n",
       "  'Conformal Prediction for Stochastic Decision-Making of PV Power in Electricity Markets',\n",
       "  'DeepHeteroIoT: Deep Local and Global Learning over Heterogeneous IoT Sensor Data',\n",
       "  'Learning using granularity statistical invariants for classification',\n",
       "  'Mining Invariance from Nonlinear Multi-Environment Data: Binary\\n  Classification',\n",
       "  'OptiGrad: A Fair and more Efficient Price Elasticity Optimization via a\\n  Gradient Based Learning',\n",
       "  'Reinforcement Learning in Agent-Based Market Simulation: Unveiling Realistic Stylized Facts and Behavior',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'Using ARIMA to Predict the Expansion of Subscriber Data Consumption',\n",
       "  'VC Theory for Inventory Policies'},\n",
       " 'Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning': {'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'Cross-Modal Adapter: Parameter-Efficient Transfer Learning Approach for\\n  Vision-Language Models',\n",
       "  'Data-Efficient Multimodal Fusion on a Single GPU',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept\\n  Understanding',\n",
       "  'RankCLIP: Ranking-Consistent Language-Image Pretraining',\n",
       "  'Residual-based Language Models are Free Boosters for Biomedical Imaging',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'Are Emergent Abilities in Large Language Models just In-Context Learning?': {'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Large Language Models as Optimizers': {'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\\n  Prediction',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " \"Fluid Transformers and Creative Analogies: Exploring Large Language Models' Capacity for Augmenting Cross-Domain Analogical Creativity\": {'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level\\n  Knowledge Distillation',\n",
       "  'Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept\\n  Understanding',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'NExT-GPT: Any-to-Any Multimodal LLM': {'Bridging Vision and Language Spaces with Assignment Prediction',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Cross-Modal Adapter: Parameter-Efficient Transfer Learning Approach for\\n  Vision-Language Models',\n",
       "  'Data-Efficient Multimodal Fusion on a Single GPU',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Sparse multimodal fusion with modal channel attention',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'Cognitive Architectures for Language Agents': {'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions',\n",
       "  'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'RAIN: Your Language Models Can Align Themselves without Finetuning': {'Aligning language models with human preferences',\n",
       "  'Filtered Direct Preference Optimization',\n",
       "  'Learn Your Reference Model for Real Good Alignment',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Stepwise Alignment for Constrained Language Model Policy Optimization',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'RWKV: Reinventing RNNs for the Transformer Era': {'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey'},\n",
       " 'Large Language Model for Science: A Study on P vs. NP': {'CT-Agent: Clinical Trial Multi-Agent with Large Language Model-based\\n  Reasoning',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'Self-playing Adversarial Language Game Enhances LLM Reasoning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'FedEval-LLM: Federated Evaluation of Large Language Models on Downstream\\n  Tasks with Collective Wisdom',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'Stronger Random Baselines for In-Context Learning',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Agents: An Open-source Framework for Autonomous Language Agents': {'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions',\n",
       "  'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models',\n",
       "  'Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\\n  Prediction',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing'},\n",
       " 'Test-Time Training with Self-Supervision for Generalization under Distribution Shifts': {'A Survey of Deep Long-Tail Classification Advancements',\n",
       "  'Distilled Datamodel with Reverse Gradient Matching',\n",
       "  'Gradient-Regularized Out-of-Distribution Detection',\n",
       "  'Mining Invariance from Nonlinear Multi-Environment Data: Binary\\n  Classification',\n",
       "  'NC-TTT: A Noise Contrastive Approach for Test-Time Training',\n",
       "  'OTTER: Improving Zero-Shot Classification via Optimal Transport',\n",
       "  'Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer',\n",
       "  'Toward a Realistic Benchmark for Out-of-Distribution Detection'},\n",
       " 'The Rise and Potential of Large Language Model Based Agents: A Survey': {'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions',\n",
       "  'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Aligning language models with human preferences',\n",
       "  'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Self-playing Adversarial Language Game Enhances LLM Reasoning',\n",
       "  'Stepwise Alignment for Constrained Language Model Policy Optimization'},\n",
       " 'Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers': {'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Language Model Prompt Selection via Simulation Optimization',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing'},\n",
       " 'Textbooks Are All You Need II: phi-1.5 technical report': {'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'Transformers Can Represent $n$-gram Language Models',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'PDFTriage: Question Answering over Long, Structured Documents': {'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Latxa: An Open Language Model and Evaluation Suite for Basque',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Towards a Robust Retrieval-Based Summarization System'},\n",
       " 'Training Convolutional Networks with Noisy Labels': {'Can We Break Free from Strong Data Augmentations in Self-Supervised\\n  Learning?',\n",
       "  'DeNetDM: Debiasing by Network Depth Modulation',\n",
       "  'Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World',\n",
       "  'Gradient-Regularized Out-of-Distribution Detection',\n",
       "  'NC-TTT: A Noise Contrastive Approach for Test-Time Training',\n",
       "  'Noise contrastive estimation with soft targets for conditional models',\n",
       "  'OTTER: Improving Zero-Shot Classification via Optimal Transport',\n",
       "  'Toward a Realistic Benchmark for Out-of-Distribution Detection'},\n",
       " 'Understanding and Utilizing Deep Neural Networks Trained with Noisy Labels': {'Can We Break Free from Strong Data Augmentations in Self-Supervised\\n  Learning?',\n",
       "  'DeNetDM: Debiasing by Network Depth Modulation',\n",
       "  'NC-TTT: A Noise Contrastive Approach for Test-Time Training',\n",
       "  'Noise contrastive estimation with soft targets for conditional models',\n",
       "  'Optimizing Calibration by Gaining Aware of Prediction Correctness',\n",
       "  'Positive Unlabeled Contrastive Learning',\n",
       "  'Reactive Model Correction: Mitigating Harm to Task-Relevant Features via\\n  Conditional Bias Suppression',\n",
       "  'Trusted Multi-view Learning with Label Noise'},\n",
       " 'Generating Images with Multimodal Language Models': {'Bridging Vision and Language Spaces with Assignment Prediction',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Cross-Modal Adapter: Parameter-Efficient Transfer Learning Approach for\\n  Vision-Language Models',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept\\n  Understanding',\n",
       "  'Residual-based Language Models are Free Boosters for Biomedical Imaging',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'iRAG: An Incremental Retrieval Augmented Generation System for Videos',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'Selective Mixup Helps with Distribution Shifts, But Not (Only) because of Mixup': {'A Survey of Deep Long-Tail Classification Advancements',\n",
       "  'DeNetDM: Debiasing by Network Depth Modulation',\n",
       "  'Gradient-Regularized Out-of-Distribution Detection',\n",
       "  'Improving Generalization via Meta-Learning on Hard Samples',\n",
       "  'Mining Invariance from Nonlinear Multi-Environment Data: Binary\\n  Classification',\n",
       "  'NC-TTT: A Noise Contrastive Approach for Test-Time Training',\n",
       "  'Noise contrastive estimation with soft targets for conditional models',\n",
       "  'OTTER: Improving Zero-Shot Classification via Optimal Transport'},\n",
       " 'The Future of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence': {'A Self-feedback Knowledge Elicitation Approach for Chemical Reaction\\n  Predictions',\n",
       "  'Decoding AI: The inside story of data analysis in ChatGPT',\n",
       "  'Enhancing Counterfactual Explanation Search with Diffusion Distance and\\n  Directional Coherence',\n",
       "  'Explainable Artificial Intelligence Techniques for Accurate Fault\\n  Detection and Diagnosis: A Review',\n",
       "  'FMint: Bridging Human Designed and Data Pretrained Models for\\n  Differential Equation Foundation Model',\n",
       "  'LLM-Enhanced Causal Discovery in Temporal Domain from Interventional\\n  Data',\n",
       "  'Learning to Solve the Constrained Most Probable Explanation Task in\\n  Probabilistic Graphical Models',\n",
       "  'Mol-AIR: Molecular Reinforcement Learning with Adaptive Intrinsic Rewards for Goal-directed Molecular Generation',\n",
       "  'Sparse Explanations of Neural Networks Using Pruned Layer-Wise Relevance\\n  Propagation',\n",
       "  'The Causal Chambers: Real Physical Systems as a Testbed for AI\\n  Methodology'},\n",
       " 'Auto-Regressive Next-Token Predictors are Universal Learners': {'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'Toward a Theory of Tokenization in LLMs',\n",
       "  'Transformers Can Represent $n$-gram Language Models',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'Neural Graphical Models': {'A Review of Graph Neural Networks in Epidemic Modeling',\n",
       "  'Graph Machine Learning in the Era of Large Language Models (LLMs)',\n",
       "  'Graph Neural Aggregation-diffusion with Metastability',\n",
       "  'Graph Neural Networks for Protein-Protein Interactions - A Short Survey',\n",
       "  'Improving the interpretability of GNN predictions through\\n  conformal-based graph sparsification',\n",
       "  'Multi-View Subgraph Neural Networks: Self-Supervised Learning with\\n  Scarce Labeled Data',\n",
       "  'Time-aware Heterogeneous Graph Transformer with Adaptive Attention\\n  Merging for Health Event Prediction',\n",
       "  'Uncertainty Quantification on Graph Learning: A Survey'},\n",
       " 'TSMixer: An All-MLP Architecture for Time Series Forecasting': {'Computationally and Memory-Efficient Robust Predictive Analytics Using Big Data',\n",
       "  'DeepHeteroIoT: Deep Local and Global Learning over Heterogeneous IoT Sensor Data',\n",
       "  'Feature-Based Echo-State Networks: A Step Towards Interpretability and Minimalism in Reservoir Computer',\n",
       "  'Generating Synthetic Time Series Data for Cyber-Physical Systems',\n",
       "  'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'TSLANet: Rethinking Transformers for Time Series Representation Learning',\n",
       "  'Variational quantization for state space models'},\n",
       " 'Physics of Language Models: Part 1, Context-Free Grammar': {'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'Localizing Paragraph Memorization in Language Models',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Toward a Theory of Tokenization in LLMs',\n",
       "  'Transformers Can Represent $n$-gram Language Models',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model',\n",
       "  'What do Transformers Know about Government?',\n",
       "  'σ-GPTs: A New Approach to Autoregressive Models'},\n",
       " 'Directly Fine-Tuning Diffusion Models on Differentiable Rewards': {'Balanced Mixed-Type Tabular Data Synthesis with Diffusion Models',\n",
       "  'Dataset Reset Policy Optimization for RLHF',\n",
       "  'EdgeFusion: On-Device Text-to-Image Generation',\n",
       "  'Filtered Direct Preference Optimization',\n",
       "  'Gradient Guidance for Diffusion Models: An Optimization Perspective',\n",
       "  'Neural Flow Diffusion Models: Learnable Forward Process for Improved\\n  Diffusion Modelling',\n",
       "  'Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging\\n  Perturbations That Efficiently Fool Customized Diffusion Models',\n",
       "  'Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept\\n  Understanding',\n",
       "  'SparseDM: Toward Sparse Efficient Diffusion Models',\n",
       "  'TMPQ-DM: Joint Timestep Reduction and Quantization Precision Selection\\n  for Efficient Diffusion Models'},\n",
       " 'The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)': {'Bridging Vision and Language Spaces with Assignment Prediction',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'Harnessing GPT-4V(ision) for Insurance: A Preliminary Exploration',\n",
       "  'Pre-trained Vision-Language Models Learn Discoverable Visual Concepts',\n",
       "  'Self-Supervised Visual Preference Alignment',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution': {'Aligning language models with human preferences',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Self-playing Adversarial Language Game Enhances LLM Reasoning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Hypothesis Search: Inductive Reasoning with Language Models': {'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'Pre-trained Vision-Language Models Learn Discoverable Visual Concepts',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning'},\n",
       " 'Representation Engineering: A Top-Down Approach to AI Transparency': {'Closing the Gap in the Trade-off between Fair Representations and\\n  Accuracy',\n",
       "  'Decomposing and Editing Predictions by Modeling Model Computation',\n",
       "  'GenFighter: A Generative and Evolutive Textual Attack Removal',\n",
       "  'Holistic Safety and Responsibility Evaluations of Advanced AI Models',\n",
       "  'Private Attribute Inference from Images with Vision-Language Models',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'RTP-LX: Can LLMs Evaluate Toxicity in Multilingual Scenarios?',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'Self-playing Adversarial Language Game Enhances LLM Reasoning',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Can large language models provide useful feedback on research papers? A large-scale empirical analysis': {'Beyond Code Generation: An Observational Study of ChatGPT Usage in\\n  Software Engineering Practice',\n",
       "  'Bias patterns in the application of LLMs for clinical decision support:\\n  A comprehensive study',\n",
       "  'CT-Agent: Clinical Trial Multi-Agent with Large Language Model-based\\n  Reasoning',\n",
       "  'FedEval-LLM: Federated Evaluation of Large Language Models on Downstream\\n  Tasks with Collective Wisdom',\n",
       "  \"Is ChatGPT Transforming Academics' Writing Style?\",\n",
       "  'Learning representations of learning representations',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'STRUM-LLM: Attributed and Structured Contrastive Summarization'},\n",
       " 'Borges and AI': {'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions',\n",
       "  'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'MUGC: Machine Generated versus User Generated Content Detection',\n",
       "  \"On Large Language Models' Hallucination with Regard to Known Facts\",\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Taxonomy to Regulation: A (Geo)Political Taxonomy for AI Risks and\\n  Regulatory Measures in the EU AI Act',\n",
       "  'Transformers Can Represent $n$-gram Language Models',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation': {'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Automating REST API Postman Test Cases Using LLM',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'LLM-Powered Test Case Generation for Detecting Tricky Bugs',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\\n  Upcycled Mixture-of-Experts'},\n",
       " 'Deep Neural Networks Tend To Extrapolate Predictably': {'A Survey of Deep Long-Tail Classification Advancements',\n",
       "  'Combining Statistical Depth and Fermat Distance for Uncertainty\\n  Quantification',\n",
       "  'Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World',\n",
       "  'Gradient-Regularized Out-of-Distribution Detection',\n",
       "  'Noise contrastive estimation with soft targets for conditional models',\n",
       "  'Reactive Model Correction: Mitigating Harm to Task-Relevant Features via\\n  Conditional Bias Suppression',\n",
       "  'Strong Transferable Adversarial Attacks via Ensembled Asymptotically Normal Distribution Learning',\n",
       "  'Toward a Realistic Benchmark for Out-of-Distribution Detection'},\n",
       " 'How Two-Layer Neural Networks Learn, One (Giant) Step at a Time': {'Deep Neural Networks via Complex Network Theory: a Perspective',\n",
       "  'How Deep Networks Learn Sparse and Hierarchical Data: the Sparse Random\\n  Hierarchy Model',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'Matching the Statistical Query Lower Bound for k-sparse Parity Problems\\n  with Stochastic Gradient Descent',\n",
       "  'Noise contrastive estimation with soft targets for conditional models',\n",
       "  'Sliding down the stairs: how correlated latent variables accelerate\\n  learning with neural networks',\n",
       "  'Strong Transferable Adversarial Attacks via Ensembled Asymptotically Normal Distribution Learning',\n",
       "  'Training neural networks with structured noise improves classification and generalization'},\n",
       " 'Explaining grokking through circuit efficiency': {'Can We Break Free from Strong Data Augmentations in Self-Supervised\\n  Learning?',\n",
       "  'Deep Neural Networks via Complex Network Theory: a Perspective',\n",
       "  'Deep Regression Representation Learning with Topology',\n",
       "  'Delayed Bottlenecking: Alleviating Forgetting in Pre-trained Graph\\n  Neural Networks',\n",
       "  'Localizing Paragraph Memorization in Language Models',\n",
       "  'Mining Invariance from Nonlinear Multi-Environment Data: Binary\\n  Classification',\n",
       "  'On Size and Hardness Generalization in Unsupervised Learning for the Travelling Salesman Problem',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective',\n",
       "  'Training neural networks with structured noise improves classification and generalization'},\n",
       " 'MAPTree: Beating \"Optimal\" Decision Trees with Bayesian Decision Trees': {'AI Competitions and Benchmarks: Dataset Development',\n",
       "  'Application of the representative measure approach to assess the\\n  reliability of decision trees in dealing with unseen vehicle collision data',\n",
       "  'BayesJudge: Bayesian Kernel Language Modelling with Confidence\\n  Uncertainty in Legal Judgment Prediction',\n",
       "  'Explainable LightGBM Approach for Predicting Myocardial Infarction\\n  Mortality',\n",
       "  'Finding Decision Tree Splits in Streaming and Massively Parallel Models',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Tree Bandits for Generative Bayes',\n",
       "  'floZ: Evidence estimation from posterior samples with normalizing flows'},\n",
       " 'Neuro-Modulated Hebbian Learning for Fully Test-Time Adaptation': {'Actor-Critic Reinforcement Learning with Phased Actor',\n",
       "  'ColA: Collaborative Adaptation with Gradient Learning',\n",
       "  'Employing Layerwised Unsupervised Learning to Lessen Data and Loss\\n  Requirements in Forward-Forward Algorithms',\n",
       "  'Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'NC-TTT: A Noise Contrastive Approach for Test-Time Training',\n",
       "  'Reactive Model Correction: Mitigating Harm to Task-Relevant Features via\\n  Conditional Bias Suppression',\n",
       "  'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective'},\n",
       " 'DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines': {'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving',\n",
       "  'Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\\n  Prediction',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'Large Language Models can Learn Rules': {'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  \"On Large Language Models' Hallucination with Regard to Known Facts\",\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction'},\n",
       " 'MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens': {'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Data-Efficient Multimodal Fusion on a Single GPU',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept\\n  Understanding',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'iRAG: An Incremental Retrieval Augmented Generation System for Videos',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'VeRA: Vector-based Random Matrix Adaptation': {'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LoRA Dropout as a Sparsity Regularizer for Overfitting Control',\n",
       "  'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'Personalized Collaborative Fine-Tuning for On-Device Large Language\\n  Models',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages'},\n",
       " 'Context-Aware Meta-Learning': {'Bridging Vision and Language Spaces with Assignment Prediction',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Cross-Modal Adapter: Parameter-Efficient Transfer Learning Approach for\\n  Vision-Language Models',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'RankCLIP: Ranking-Consistent Language-Image Pretraining',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'In-Context Learning for Few-Shot Molecular Property Prediction': {'$\\\\texttt{MiniMol}$: A Parameter-Efficient Foundation Model for Molecular\\n  Learning',\n",
       "  'A Python library for efficient computation of molecular fingerprints',\n",
       "  'A Self-feedback Knowledge Elicitation Approach for Chemical Reaction\\n  Predictions',\n",
       "  'HiGraphDTI: Hierarchical Graph Representation Learning for Drug-Target\\n  Interaction Prediction',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Simple, Efficient and Scalable Structure-aware Adapter Boosts Protein\\n  Language Models',\n",
       "  'When LLMs are Unfit Use FastFit: Fast and Effective Text Classification\\n  with Many Classes'},\n",
       " 'Reset It and Forget It: Relearning Last-Layer Weights Improves Continual and Transfer Learning': {'Can We Break Free from Strong Data Augmentations in Self-Supervised\\n  Learning?',\n",
       "  'ColA: Collaborative Adaptation with Gradient Learning',\n",
       "  'Continual Learning on a Diet: Learning from Sparsely Labeled Streams\\n  Under Constrained Computation',\n",
       "  'Dynamically Anchored Prompting for Task-Imbalanced Continual Learning',\n",
       "  'Improving Generalization via Meta-Learning on Hard Samples',\n",
       "  'NC-TTT: A Noise Contrastive Approach for Test-Time Training',\n",
       "  'OTTER: Improving Zero-Shot Classification via Optimal Transport',\n",
       "  'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective',\n",
       "  'SOPHON: Non-Fine-Tunable Learning to Restrain Task Transferability For\\n  Pre-trained Models',\n",
       "  'Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer'},\n",
       " 'Few-Shot Learning Patterns in Financial Time-Series for Trend-Following Strategies': {'A Comparison of Traditional and Deep Learning Methods for Parameter\\n  Estimation of the Ornstein-Uhlenbeck Process',\n",
       "  'DeepHeteroIoT: Deep Local and Global Learning over Heterogeneous IoT Sensor Data',\n",
       "  'Generating Synthetic Time Series Data for Cyber-Physical Systems',\n",
       "  'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'Reinforcement Learning in Agent-Based Market Simulation: Unveiling Realistic Stylized Facts and Behavior',\n",
       "  'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'TSLANet: Rethinking Transformers for Time Series Representation Learning',\n",
       "  'Variational quantization for state space models'},\n",
       " 'Pseudointelligence: A Unifying Framework for Language Model Evaluation': {'FedEval-LLM: Federated Evaluation of Large Language Models on Downstream\\n  Tasks with Collective Wisdom',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'Stronger Random Baselines for In-Context Learning',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'Transformers Can Represent $n$-gram Language Models',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Retrieving Texts based on Abstract Descriptions': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'Conversational Financial Information Retrieval Model (ConFIRM)',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering'},\n",
       " 'Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence': {'GenFighter: A Generative and Evolutive Textual Attack Removal',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging\\n  Perturbations That Efficiently Fool Customized Diffusion Models',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'Talk Too Much: Poisoning Large Language Models under Token Limit',\n",
       "  'Toward a Theory of Tokenization in LLMs'},\n",
       " 'Attention over pre-trained Sentence Embeddings for Long Document Classification': {'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'LayerNorm: A key component in parameter-efficient fine-tuning',\n",
       "  'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey'},\n",
       " 'The Consensus Game: Language Model Generation via Equilibrium Search': {'Aligning language models with human preferences',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning',\n",
       "  'Towards a Robust Retrieval-Based Summarization System',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Branch-Solve-Merge Improves Large Language Model Evaluation and Generation': {'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Language Model Prompt Selection via Simulation Optimization',\n",
       "  'Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level\\n  Knowledge Distillation',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning',\n",
       "  'Towards a Robust Retrieval-Based Summarization System'},\n",
       " 'What Algorithms can Transformers Learn? A Study in Length Generalization': {'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'Toward a Theory of Tokenization in LLMs',\n",
       "  'Transformers Can Represent $n$-gram Language Models',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'What do Transformers Know about Government?'},\n",
       " 'In-Context Learning Creates Task Vectors': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy\\n  Data',\n",
       "  'Stronger Random Baselines for In-Context Learning',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'PaRaDe: Passage Ranking using Demonstrations with Large Language Models': {'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Language Model Prompt Selection via Simulation Optimization',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'Stronger Random Baselines for In-Context Learning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards a Robust Retrieval-Based Summarization System',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions': {'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  \"On Large Language Models' Hallucination with Regard to Known Facts\",\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents': {'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards a Robust Retrieval-Based Summarization System'},\n",
       " 'Papeos: Augmenting Research Papers with Talk Videos': {'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Beyond Code Generation: An Observational Study of ChatGPT Usage in\\n  Software Engineering Practice',\n",
       "  'Exploring Text-to-Motion Generation with Human Preference',\n",
       "  \"Is ChatGPT Transforming Academics' Writing Style?\",\n",
       "  'STRUM-LLM: Attributed and Structured Contrastive Summarization',\n",
       "  'SuperNOVA: Design Strategies and Opportunities for Interactive Visualization in Computational Notebooks',\n",
       "  'iRAG: An Incremental Retrieval Augmented Generation System for Videos',\n",
       "  'iTBLS: A Dataset of Interactive Conversations Over Tabular Information'},\n",
       " 'Codebook Features: Sparse and Discrete Interpretability for Neural Networks': {'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'Neuro-Inspired Information-Theoretic Hierarchical Perception for\\n  Multimodal Learning',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'Tripod: Three Complementary Inductive Biases for Disentangled\\n  Representation Learning'},\n",
       " 'How do Language Models Bind Entities in Context?': {'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Localizing Paragraph Memorization in Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'Semantic Cells: Evolutional Process to Acquire Sense Diversity of Items',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'Transformers Can Represent $n$-gram Language Models',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'CodeFusion: A Pre-trained Diffusion Model for Code Generation': {'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Automated Multi-Language to English Machine Translation Using Generative\\n  Pre-Trained Transformers',\n",
       "  'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level\\n  Knowledge Distillation',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\\n  Upcycled Mixture-of-Experts',\n",
       "  'σ-GPTs: A New Approach to Autoregressive Models'},\n",
       " 'AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation': {'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Automating REST API Postman Test Cases Using LLM',\n",
       "  'Beyond Code Generation: An Observational Study of ChatGPT Usage in\\n  Software Engineering Practice',\n",
       "  'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'ReALM: Reference Resolution As Language Modeling',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'iTBLS: A Dataset of Interactive Conversations Over Tabular Information'},\n",
       " 'Universal Self-Adaptive Prompting': {'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'Stronger Random Baselines for In-Context Learning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'Beyond Words: A Mathematical Framework for Interpreting Large Language Models': {'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Detecting and Mitigating Hallucination in Large Vision Language Models\\n  via Fine-Grained AI Feedback',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  \"On Large Language Models' Hallucination with Regard to Known Facts\",\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling': {'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey',\n",
       "  'TriForce: Lossless Acceleration of Long Sequence Generation with\\n  Hierarchical Speculative Decoding'},\n",
       " 'Unraveling the ARC Puzzle: Mimicking Human Solutions with Object-Centric Decision Transformer': {'ASID: Active Exploration for System Identification in Robotic\\n  Manipulation',\n",
       "  'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V',\n",
       "  'Decoding AI: The inside story of data analysis in ChatGPT',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function'},\n",
       " 'LILO: Learning Interpretable Libraries by Compressing and Documenting Code': {'Automated Multi-Language to English Machine Translation Using Generative\\n  Pre-Trained Transformers',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\\n  Upcycled Mixture-of-Experts'},\n",
       " 'Meta-learning of semi-supervised learning from tasks with heterogeneous attribute spaces': {'Beyond the Known: Novel Class Discovery for Open-world Graph Learning',\n",
       "  'DeNetDM: Debiasing by Network Depth Modulation',\n",
       "  'Improving Generalization via Meta-Learning on Hard Samples',\n",
       "  'Integration of Self-Supervised BYOL in Semi-Supervised Medical Image\\n  Recognition',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'Meta-Learning with Generalized Ridge Regression: High-dimensional Asymptotics, Optimality and Hyper-covariance Estimation',\n",
       "  'OTTER: Improving Zero-Shot Classification via Optimal Transport',\n",
       "  'VCC-INFUSE: Towards Accurate and Efficient Selection of Unlabeled\\n  Examples in Semi-supervised Learning'},\n",
       " 'Dense Passage Retrieval for Open-Domain Question Answering': {'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Find The Gap: Knowledge Base Reasoning For Visual Question Answering',\n",
       "  'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking',\n",
       "  'Latxa: An Open Language Model and Evaluation Suite for Basque',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'QAGCN: Answering Multi-Relation Questions via Single-Step Implicit Reasoning over Knowledge Graphs',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'Small Models Are (Still) Effective Cross-Domain Argument Extractors'},\n",
       " 'Agent Lumos: Unified and Modular Training for Open-Source Language Agents': {'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Inferring Behavior-Specific Context Improves Zero-Shot Generalization in\\n  Reinforcement Learning',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'Evaluation of Neural Architectures Trained with Square Loss vs Cross-Entropy in Classification Tasks': {'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'NC-TTT: A Noise Contrastive Approach for Test-Time Training',\n",
       "  'Neuro-Inspired Information-Theoretic Hierarchical Perception for\\n  Multimodal Learning',\n",
       "  'Next Generation Loss Function for Image Classification',\n",
       "  'Noise contrastive estimation with soft targets for conditional models',\n",
       "  'Nonlinearity Enhanced Adaptive Activation Function',\n",
       "  'PQA: Exploring the Potential of Product Quantization in DNN Hardware Acceleration',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'Toward a Theory of Tokenization in LLMs'},\n",
       " 'The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4': {'A Self-feedback Knowledge Elicitation Approach for Chemical Reaction\\n  Predictions',\n",
       "  'AlloyBERT: Alloy Property Prediction with Large Language Models',\n",
       "  'CT-Agent: Clinical Trial Multi-Agent with Large Language Model-based\\n  Reasoning',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'On the Scalability of GNNs for Molecular Graphs',\n",
       "  'Simple, Efficient and Scalable Structure-aware Adapter Boosts Protein\\n  Language Models',\n",
       "  'Towards a Foundation Model for Partial Differential Equation:\\n  Multi-Operator Learning and Extrapolation'},\n",
       " 'Cappy: Outperforming and Boosting Large Multi-Task LMs with a Small Scorer': {'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models'},\n",
       " 'Ghostbuster: Detecting Text Ghostwritten by Large Language Models': {'AI-Generated Faces in the Real World: A Large-Scale Case Study of\\n  Twitter Profile Images',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Does It Make Sense to Explain a Black Box With Another Black Box?',\n",
       "  'GenFighter: A Generative and Evolutive Textual Attack Removal',\n",
       "  'Learning representations of learning representations',\n",
       "  'MUGC: Machine Generated versus User Generated Content Detection',\n",
       "  'ParaFusion: A Large-Scale LLM-Driven English Paraphrase Dataset Infused\\n  with High-Quality Lexical and Syntactic Diversity',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'Robust CLIP-Based Detector for Exposing Diffusion Model-Generated Images',\n",
       "  'Talk Too Much: Poisoning Large Language Models under Token Limit'},\n",
       " 'Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks': {'Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second': {'Decomposing and Editing Predictions by Modeling Model Computation',\n",
       "  'Distilled Datamodel with Reverse Gradient Matching',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Learning to Solve the Constrained Most Probable Explanation Task in\\n  Probabilistic Graphical Models',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'Measuring Feature Dependency of Neural Networks by Collapsing Feature\\n  Dimensions in the Data Manifold',\n",
       "  'OTTER: Improving Zero-Shot Classification via Optimal Transport',\n",
       "  'Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science'},\n",
       " 'When Do Neural Nets Outperform Boosted Trees on Tabular Data?': {'A Survey of Deep Long-Tail Classification Advancements',\n",
       "  'AI Competitions and Benchmarks: Dataset Development',\n",
       "  'Application of the representative measure approach to assess the\\n  reliability of decision trees in dealing with unseen vehicle collision data',\n",
       "  'DeNetDM: Debiasing by Network Depth Modulation',\n",
       "  'Distilled Datamodel with Reverse Gradient Matching',\n",
       "  'Improving Generalization via Meta-Learning on Hard Samples',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Structured Evaluation of Synthetic Tabular Data',\n",
       "  'Towards Stable Machine Learning Model Retraining via Slowly Varying Sequences',\n",
       "  'Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science'},\n",
       " 'Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models': {'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing'},\n",
       " 'Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as an Alternative to Attention Layers in Transformers': {'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Enhancing Efficiency in Vision Transformer Networks: Design Techniques and Insights',\n",
       "  'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers',\n",
       "  'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey'},\n",
       " 'System 2 Attention (is something you might need too)': {'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning'},\n",
       " 'RA-DIT: Retrieval-Augmented Dual Instruction Tuning': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering'},\n",
       " 'MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning': {'Bias patterns in the application of LLMs for clinical decision support:\\n  A comprehensive study',\n",
       "  'CT-Agent: Clinical Trial Multi-Agent with Large Language Model-based\\n  Reasoning',\n",
       "  'Data Alignment for Zero-Shot Concept Generation in Dermatology AI',\n",
       "  'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'FedEval-LLM: Federated Evaluation of Large Language Models on Downstream\\n  Tasks with Collective Wisdom',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'Would You Trust an AI Doctor? Building Reliable Medical Predictions with\\n  Kernel Dropout Uncertainty'},\n",
       " 'Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models': {'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LayerNorm: A key component in parameter-efficient fine-tuning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'Stronger Random Baselines for In-Context Learning',\n",
       "  'Toward a Theory of Tokenization in LLMs',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'Text Embeddings Reveal (Almost) As Much As Text': {'Distributional Black-Box Model Inversion Attack with Multi-Agent\\n  Reinforcement Learning',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'PATE-TripleGAN: Privacy-Preserving Image Synthesis with Gaussian\\n  Differential Privacy',\n",
       "  'Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging\\n  Perturbations That Efficiently Fool Customized Diffusion Models',\n",
       "  'Private Attribute Inference from Images with Vision-Language Models',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'VFLGAN: Vertical Federated Learning-based Generative Adversarial Network\\n  for Vertically Partitioned Data Publication'},\n",
       " 'Scaling TabPFN: Sketching and Feature Selection for Tabular Prior-Data Fitted Networks': {'An improved tabular data generator with VAE-GMM integration',\n",
       "  'Application of the representative measure approach to assess the\\n  reliability of decision trees in dealing with unseen vehicle collision data',\n",
       "  'Deep Regression Representation Learning with Topology',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Measuring Feature Dependency of Neural Networks by Collapsing Feature\\n  Dimensions in the Data Manifold',\n",
       "  'Structured Evaluation of Synthetic Tabular Data',\n",
       "  'Synthesizing Realistic Data for Table Recognition',\n",
       "  'Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science'},\n",
       " 'Fine-Tuning the Retrieval Mechanism for Tabular Deep Learning': {'Delayed Bottlenecking: Alleviating Forgetting in Pre-trained Graph\\n  Neural Networks',\n",
       "  'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'FR-NAS: Forward-and-Reverse Graph Predictor for Efficient Neural\\n  Architecture Search',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'Structured Evaluation of Synthetic Tabular Data',\n",
       "  'Synthesizing Realistic Data for Table Recognition',\n",
       "  'Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science'},\n",
       " 'XTab: Cross-table Pretraining for Tabular Transformers': {'An improved tabular data generator with VAE-GMM integration',\n",
       "  'Can We Break Free from Strong Data Augmentations in Self-Supervised\\n  Learning?',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'One-Shot Sequential Federated Learning for Non-IID Data by Enhancing\\n  Local Model Diversity',\n",
       "  'Structured Evaluation of Synthetic Tabular Data',\n",
       "  'Synthesizing Realistic Data for Table Recognition',\n",
       "  'Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science'},\n",
       " 'Deep incremental learning models for financial temporal tabular datasets with distribution shifts': {'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting',\n",
       "  'Integrating Marketing Channels into Quantile Transformation and Bayesian\\n  Optimization of Ensemble Kernels for Sales Prediction with Gaussian Process\\n  Models',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'Mining Invariance from Nonlinear Multi-Environment Data: Binary\\n  Classification',\n",
       "  'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'TSLANet: Rethinking Transformers for Time Series Representation Learning',\n",
       "  'Towards Stable Machine Learning Model Retraining via Slowly Varying Sequences',\n",
       "  'Using ARIMA to Predict the Expansion of Subscriber Data Consumption',\n",
       "  'Variational quantization for state space models'},\n",
       " 'Deep Regression Ensembles': {'Can We Break Free from Strong Data Augmentations in Self-Supervised\\n  Learning?',\n",
       "  'DeNetDM: Debiasing by Network Depth Modulation',\n",
       "  'Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World',\n",
       "  'Gradient-Regularized Out-of-Distribution Detection',\n",
       "  'Measuring Feature Dependency of Neural Networks by Collapsing Feature\\n  Dimensions in the Data Manifold',\n",
       "  'Reactive Model Correction: Mitigating Harm to Task-Relevant Features via\\n  Conditional Bias Suppression'},\n",
       " 'War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars': {'A survey of air combat behavior modeling using machine learning',\n",
       "  'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'LLM-Enhanced Causal Discovery in Temporal Domain from Interventional\\n  Data',\n",
       "  'Self-playing Adversarial Language Game Enhances LLM Reasoning',\n",
       "  'Social Choice for AI Alignment: Dealing with Diverse Human Feedback',\n",
       "  'Taxonomy to Regulation: A (Geo)Political Taxonomy for AI Risks and\\n  Regulatory Measures in the EU AI Act',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'Using LLMs to Model the Beliefs and Preferences of Targeted Populations'},\n",
       " 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces': {'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey'},\n",
       " 'The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning': {'Aligning language models with human preferences',\n",
       "  'Learn Your Reference Model for Real Good Alignment',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy\\n  Data',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'SciRepEval: A Multi-Format Benchmark for Scientific Document Representations': {'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking',\n",
       "  'Learning Word Embedding with Better Distance Weighting and Window Size\\n  Scheduling',\n",
       "  'Learning representations of learning representations',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'STRUM-LLM: Attributed and Structured Contrastive Summarization',\n",
       "  'Simple, Efficient and Scalable Structure-aware Adapter Boosts Protein\\n  Language Models',\n",
       "  'Sparse multimodal fusion with modal channel attention'},\n",
       " 'Generalization to New Sequential Decision Making Tasks with In-Context Learning': {'Adversarial Imitation Learning via Boosting',\n",
       "  'Continual Offline Reinforcement Learning via Diffusion-based Dual\\n  Generative Replay',\n",
       "  'Decision Mamba: Reinforcement Learning via Sequence Modeling with Selective State Spaces',\n",
       "  'Deep Policy Optimization with Temporal Logic Constraints',\n",
       "  'Dynamically Anchored Prompting for Task-Imbalanced Continual Learning',\n",
       "  'Higher Replay Ratio Empowers Sample-Efficient Multi-Agent Reinforcement\\n  Learning',\n",
       "  'Inferring Behavior-Specific Context Improves Zero-Shot Generalization in\\n  Reinforcement Learning',\n",
       "  'Meta Reinforcement Learning with Finite Training Tasks -- a Density Estimation Approach',\n",
       "  'Offline Trajectory Generalization for Offline Reinforcement Learning',\n",
       "  'Zero-Shot Stitching in Reinforcement Learning using Relative\\n  Representations'},\n",
       " 'Errors are Useful Prompts: Instruction Guided Task Programming with Verifier-Assisted Iterative Prompting': {'A Self-feedback Knowledge Elicitation Approach for Chemical Reaction\\n  Predictions',\n",
       "  'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Automating REST API Postman Test Cases Using LLM',\n",
       "  'Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V',\n",
       "  'Concept-based Analysis of Neural Networks via Vision-Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'iTBLS: A Dataset of Interactive Conversations Over Tabular Information'},\n",
       " 'AI and Jobs: Has the Inflection Point Arrived? Evidence from an Online Labor Platform': {'AI Competitions and Benchmarks: Dataset Development',\n",
       "  'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions',\n",
       "  'An Economic Solution to Copyright Challenges of Generative AI',\n",
       "  'Beyond Code Generation: An Observational Study of ChatGPT Usage in\\n  Software Engineering Practice',\n",
       "  'Decoding AI: The inside story of data analysis in ChatGPT',\n",
       "  'Enhancing Fairness and Performance in Machine Learning Models: A\\n  Multi-Task Learning Approach with Monte-Carlo Dropout and Pareto Optimality',\n",
       "  'Fair Concurrent Training of Multiple Models in Federated Learning',\n",
       "  'LLM-Enhanced Causal Discovery in Temporal Domain from Interventional\\n  Data',\n",
       "  'Taxonomy to Regulation: A (Geo)Political Taxonomy for AI Risks and\\n  Regulatory Measures in the EU AI Act',\n",
       "  'The Impact of Machine Learning on Society: An Analysis of Current Trends\\n  and Future Implications'},\n",
       " 'Dense X Retrieval: What Retrieval Granularity Should We Use?': {'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Conversational Financial Information Retrieval Model (ConFIRM)',\n",
       "  'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'Small Models Are (Still) Effective Cross-Domain Argument Extractors',\n",
       "  'Towards a Robust Retrieval-Based Summarization System'},\n",
       " 'MotherNet: A Foundational Hypernetwork for Tabular Classification': {'Distilled Datamodel with Reverse Gradient Matching',\n",
       "  'Employing Layerwised Unsupervised Learning to Lessen Data and Loss\\n  Requirements in Forward-Forward Algorithms',\n",
       "  'Engineering software 2.0 by interpolating neural networks: unifying\\n  training, solving, and calibration',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Improving Generalization via Meta-Learning on Hard Samples',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'Noise contrastive estimation with soft targets for conditional models',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science'},\n",
       " 'Weight subcloning: direct initialization of transformers using larger pretrained ones': {'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Efficient Transformer Encoders for Mask2Former-style models',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LayerNorm: A key component in parameter-efficient fine-tuning',\n",
       "  'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Residual-based Language Models are Free Boosters for Biomedical Imaging',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey'},\n",
       " 'A mathematical perspective on Transformers': {'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'Localizing Paragraph Memorization in Language Models',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey',\n",
       "  'Toward a Theory of Tokenization in LLMs',\n",
       "  'Transformers Can Represent $n$-gram Language Models',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'What do Transformers Know about Government?'},\n",
       " 'Perspectives on the State and Future of Deep Learning - 2023': {'AI Competitions and Benchmarks: Dataset Development',\n",
       "  'Closing the Gap in the Trade-off between Fair Representations and\\n  Accuracy',\n",
       "  'DXAI: Explaining Classification by Image Decomposition',\n",
       "  'Does It Make Sense to Explain a Black Box With Another Black Box?',\n",
       "  'Enhancing Counterfactual Explanation Search with Diffusion Distance and\\n  Directional Coherence',\n",
       "  'Explainable Artificial Intelligence Techniques for Accurate Fault\\n  Detection and Diagnosis: A Review',\n",
       "  'Gradient strikes back: How filtering out high frequencies improves explanations',\n",
       "  'Learning representations of learning representations',\n",
       "  'MUGC: Machine Generated versus User Generated Content Detection',\n",
       "  'Toward Understanding the Disagreement Problem in Neural Network Feature\\n  Attribution'},\n",
       " 'Retrieval-Augmented Generation for Large Language Models: A Survey': {'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'Towards a Robust Retrieval-Based Summarization System',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'Time is Encoded in the Weights of Finetuned Language Models': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'Localizing Paragraph Memorization in Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'Stronger Random Baselines for In-Context Learning',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'Recasting Continual Learning as Sequence Modeling': {'Continual Learning on a Diet: Learning from Sparsely Labeled Streams\\n  Under Constrained Computation',\n",
       "  'Decision Mamba: Reinforcement Learning via Sequence Modeling with Selective State Spaces',\n",
       "  'Dynamically Anchored Prompting for Task-Imbalanced Continual Learning',\n",
       "  'FedMeS: Personalized Federated Continual Learning Leveraging Local\\n  Memory',\n",
       "  'Improving Generalization via Meta-Learning on Hard Samples',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective',\n",
       "  'Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer',\n",
       "  'Toward a Theory of Tokenization in LLMs'},\n",
       " \"An In-depth Look at Gemini's Language Abilities\": {'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning': {'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V',\n",
       "  'Deep Policy Optimization with Temporal Logic Constraints',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning'},\n",
       " 'DBOS: A Proposal for a Data-Centric Operating System': {'AntDT: A Self-Adaptive Distributed Training Framework for Leader and\\n  Straggler Nodes',\n",
       "  'Apodotiko: Enabling Efficient Serverless Federated Learning in\\n  Heterogeneous Environments',\n",
       "  'Confidential Federated Computations',\n",
       "  'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models',\n",
       "  'Dynamic Frequency-Based Fingerprinting Attacks against Modern Sandbox\\n  Environments',\n",
       "  'End-to-End Verifiable Decentralized Federated Learning',\n",
       "  'FedFa: A Fully Asynchronous Training Paradigm for Federated Learning',\n",
       "  'I/O in Machine Learning Applications on HPC Systems: A 360-degree Survey',\n",
       "  'Privacy-Preserving Intrusion Detection using Convolutional Neural\\n  Networks',\n",
       "  'Privacy-Preserving Training-as-a-Service for On-Device Intelligence:\\n  Concept, Architectural Scheme, and Open Problems'},\n",
       " 'Mindstorms in Natural Language-Based Societies of Mind': {'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions',\n",
       "  'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Social Choice for AI Alignment: Dealing with Diverse Human Feedback',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing'},\n",
       " 'TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones': {'Bridging Vision and Language Spaces with Assignment Prediction',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Cross-Modal Adapter: Parameter-Efficient Transfer Learning Approach for\\n  Vision-Language Models',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Find The Gap: Knowledge Base Reasoning For Visual Question Answering',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'RankCLIP: Ranking-Consistent Language-Image Pretraining',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'Fast Inference of Mixture-of-Experts Language Models with Offloading': {'Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\\n  Prediction',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LD-Pruner: Efficient Pruning of Latent Diffusion Models using\\n  Task-Agnostic Insights',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'TriForce: Lossless Acceleration of Long Sequence Generation with\\n  Hierarchical Speculative Decoding'},\n",
       " 'Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language Models': {'Bridging Vision and Language Spaces with Assignment Prediction',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Find The Gap: Knowledge Base Reasoning For Visual Question Answering',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Self-Supervised Visual Preference Alignment',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'LLM Augmented LLMs: Expanding Capabilities through Composition': {'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'Mixtral of Experts': {'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving',\n",
       "  'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'Multi-Head Mixture-of-Experts',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'TriForce: Lossless Acceleration of Long Sequence Generation with\\n  Hierarchical Speculative Decoding'},\n",
       " 'ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'LayerNorm: A key component in parameter-efficient fine-tuning',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering'},\n",
       " 'Transformers are Multi-State RNNs': {'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling'},\n",
       " 'GenCast: Diffusion-based ensemble forecasting for medium-range weather': {'Four-hour thunderstorm nowcasting using deep diffusion models of\\n  satellite',\n",
       "  'Fuxi-DA: A Generalized Deep Learning Data Assimilation Framework for\\n  Assimilating Satellite Observations',\n",
       "  'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting',\n",
       "  'Probabilistic forecasting of power system imbalance using neural\\n  network-based ensembles',\n",
       "  'Recurrent Neural Networks for Modelling Gross Primary Production',\n",
       "  'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion',\n",
       "  'Scalable Data Assimilation with Message Passing',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'Toward Routing River Water in Land Surface Models with Recurrent Neural\\n  Networks',\n",
       "  'Uncertainty Aware Tropical Cyclone Wind Speed Estimation from Satellite\\n  Data'},\n",
       " 'The Unreasonable Effectiveness of Easy Training Data for Hard Tasks': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Latxa: An Open Language Model and Evaluation Suite for Basque',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'Stronger Random Baselines for In-Context Learning',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy': {'A multiobjective continuation method to compute the regularization path of deep neural networks',\n",
       "  'DeNetDM: Debiasing by Network Depth Modulation',\n",
       "  'Delayed Bottlenecking: Alleviating Forgetting in Pre-trained Graph\\n  Neural Networks',\n",
       "  'Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World',\n",
       "  'Graph Neural Aggregation-diffusion with Metastability',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'Multi-Head Mixture-of-Experts',\n",
       "  'Neuro-Inspired Information-Theoretic Hierarchical Perception for\\n  Multimodal Learning',\n",
       "  'Separate, Dynamic and Differentiable (SMART) Pruner for Block/Output Channel Pruning on Computer Vision Tasks'},\n",
       " 'Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering': {'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Automating REST API Postman Test Cases Using LLM',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'LLM-Powered Test Case Generation for Detecting Tricky Bugs',\n",
       "  'Language Model Prompt Selection via Simulation Optimization',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\\n  Upcycled Mixture-of-Experts'},\n",
       " 'Foundations of Vector Retrieval': {'Bridging Vision and Language Spaces with Assignment Prediction',\n",
       "  'GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers',\n",
       "  'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'VDTuner: Automated Performance Tuning for Vector Data Management Systems',\n",
       "  'iRAG: An Incremental Retrieval Augmented Generation System for Videos'},\n",
       " 'An Emulator for Fine-Tuning Large Language Models using Small Language Models': {'Aligning language models with human preferences',\n",
       "  'Filtered Direct Preference Optimization',\n",
       "  'Learn Your Reference Model for Real Good Alignment',\n",
       "  'Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy\\n  Data',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'Tuning Language Models by Proxy': {'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LayerNorm: A key component in parameter-efficient fine-tuning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy\\n  Data',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model',\n",
       "  'XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\\n  Upcycled Mixture-of-Experts'},\n",
       " 'Context is Environment': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Stronger Random Baselines for In-Context Learning',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'Self-Rewarding Language Models': {'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Filtered Direct Preference Optimization',\n",
       "  'From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function',\n",
       "  'Generalized Population-Based Training for Hyperparameter Optimization in\\n  Reinforcement Learning',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Self-playing Adversarial Language Game Enhances LLM Reasoning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning': {'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving',\n",
       "  'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models'},\n",
       " 'InRanker: Distilled Rankers for Zero-shot Information Retrieval': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'Estimating the Hessian Matrix of Ranking Objectives for Stochastic\\n  Learning to Rank with Gradient Boosted Trees',\n",
       "  'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering'},\n",
       " 'Explaining Anomalies using Denoising Autoencoders for Financial Tabular Data': {'A Customer Level Fraudulent Activity Detection Benchmark for Enhancing\\n  Machine Learning Model Research and Evaluation',\n",
       "  'An improved tabular data generator with VAE-GMM integration',\n",
       "  'Anomaly Correction of Business Processes Using Transformer Autoencoder',\n",
       "  'BayesJudge: Bayesian Kernel Language Modelling with Confidence\\n  Uncertainty in Legal Judgment Prediction',\n",
       "  'DACAD: Domain Adaptation Contrastive Learning for Anomaly Detection in\\n  Multivariate Time Series',\n",
       "  'Explainable Artificial Intelligence Techniques for Accurate Fault\\n  Detection and Diagnosis: A Review',\n",
       "  'Fin-Fed-OD: Federated Outlier Detection on Financial Tabular Data',\n",
       "  'Gradient strikes back: How filtering out high frequencies improves explanations',\n",
       "  'Sparse Explanations of Neural Networks Using Pruned Layer-Wise Relevance\\n  Propagation',\n",
       "  'Toward Understanding the Disagreement Problem in Neural Network Feature\\n  Attribution'},\n",
       " 'Learning to Reweight Examples for Robust Deep Learning': {'A Survey of Deep Long-Tail Classification Advancements',\n",
       "  'DeNetDM: Debiasing by Network Depth Modulation',\n",
       "  'Distilled Datamodel with Reverse Gradient Matching',\n",
       "  'Gradient-Regularized Out-of-Distribution Detection',\n",
       "  'Improving Generalization via Meta-Learning on Hard Samples',\n",
       "  'NC-TTT: A Noise Contrastive Approach for Test-Time Training',\n",
       "  'OTTER: Improving Zero-Shot Classification via Optimal Transport',\n",
       "  'Positive Unlabeled Contrastive Learning',\n",
       "  'Reactive Model Correction: Mitigating Harm to Task-Relevant Features via\\n  Conditional Bias Suppression',\n",
       "  'SOPHON: Non-Fine-Tunable Learning to Restrain Task Transferability For\\n  Pre-trained Models'},\n",
       " 'In-Context Learning for Extreme Multi-Label Classification': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy\\n  Data',\n",
       "  'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages',\n",
       "  'Stronger Random Baselines for In-Context Learning',\n",
       "  'When LLMs are Unfit Use FastFit: Fast and Effective Text Classification\\n  with Many Classes'},\n",
       " 'Introducing Neural Bag of Whole-Words with ColBERTer: Contextualized Late Interactions using Enhanced Reduction': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'Conversational Financial Information Retrieval Model (ConFIRM)',\n",
       "  'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'VDTuner: Automated Performance Tuning for Vector Data Management Systems'},\n",
       " 'Progress measures for grokking via mechanistic interpretability': {'DXAI: Explaining Classification by Image Decomposition',\n",
       "  'Deep Learning as Ricci Flow',\n",
       "  'Deep Neural Networks via Complex Network Theory: a Perspective',\n",
       "  'Engineering software 2.0 by interpolating neural networks: unifying\\n  training, solving, and calibration',\n",
       "  'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'Listen to the Waves: Using a Neuronal Model of the Human Auditory System\\n  to Predict Ocean Waves',\n",
       "  'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective',\n",
       "  'Training neural networks with structured noise improves classification and generalization'},\n",
       " 'ChatQA: Building GPT-4 Level Conversational QA Models': {'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Towards a Robust Retrieval-Based Summarization System'},\n",
       " 'Qlib: An AI-oriented Quantitative Investment Platform': {'AI Competitions and Benchmarks: Dataset Development',\n",
       "  'Decision Mamba: Reinforcement Learning via Sequence Modeling with Selective State Spaces',\n",
       "  'Digital Twins for forecasting and decision optimisation with machine\\n  learning: applications in wastewater treatment',\n",
       "  'OptiGrad: A Fair and more Efficient Price Elasticity Optimization via a\\n  Gradient Based Learning',\n",
       "  'Reinforcement Learning in Agent-Based Market Simulation: Unveiling Realistic Stylized Facts and Behavior',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'The New Agronomists: Language Models are Experts in Crop Management',\n",
       "  'Towards Stable Machine Learning Model Retraining via Slowly Varying Sequences',\n",
       "  'Using ARIMA to Predict the Expansion of Subscriber Data Consumption',\n",
       "  'VC Theory for Inventory Policies'},\n",
       " 'Matryoshka Representation Learning': {'Bridging Vision and Language Spaces with Assignment Prediction',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'RankCLIP: Ranking-Consistent Language-Image Pretraining',\n",
       "  'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'},\n",
       " 'Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding': {'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Beyond Code Generation: An Observational Study of ChatGPT Usage in\\n  Software Engineering Practice',\n",
       "  'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'iTBLS: A Dataset of Interactive Conversations Over Tabular Information'},\n",
       " 'MambaTab: A Simple Yet Effective Approach for Handling Tabular Data': {'An improved tabular data generator with VAE-GMM integration',\n",
       "  'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey',\n",
       "  'Structured Evaluation of Synthetic Tabular Data',\n",
       "  'Synthesizing Realistic Data for Table Recognition',\n",
       "  'Towards Universal Performance Modeling for Machine Learning Training on\\n  Multi-GPU Platforms',\n",
       "  'Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science'},\n",
       " 'ReGAL: Refactoring Programs to Discover Generalizable Abstractions': {'Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\\n  Prediction',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LLM-Powered Test Case Generation for Detecting Tricky Bugs',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\\n  Upcycled Mixture-of-Experts'},\n",
       " 'Learning Transformer Programs': {'A Learning Paradigm for Interpretable Gradients',\n",
       "  'AlloyBERT: Alloy Property Prediction with Large Language Models',\n",
       "  'DXAI: Explaining Classification by Image Decomposition',\n",
       "  'Decomposing and Editing Predictions by Modeling Model Computation',\n",
       "  'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'Gradient strikes back: How filtering out high frequencies improves explanations',\n",
       "  'LayerNorm: A key component in parameter-efficient fine-tuning',\n",
       "  'Sparse Explanations of Neural Networks Using Pruned Layer-Wise Relevance\\n  Propagation'},\n",
       " 'RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'Towards a Robust Retrieval-Based Summarization System'},\n",
       " 'Learning Universal Predictors': {'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'Improving Generalization via Meta-Learning on Hard Samples',\n",
       "  'MPC of Uncertain Nonlinear Systems with Meta-Learning for Fast\\n  Adaptation of Neural Predictive Models',\n",
       "  'Meta Reinforcement Learning with Finite Training Tasks -- a Density Estimation Approach',\n",
       "  'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective',\n",
       "  'Training neural networks with structured noise improves classification and generalization'},\n",
       " 'DoubleEnsemble: A New Ensemble Method Based on Sample Reweighting and Feature Selection for Financial Data Analysis': {'A Guide to Feature Importance Methods for Scientific Inference',\n",
       "  'A Survey of Deep Long-Tail Classification Advancements',\n",
       "  'A novel decision fusion approach for sale price prediction using Elastic Net and MOPSO',\n",
       "  'Application of the representative measure approach to assess the\\n  reliability of decision trees in dealing with unseen vehicle collision data',\n",
       "  'Explainable LightGBM Approach for Predicting Myocardial Infarction\\n  Mortality',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'TSLANet: Rethinking Transformers for Time Series Representation Learning',\n",
       "  'Towards Stable Machine Learning Model Retraining via Slowly Varying Sequences'},\n",
       " 'Time-LLM: Time Series Forecasting by Reprogramming Large Language Models': {'DeepHeteroIoT: Deep Local and Global Learning over Heterogeneous IoT Sensor Data',\n",
       "  'Generating Synthetic Time Series Data for Cyber-Physical Systems',\n",
       "  'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'TSLANet: Rethinking Transformers for Time Series Representation Learning',\n",
       "  'Variational quantization for state space models',\n",
       "  'σ-GPTs: A New Approach to Autoregressive Models'},\n",
       " 'DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines': {'Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\\n  Prediction',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'LLM-Powered Test Case Generation for Detecting Tricky Bugs',\n",
       "  'Language Model Prompt Selection via Simulation Optimization',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path\\n  Forward',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards a Robust Retrieval-Based Summarization System'},\n",
       " 'A decoder-only foundation model for time-series forecasting': {'Advancing Long-Term Multi-Energy Load Forecasting with Patchformer: A\\n  Patch and Transformer-Based Approach',\n",
       "  'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting',\n",
       "  'LayerNorm: A key component in parameter-efficient fine-tuning',\n",
       "  'Learning representations of learning representations',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods'},\n",
       " 'A Time Series is Worth 64 Words: Long-term Forecasting with Transformers': {'Advancing Long-Term Multi-Energy Load Forecasting with Patchformer: A\\n  Patch and Transformer-Based Approach',\n",
       "  'DeepHeteroIoT: Deep Local and Global Learning over Heterogeneous IoT Sensor Data',\n",
       "  'Generating Synthetic Time Series Data for Cyber-Physical Systems',\n",
       "  'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'TSLANet: Rethinking Transformers for Time Series Representation Learning',\n",
       "  'Variational quantization for state space models'},\n",
       " 'TransTab: Learning Transferable Tabular Transformers Across Tables': {'An improved tabular data generator with VAE-GMM integration',\n",
       "  'Guided Discrete Diffusion for Electronic Health Record Generation',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'Machine Learning Techniques for MRI Data Processing at Expanding Scale',\n",
       "  'Segmentation, Classification and Interpretation of Breast Cancer Medical Images using Human-in-the-Loop Machine Learning',\n",
       "  'Structured Evaluation of Synthetic Tabular Data',\n",
       "  'Supervised Contrastive Vision Transformer for Breast Histopathological\\n  Image Classification',\n",
       "  'Synthesizing Realistic Data for Table Recognition',\n",
       "  'Time-aware Heterogeneous Graph Transformer with Adaptive Attention\\n  Merging for Health Event Prediction',\n",
       "  'Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science'},\n",
       " 'Spectral State Space Models': {'Computationally and Memory-Efficient Robust Predictive Analytics Using Big Data',\n",
       "  'Feature-Based Echo-State Networks: A Step Towards Interpretability and Minimalism in Reservoir Computer',\n",
       "  'Generating Synthetic Time Series Data for Cyber-Physical Systems',\n",
       "  'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey',\n",
       "  'TSLANet: Rethinking Transformers for Time Series Representation Learning',\n",
       "  'Variational quantization for state space models',\n",
       "  'σ-GPTs: A New Approach to Autoregressive Models'},\n",
       " 'Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks': {'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey'},\n",
       " 'Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models': {'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Towards Logically Consistent Language Models via Probabilistic Reasoning',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'In-Context Principle Learning from Mistakes': {'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Find The Gap: Knowledge Base Reasoning For Visual Question Answering',\n",
       "  'Latxa: An Open Language Model and Evaluation Suite for Basque',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'QAGCN: Answering Multi-Relation Questions via Single-Step Implicit Reasoning over Knowledge Graphs',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing'},\n",
       " 'Neural Network Diffusion': {'An improved tabular data generator with VAE-GMM integration',\n",
       "  'Generative Modelling with High-Order Langevin Dynamics',\n",
       "  'LD-Pruner: Efficient Pruning of Latent Diffusion Models using\\n  Task-Agnostic Insights',\n",
       "  'Neural Flow Diffusion Models: Learnable Forward Process for Improved\\n  Diffusion Modelling',\n",
       "  'Physics-integrated generative modeling using attentive planar\\n  normalizing flow based variational autoencoder',\n",
       "  'SparseDM: Toward Sparse Efficient Diffusion Models'},\n",
       " 'RAGIC: Risk-Aware Generative Adversarial Model for Stock Interval Construction': {'An improved tabular data generator with VAE-GMM integration',\n",
       "  'Conformal Prediction for Stochastic Decision-Making of PV Power in Electricity Markets',\n",
       "  'Guided Discrete Diffusion for Electronic Health Record Generation',\n",
       "  'Reinforcement Learning in Agent-Based Market Simulation: Unveiling Realistic Stylized Facts and Behavior',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'TSLANet: Rethinking Transformers for Time Series Representation Learning',\n",
       "  'Variational quantization for state space models',\n",
       "  'σ-GPTs: A New Approach to Autoregressive Models'},\n",
       " 'OmniPred: Language Models as Universal Regressors': {'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'FedEval-LLM: Federated Evaluation of Large Language Models on Downstream\\n  Tasks with Collective Wisdom',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Learning representations of learning representations',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages',\n",
       "  'Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science',\n",
       "  'Using LLMs to Model the Beliefs and Preferences of Targeted Populations'},\n",
       " 'A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'A Comparative Study of AI-Generated (GPT-4) and Human-crafted MCQs in Programming Education': {'Beyond Code Generation: An Observational Study of ChatGPT Usage in\\n  Software Engineering Practice',\n",
       "  'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'LLM-Powered Test Case Generation for Detecting Tricky Bugs',\n",
       "  'Language Model Prompt Selection via Simulation Optimization',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Towards a Robust Retrieval-Based Summarization System'},\n",
       " 'Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models': {'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey'},\n",
       " 'Latent Attention for Linear Time Transformers': {'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving',\n",
       "  'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey',\n",
       "  'Toward a Theory of Tokenization in LLMs',\n",
       "  'TriForce: Lossless Acceleration of Long Sequence Generation with\\n  Hierarchical Speculative Decoding'},\n",
       " 'Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution': {'EdgeFusion: On-Device Text-to-Image Generation',\n",
       "  'LD-Pruner: Efficient Pruning of Latent Diffusion Models using\\n  Task-Agnostic Insights',\n",
       "  'Neural Flow Diffusion Models: Learnable Forward Process for Improved\\n  Diffusion Modelling',\n",
       "  'Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging\\n  Perturbations That Efficiently Fool Customized Diffusion Models',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Toward a Theory of Tokenization in LLMs'},\n",
       " 'Repetition Improves Language Model Embeddings': {'CroissantLLM: A Truly Bilingual French-English Language Model',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling'},\n",
       " 'Fine-tuning with Very Large Dropout': {'A Survey of Deep Long-Tail Classification Advancements',\n",
       "  'DeNetDM: Debiasing by Network Depth Modulation',\n",
       "  'Distilled Datamodel with Reverse Gradient Matching',\n",
       "  'Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World',\n",
       "  'Gradient-Regularized Out-of-Distribution Detection',\n",
       "  'LoRA Dropout as a Sparsity Regularizer for Overfitting Control',\n",
       "  'Noise contrastive estimation with soft targets for conditional models',\n",
       "  'Toward a Realistic Benchmark for Out-of-Distribution Detection'},\n",
       " 'Is Cosine-Similarity of Embeddings Really About Similarity?': {'A multiobjective continuation method to compute the regularization path of deep neural networks',\n",
       "  'Closing the Gap in the Trade-off between Fair Representations and\\n  Accuracy',\n",
       "  'Deep Regression Representation Learning with Topology',\n",
       "  'How Deep Networks Learn Sparse and Hierarchical Data: the Sparse Random\\n  Hierarchy Model',\n",
       "  'Hyperbolic Delaunay Geometric Alignment',\n",
       "  'LipSim: A Provably Robust Perceptual Similarity Metric',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'Review-Based Cross-Domain Recommendation via Hyperbolic Embedding and Hierarchy-Aware Domain Disentanglement'},\n",
       " 'Dual Operating Modes of In-Context Learning': {'Dynamically Anchored Prompting for Task-Imbalanced Continual Learning',\n",
       "  'Localizing Paragraph Memorization in Language Models',\n",
       "  'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Meta Reinforcement Learning with Finite Training Tasks -- a Density Estimation Approach',\n",
       "  'Meta-Learning with Generalized Ridge Regression: High-dimensional Asymptotics, Optimality and Hyper-covariance Estimation',\n",
       "  'NC-TTT: A Noise Contrastive Approach for Test-Time Training',\n",
       "  'OTTER: Improving Zero-Shot Classification via Optimal Transport',\n",
       "  'Stronger Random Baselines for In-Context Learning',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models'},\n",
       " 'Chronos: Learning the Language of Time Series': {'Advancing Long-Term Multi-Energy Load Forecasting with Patchformer: A\\n  Patch and Transformer-Based Approach',\n",
       "  'DeepHeteroIoT: Deep Local and Global Learning over Heterogeneous IoT Sensor Data',\n",
       "  'Generating Synthetic Time Series Data for Cyber-Physical Systems',\n",
       "  'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'TSLANet: Rethinking Transformers for Time Series Representation Learning',\n",
       "  'Toward a Theory of Tokenization in LLMs',\n",
       "  'Variational quantization for state space models'},\n",
       " 'How Far Are We from Intelligent Visual Deductive Reasoning?': {'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Bridging Vision and Language Spaces with Assignment Prediction',\n",
       "  'Compositional Chain-of-Thought Prompting for Large Multimodal Models',\n",
       "  'Concept-based Analysis of Neural Networks via Vision-Language Models',\n",
       "  'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models',\n",
       "  'Find The Gap: Knowledge Base Reasoning For Visual Question Answering',\n",
       "  'Pre-trained Vision-Language Models Learn Discoverable Visual Concepts',\n",
       "  'Self-Supervised Visual Preference Alignment',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning',\n",
       "  'Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models'},\n",
       " 'Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset': {'Analyzing the Roles of Language and Vision in Learning from Limited Data',\n",
       "  'Beyond Code Generation: An Observational Study of ChatGPT Usage in\\n  Software Engineering Practice',\n",
       "  'Concept-based Analysis of Neural Networks via Vision-Language Models',\n",
       "  'GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers',\n",
       "  'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'Pre-trained Vision-Language Models Learn Discoverable Visual Concepts',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Self-Supervised Visual Preference Alignment',\n",
       "  'TextSquare: Scaling up Text-Centric Visual Instruction Tuning'},\n",
       " 'Data Interpreter: An LLM Agent For Data Science': {'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Automating REST API Postman Test Cases Using LLM',\n",
       "  'CT-Agent: Clinical Trial Multi-Agent with Large Language Model-based\\n  Reasoning',\n",
       "  'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models',\n",
       "  'Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\\n  Prediction',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing'},\n",
       " 'Mechanics of Next Token Prediction with Self-Attention': {'DiJiang: Efficient Large Language Models through Compact Kernelization',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models',\n",
       "  'Localizing Paragraph Memorization in Language Models',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'SpaceByte: Towards Deleting Tokenization from Large Language Modeling',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey',\n",
       "  'Toward a Theory of Tokenization in LLMs',\n",
       "  'Transformers Can Represent $n$-gram Language Models',\n",
       "  'What do Transformers Know about Government?'},\n",
       " 'Transformers for Supervised Online Continual Learning': {'A Unified Replay-based Continuous Learning Framework for Spatio-Temporal\\n  Prediction on Streaming Data',\n",
       "  'Continual Learning on a Diet: Learning from Sparsely Labeled Streams\\n  Under Constrained Computation',\n",
       "  'Continual Offline Reinforcement Learning via Diffusion-based Dual\\n  Generative Replay',\n",
       "  'Dynamically Anchored Prompting for Task-Imbalanced Continual Learning',\n",
       "  'FedMeS: Personalized Federated Continual Learning Leveraging Local\\n  Memory',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'Offline Trajectory Generalization for Offline Reinforcement Learning',\n",
       "  'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective',\n",
       "  'Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer'},\n",
       " 'Transformer-Based Models Are Not Yet Perfect At Learning to Emulate Structural Recursion': {'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'On the Empirical Complexity of Reasoning and Planning in LLMs',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'Symbolic Integration Algorithm Selection with Machine Learning: LSTMs vs\\n  Tree LSTMs',\n",
       "  'Transformers Can Represent $n$-gram Language Models',\n",
       "  'What do Transformers Know about Government?'},\n",
       " 'TnT-LLM: Text Mining at Scale with Large Language Models': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'CT-Agent: Clinical Trial Multi-Agent with Large Language Model-based\\n  Reasoning',\n",
       "  'Conversational Financial Information Retrieval Model (ConFIRM)',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction'},\n",
       " 'FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'Conversational Financial Information Retrieval Model (ConFIRM)',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering',\n",
       "  'Towards a Robust Retrieval-Based Summarization System'},\n",
       " 'Gecko: Versatile Text Embeddings Distilled from Large Language Models': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models',\n",
       "  'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Learning Word Embedding with Better Distance Weighting and Window Size\\n  Scheduling',\n",
       "  'LongEmbed: Extending Embedding Models for Long Context Retrieval',\n",
       "  'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages',\n",
       "  'Retrieval Augmented Generation for Domain-specific Question Answering'},\n",
       " 'Stream of Search (SoS): Learning to Search in Language': {'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function',\n",
       "  'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Self-playing Adversarial Language Game Enhances LLM Reasoning',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'},\n",
       " 'TimeGPT-1': {'A Comparison of Traditional and Deep Learning Methods for Parameter\\n  Estimation of the Ornstein-Uhlenbeck Process',\n",
       "  'Computationally and Memory-Efficient Robust Predictive Analytics Using Big Data',\n",
       "  'DeepHeteroIoT: Deep Local and Global Learning over Heterogeneous IoT Sensor Data',\n",
       "  'FMint: Bridging Human Designed and Data Pretrained Models for\\n  Differential Equation Foundation Model',\n",
       "  'Generating Synthetic Time Series Data for Cyber-Physical Systems',\n",
       "  'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'TSLANet: Rethinking Transformers for Time Series Representation Learning',\n",
       "  'Variational quantization for state space models'},\n",
       " 'Addressing the Abstraction and Reasoning Corpus via Procedural Example Generation': {'ASID: Active Exploration for System Identification in Robotic\\n  Manipulation',\n",
       "  'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory',\n",
       "  'GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers',\n",
       "  'NC-TTT: A Noise Contrastive Approach for Test-Time Training',\n",
       "  'PyTorchGeoNodes: Enabling Differentiable Shape Programs for 3D Shape\\n  Reconstruction',\n",
       "  'Zero-Shot Stitching in Reinforcement Learning using Relative\\n  Representations'},\n",
       " 'From Words to Numbers: Your Large Language Model Is Secretly A Capable Regressor When Given In-Context Examples': {'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees',\n",
       "  'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning',\n",
       "  'Many-Shot In-Context Learning',\n",
       "  'Nearly Optimal Algorithms for Contextual Dueling Bandits from\\n  Adversarial Feedback',\n",
       "  'Pre-training Small Base LMs with Fewer Tokens',\n",
       "  'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'Stronger Random Baselines for In-Context Learning',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science'},\n",
       " 'The Rise of Diffusion Models in Time-Series Forecasting': {'ICST-DNET: An Interpretable Causal Spatio-Temporal Diffusion Network for\\n  Traffic Speed Prediction',\n",
       "  'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting',\n",
       "  'Integrating Marketing Channels into Quantile Transformation and Bayesian\\n  Optimization of Ensemble Kernels for Sales Prediction with Gaussian Process\\n  Models',\n",
       "  'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection',\n",
       "  'Neural Flow Diffusion Models: Learnable Forward Process for Improved\\n  Diffusion Modelling',\n",
       "  'Neural McKean-Vlasov Processes: Distributional Dependence in Diffusion\\n  Processes',\n",
       "  'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion',\n",
       "  'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods',\n",
       "  'Using ARIMA to Predict the Expansion of Subscriber Data Consumption',\n",
       "  'Variational quantization for state space models'},\n",
       " 'The Illusion of State in State-Space Models': {'Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\\n  Prediction',\n",
       "  'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs',\n",
       "  'Jamba: A Hybrid Transformer-Mamba Language Model',\n",
       "  'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory',\n",
       "  'Rethinking LLM Memorization through the Lens of Adversarial Compression',\n",
       "  'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey',\n",
       "  'Toward a Theory of Tokenization in LLMs',\n",
       "  'Transformers Can Represent $n$-gram Language Models',\n",
       "  'Uncertainty Quantification for In-Context Learning of Large Language Models',\n",
       "  'What do Transformers Know about Government?'},\n",
       " 'AgentKit: Flow Engineering with Graphs, not Coding': {'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions',\n",
       "  'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Beyond Code Generation: An Observational Study of ChatGPT Usage in\\n  Software Engineering Practice',\n",
       "  'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation',\n",
       "  'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing',\n",
       "  'iTBLS: A Dataset of Interactive Conversations Over Tabular Information'},\n",
       " 'GoEX: Perspectives and Designs Towards a Runtime for Autonomous LLM Applications': {'AgentKit: Flow Engineering with Graphs, not Coding',\n",
       "  'Aligning LLM Agents by Learning Latent Preference from User Edits',\n",
       "  'Automating REST API Postman Test Cases Using LLM',\n",
       "  'Beyond Code Generation: An Observational Study of ChatGPT Usage in\\n  Software Engineering Practice',\n",
       "  'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models',\n",
       "  'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning',\n",
       "  'NExT: Teaching Large Language Models to Reason about Code Execution',\n",
       "  'Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path\\n  Forward',\n",
       "  'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs',\n",
       "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation'}}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Papers similar to papers_of_interest recommended via euclidean distance are :\")\n",
    "recom_euc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Recommendations as a combined result of the above rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the paper A Generalist Neural Algorithmic Learner  we have the following recommendation:  {'Improving the interpretability of GNN predictions through\\n  conformal-based graph sparsification', 'Graph Continual Learning with Debiased Lossless Memory Replay', 'Beyond the Known: Novel Class Discovery for Open-world Graph Learning', 'GNNavigator: Towards Adaptive Training of Graph Neural Networks via\\n  Automatic Guideline Exploration', 'Graph Machine Learning in the Era of Large Language Models (LLMs)', 'Delayed Bottlenecking: Alleviating Forgetting in Pre-trained Graph\\n  Neural Networks', 'FR-NAS: Forward-and-Reverse Graph Predictor for Efficient Neural\\n  Architecture Search', 'On the Scalability of GNNs for Molecular Graphs'}\n",
      "For the paper Meta-Learning Fast Weight Language Models  we have the following recommendation:  {'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models', 'SpaceByte: Towards Deleting Tokenization from Large Language Modeling', 'LayerNorm: A key component in parameter-efficient fine-tuning', 'CroissantLLM: A Truly Bilingual French-English Language Model', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models', 'DiJiang: Efficient Large Language Models through Compact Kernelization', 'Pre-training Small Base LMs with Fewer Tokens'}\n",
      "For the paper Testing GLOM's ability to infer wholes from ambiguous parts  we have the following recommendation:  {'Concept-based Analysis of Neural Networks via Vision-Language Models', 'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'Residual-based Language Models are Free Boosters for Biomedical Imaging', 'GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers', 'Compositional Chain-of-Thought Prompting for Large Multimodal Models', 'Sparse multimodal fusion with modal channel attention', 'Neuro-Inspired Information-Theoretic Hierarchical Perception for\\n  Multimodal Learning', 'Bridging Vision and Language Spaces with Assignment Prediction', 'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models'}\n",
      "For the paper Seven Sketches in Compositionality: An Invitation to Applied Category Theory  we have the following recommendation:  {'Generalized Gradient Descent is a Hypergraph Functor', 'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions', 'Accounting for AI and Users Shaping One Another: The Role of\\n  Mathematical Models', 'On permutation-invariant neural networks', 'Complexity of Probabilistic Reasoning for Neurosymbolic Classification\\n  Techniques', 'Semantic Cells: Evolutional Process to Acquire Sense Diversity of Items', 'What do Transformers Know about Government?', 'Transformers Can Represent $n$-gram Language Models'}\n",
      "For the paper Evolution through Large Models  we have the following recommendation:  {'Analyzing the Roles of Language and Vision in Learning from Limited Data', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'OpenBezoar: Small, Cost-Effective and Open Models Trained on Mixes of\\n  Instruction Data', 'Next Generation Loss Function for Image Classification', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'NExT: Teaching Large Language Models to Reason about Code Execution', 'XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\\n  Upcycled Mixture-of-Experts'}\n",
      "For the paper Teaching Algorithmic Reasoning via In-context Learning  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction', 'On the Empirical Complexity of Reasoning and Planning in LLMs', 'Rethinking LLM Memorization through the Lens of Adversarial Compression', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper Emergent Analogical Reasoning in Large Language Models  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Analyzing the Roles of Language and Vision in Learning from Limited Data', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Compositional Chain-of-Thought Prompting for Large Multimodal Models', 'TextSquare: Scaling up Text-Centric Visual Instruction Tuning', 'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models', 'On the Empirical Complexity of Reasoning and Planning in LLMs', 'Transformers Can Represent $n$-gram Language Models', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity'}\n",
      "For the paper Autoformalization with Large Language Models  we have the following recommendation:  {'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium', 'Concept-based Analysis of Neural Networks via Vision-Language Models', 'Learning representations of learning representations', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Symbolic Integration Algorithm Selection with Machine Learning: LSTMs vs\\n  Tree LSTMs', 'Automating REST API Postman Test Cases Using LLM', 'CroissantLLM: A Truly Bilingual French-English Language Model', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'PARAMANU-GANITA: Language Model with Mathematical Capabilities', 'Rethinking LLM Memorization through the Lens of Adversarial Compression'}\n",
      "For the paper In-context Reinforcement Learning with Algorithm Distillation  we have the following recommendation:  {'Continual Offline Reinforcement Learning via Diffusion-based Dual\\n  Generative Replay', 'Deep Policy Optimization with Temporal Logic Constraints', 'Decision Mamba: Reinforcement Learning via Sequence Modeling with Selective State Spaces', 'Zero-Shot Stitching in Reinforcement Learning using Relative\\n  Representations', 'Goal Exploration via Adaptive Skill Distribution for Goal-Conditioned\\n  Reinforcement Learning', 'DPO: Differential reinforcement learning with application to optimal\\n  configuration search', 'Adversarial Imitation Learning via Boosting', 'Adaptive Regularization of Representation Rank as an Implicit Constraint\\n  of Bellman Equation', 'Inferring Behavior-Specific Context Improves Zero-Shot Generalization in\\n  Reinforcement Learning', 'Offline Trajectory Generalization for Offline Reinforcement Learning'}\n",
      "For the paper What learning algorithm is in-context learning? Investigations with linear models  we have the following recommendation:  {'Toward a Theory of Tokenization in LLMs', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Decision Mamba: Reinforcement Learning via Sequence Modeling with Selective State Spaces', 'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory', 'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection', 'Decomposing and Editing Predictions by Modeling Model Computation', 'DiJiang: Efficient Large Language Models through Compact Kernelization', 'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory'}\n",
      "For the paper Transformers learn in-context by gradient descent  we have the following recommendation:  {'NC-TTT: A Noise Contrastive Approach for Test-Time Training', 'Employing Layerwised Unsupervised Learning to Lessen Data and Loss\\n  Requirements in Forward-Forward Algorithms', 'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective', 'Enhancing Efficiency in Vision Transformer Networks: Design Techniques and Insights', 'Can We Break Free from Strong Data Augmentations in Self-Supervised\\n  Learning?', 'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning', 'DiJiang: Efficient Large Language Models through Compact Kernelization', 'Decomposing and Editing Predictions by Modeling Model Computation', 'Reactive Model Correction: Mitigating Harm to Task-Relevant Features via\\n  Conditional Bias Suppression', 'Improving Generalization via Meta-Learning on Hard Samples'}\n",
      "For the paper Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'LayerNorm: A key component in parameter-efficient fine-tuning', 'CroissantLLM: A Truly Bilingual French-English Language Model', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models', 'DiJiang: Efficient Large Language Models through Compact Kernelization', 'Pre-training Small Base LMs with Fewer Tokens', 'Localizing Paragraph Memorization in Language Models'}\n",
      "For the paper DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models  we have the following recommendation:  {'Guided Discrete Diffusion for Electronic Health Record Generation', 'LD-Pruner: Efficient Pruning of Latent Diffusion Models using\\n  Task-Agnostic Insights', 'EdgeFusion: On-Device Text-to-Image Generation', 'Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level\\n  Knowledge Distillation', 'FlashSpeech: Efficient Zero-Shot Speech Synthesis'}\n",
      "For the paper Tracr: Compiled Transformers as a Laboratory for Interpretability  we have the following recommendation:  {'Toward a Theory of Tokenization in LLMs', 'Concept-based Analysis of Neural Networks via Vision-Language Models', 'SpaceByte: Towards Deleting Tokenization from Large Language Modeling', 'LayerNorm: A key component in parameter-efficient fine-tuning', 'What do Transformers Know about Government?', 'Decomposing and Editing Predictions by Modeling Model Computation', 'Transformers Can Represent $n$-gram Language Models', 'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory'}\n",
      "For the paper Learning-Rate-Free Learning by D-Adaptation  we have the following recommendation:  {'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium', 'Data-Driven Performance Guarantees for Classical and Learned Optimizers', 'Generalized Population-Based Training for Hyperparameter Optimization in\\n  Reinforcement Learning', 'Federated Optimization with Doubly Regularized Drift Correction', 'DFWLayer: Differentiable Frank-Wolfe Optimization Layer', 'Personalized Collaborative Fine-Tuning for On-Device Large Language\\n  Models', 'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning', 'Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models', 'Regularized Gauss-Newton for Optimizing Overparameterized Neural\\n  Networks', 'ColA: Collaborative Adaptation with Gradient Learning'}\n",
      "For the paper A Watermark for Large Language Models  we have the following recommendation:  {'Toward a Theory of Tokenization in LLMs', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Talk Too Much: Poisoning Large Language Models under Token Limit', 'Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path\\n  Forward', 'SpaceByte: Towards Deleting Tokenization from Large Language Modeling', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'Negative Label Guided OOD Detection with Pretrained Vision-Language Models', 'Rethinking LLM Memorization through the Lens of Adversarial Compression', 'Pre-training Small Base LMs with Fewer Tokens', 'Private Attribute Inference from Images with Vision-Language Models'}\n",
      "For the paper DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature  we have the following recommendation:  {'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Talk Too Much: Poisoning Large Language Models under Token Limit', 'Does It Make Sense to Explain a Black Box With Another Black Box?', 'MUGC: Machine Generated versus User Generated Content Detection', 'Robust CLIP-Based Detector for Exposing Diffusion Model-Generated Images', 'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction', 'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages', 'Rethinking LLM Memorization through the Lens of Adversarial Compression', 'Negative Label Guided OOD Detection with Pretrained Vision-Language Models', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper Grounding Language Models to Images for Multimodal Inputs and Outputs  we have the following recommendation:  {'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality', 'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models', 'RankCLIP: Ranking-Consistent Language-Image Pretraining', 'Compositional Chain-of-Thought Prompting for Large Multimodal Models', 'TextSquare: Scaling up Text-Centric Visual Instruction Tuning', 'Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept\\n  Understanding', 'Cross-Modal Adapter: Parameter-Efficient Transfer Learning Approach for\\n  Vision-Language Models', 'Bridging Vision and Language Spaces with Assignment Prediction', 'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models', 'Find The Gap: Knowledge Base Reasoning For Visual Question Answering'}\n",
      "For the paper Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP  we have the following recommendation:  {'Many-Shot In-Context Learning', 'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality', 'Stronger Random Baselines for In-Context Learning', 'Retrieval Augmented Generation for Domain-specific Question Answering', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees', 'TextSquare: Scaling up Text-Centric Visual Instruction Tuning', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing'}\n",
      "For the paper ReAct: Synergizing Reasoning and Acting in Language Models  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction', 'Towards Logically Consistent Language Models via Probabilistic Reasoning', 'On the Empirical Complexity of Reasoning and Planning in LLMs', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity'}\n",
      "For the paper STaR: Bootstrapping Reasoning With Reasoning  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity', 'Retrieval Augmented Generation for Domain-specific Question Answering', 'TextSquare: Scaling up Text-Centric Visual Instruction Tuning', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'Towards Logically Consistent Language Models via Probabilistic Reasoning', 'NExT: Teaching Large Language Models to Reason about Code Execution', 'On the Empirical Complexity of Reasoning and Planning in LLMs', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper Hungry Hungry Hippos: Towards Language Modeling with State Space Models  we have the following recommendation:  {'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models', 'SpaceByte: Towards Deleting Tokenization from Large Language Modeling', 'TriForce: Lossless Acceleration of Long Sequence Generation with\\n  Hierarchical Speculative Decoding', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey', 'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models', 'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory'}\n",
      "For the paper Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment  we have the following recommendation:  {'Data-Efficient Multimodal Fusion on a Single GPU', 'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality', 'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models', 'RankCLIP: Ranking-Consistent Language-Image Pretraining', 'Residual-based Language Models are Free Boosters for Biomedical Imaging', 'Compositional Chain-of-Thought Prompting for Large Multimodal Models', 'TextSquare: Scaling up Text-Centric Visual Instruction Tuning', 'Cross-Modal Adapter: Parameter-Efficient Transfer Learning Approach for\\n  Vision-Language Models', 'Bridging Vision and Language Spaces with Assignment Prediction', 'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models'}\n",
      "For the paper Visual Time Series Forecasting: An Image-driven Approach  we have the following recommendation:  {'Using ARIMA to Predict the Expansion of Subscriber Data Consumption', 'Variational quantization for state space models', 'Integrating Marketing Channels into Quantile Transformation and Bayesian\\n  Optimization of Ensemble Kernels for Sales Prediction with Gaussian Process\\n  Models', 'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods', 'DeepHeteroIoT: Deep Local and Global Learning over Heterogeneous IoT Sensor Data', 'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting', 'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion', 'TSLANet: Rethinking Transformers for Time Series Representation Learning', 'Generating Synthetic Time Series Data for Cyber-Physical Systems', 'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection'}\n",
      "For the paper Unveiling Transformers with LEGO: a synthetic reasoning task  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'LayerNorm: A key component in parameter-efficient fine-tuning', 'GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers', 'What do Transformers Know about Government?', 'DiJiang: Efficient Large Language Models through Compact Kernelization', 'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity'}\n",
      "For the paper Looped Transformers as Programmable Computers  we have the following recommendation:  {'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium', 'Transformer tricks: Removing weights for skipless transformers', 'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory', 'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection', 'DiJiang: Efficient Large Language Models through Compact Kernelization', 'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory'}\n",
      "For the paper Evaluating Large Language Models in Theory of Mind Tasks  we have the following recommendation:  {'Analyzing the Roles of Language and Vision in Learning from Limited Data', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', \"On Large Language Models' Hallucination with Regard to Known Facts\", 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction', 'On the Empirical Complexity of Reasoning and Planning in LLMs', 'Transformers Can Represent $n$-gram Language Models', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper Learning threshold neurons via the \"edge of stability\"  we have the following recommendation:  {'Training neural networks with structured noise improves classification and generalization', 'A mean curvature flow arising in adversarial training', 'Singular-limit analysis of gradient descent with noise injection', 'A Mean-Field Analysis of Neural Gradient Descent-Ascent: Applications to\\n  Functional Conditional Moment Equations', 'Regularized Gauss-Newton for Optimizing Overparameterized Neural\\n  Networks', 'Regularized Gradient Clipping Provably Trains Wide and Deep Neural\\n  Networks'}\n",
      "For the paper Symbolic Discovery of Optimization Algorithms  we have the following recommendation:  {'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium', 'LD-Pruner: Efficient Pruning of Latent Diffusion Models using\\n  Task-Agnostic Insights', 'Pre-training Small Base LMs with Fewer Tokens', 'FR-NAS: Forward-and-Reverse Graph Predictor for Efficient Neural\\n  Architecture Search', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning', 'Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World', 'PQA: Exploring the Potential of Product Quantization in DNN Hardware Acceleration'}\n",
      "For the paper MoËT: Mixture of Expert Trees and its Application to Verifiable Reinforcement Learning  we have the following recommendation:  {'CtRL-Sim: Reactive and Controllable Driving Agents with Offline Reinforcement Learning', 'Random Network Distillation Based Deep Reinforcement Learning for AGV\\n  Path Planning', 'Generalized Population-Based Training for Hyperparameter Optimization in\\n  Reinforcement Learning', 'BayesJudge: Bayesian Kernel Language Modelling with Confidence\\n  Uncertainty in Legal Judgment Prediction', 'Enhancing Autonomous Vehicle Training with Language Model Integration\\n  and Critical Scenario Generation', 'Adversarial Imitation Learning via Boosting', 'Actor-Critic Reinforcement Learning with Phased Actor', 'Application of the representative measure approach to assess the\\n  reliability of decision trees in dealing with unseen vehicle collision data'}\n",
      "For the paper Continual Backprop: Stochastic Gradient Descent with Persistent Randomness  we have the following recommendation:  {'Training neural networks with structured noise improves classification and generalization', 'Continual Offline Reinforcement Learning via Diffusion-based Dual\\n  Generative Replay', 'FedMeS: Personalized Federated Continual Learning Leveraging Local\\n  Memory', 'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective', 'Actor-Critic Reinforcement Learning with Phased Actor', 'Single-Task Continual Offline Reinforcement Learning', 'Dynamically Anchored Prompting for Task-Imbalanced Continual Learning', 'Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer'}\n",
      "For the paper General-Purpose In-Context Learning by Meta-Learning Transformers  we have the following recommendation:  {'SOPHON: Non-Fine-Tunable Learning to Restrain Task Transferability For\\n  Pre-trained Models', 'Distilled Datamodel with Reverse Gradient Matching', 'NC-TTT: A Noise Contrastive Approach for Test-Time Training', 'Meta Reinforcement Learning with Finite Training Tasks -- a Density Estimation Approach', 'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning', 'Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer', 'Improving Generalization via Meta-Learning on Hard Samples', 'MPC of Uncertain Nonlinear Systems with Meta-Learning for Fast\\n  Adaptation of Neural Predictive Models'}\n",
      "For the paper You Only Live Once: Single-Life Reinforcement Learning  we have the following recommendation:  {'Continual Offline Reinforcement Learning via Diffusion-based Dual\\n  Generative Replay', 'Deep Policy Optimization with Temporal Logic Constraints', 'Goal Exploration via Adaptive Skill Distribution for Goal-Conditioned\\n  Reinforcement Learning', 'Dataset Reset Policy Optimization for RLHF', 'DPO: Differential reinforcement learning with application to optimal\\n  configuration search', 'Adversarial Imitation Learning via Boosting', 'Single-Task Continual Offline Reinforcement Learning', 'Meta Reinforcement Learning with Finite Training Tasks -- a Density Estimation Approach', 'Inferring Behavior-Specific Context Improves Zero-Shot Generalization in\\n  Reinforcement Learning', 'Offline Trajectory Generalization for Offline Reinforcement Learning'}\n",
      "For the paper Human-Timescale Adaptation in an Open-Ended Task Space  we have the following recommendation:  {'Inferring Behavior-Specific Context Improves Zero-Shot Generalization in\\n  Reinforcement Learning', 'Zero-Shot Stitching in Reinforcement Learning using Relative\\n  Representations', 'Goal Exploration via Adaptive Skill Distribution for Goal-Conditioned\\n  Reinforcement Learning', 'MAexp: A Generic Platform for RL-based Multi-Agent Exploration', 'ASID: Active Exploration for System Identification in Robotic\\n  Manipulation', 'Adversarial Imitation Learning via Boosting', 'Meta Reinforcement Learning with Finite Training Tasks -- a Density Estimation Approach', 'Rapid Motor Adaptation for Robotic Manipulator Arms'}\n",
      "For the paper Language Is Not All You Need: Aligning Perception with Language Models  we have the following recommendation:  {'Many-Shot In-Context Learning', 'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality', 'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models', 'RankCLIP: Ranking-Consistent Language-Image Pretraining', 'Compositional Chain-of-Thought Prompting for Large Multimodal Models', 'TextSquare: Scaling up Text-Centric Visual Instruction Tuning', 'Cross-Modal Adapter: Parameter-Efficient Transfer Learning Approach for\\n  Vision-Language Models', 'Bridging Vision and Language Spaces with Assignment Prediction', 'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models', 'Find The Gap: Knowledge Base Reasoning For Visual Question Answering'}\n",
      "For the paper Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback  we have the following recommendation:  {'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Retrieval Augmented Generation for Domain-specific Question Answering', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'TextSquare: Scaling up Text-Centric Visual Instruction Tuning', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity'}\n",
      "For the paper A study on the plasticity of neural networks  we have the following recommendation:  {'NC-TTT: A Noise Contrastive Approach for Test-Time Training', 'Continual Offline Reinforcement Learning via Diffusion-based Dual\\n  Generative Replay', 'FedMeS: Personalized Federated Continual Learning Leveraging Local\\n  Memory', 'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective', 'Delayed Bottlenecking: Alleviating Forgetting in Pre-trained Graph\\n  Neural Networks', 'Single-Task Continual Offline Reinforcement Learning', 'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning', 'Dynamically Anchored Prompting for Task-Imbalanced Continual Learning', 'Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer', 'Continual Learning on a Diet: Learning from Sparsely Labeled Streams\\n  Under Constrained Computation'}\n",
      "For the paper Consistency Models  we have the following recommendation:  {'LD-Pruner: Efficient Pruning of Latent Diffusion Models using\\n  Task-Agnostic Insights', 'Balanced Mixed-Type Tabular Data Synthesis with Diffusion Models', 'EdgeFusion: On-Device Text-to-Image Generation', 'SparseDM: Toward Sparse Efficient Diffusion Models', 'Efficient Conditional Diffusion Model with Probability Flow Sampling for\\n  Image Super-resolution', 'TMPQ-DM: Joint Timestep Reduction and Quantization Precision Selection\\n  for Efficient Diffusion Models'}\n",
      "For the paper Larger language models do in-context learning differently  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Stronger Random Baselines for In-Context Learning', 'Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy\\n  Data', 'Aligning language models with human preferences', 'Pre-training Small Base LMs with Fewer Tokens', 'Localizing Paragraph Memorization in Language Models', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper Permutation Equivariant Neural Functionals  we have the following recommendation:  {'Deep Learning as Ricci Flow', 'Deep Neural Networks via Complex Network Theory: a Perspective', 'Strong Transferable Adversarial Attacks via Ensembled Asymptotically Normal Distribution Learning', 'HOIN: High-Order Implicit Neural Representations', 'FR-NAS: Forward-and-Reverse Graph Predictor for Efficient Neural\\n  Architecture Search', 'Can We Break Free from Strong Data Augmentations in Self-Supervised\\n  Learning?', 'A multiobjective continuation method to compute the regularization path of deep neural networks', 'On permutation-invariant neural networks', 'Reactive Model Correction: Mitigating Harm to Task-Relevant Features via\\n  Conditional Bias Suppression', 'LipSim: A Provably Robust Perceptual Similarity Metric'}\n",
      "For the paper The Prompt Artists  we have the following recommendation:  {'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation', 'Exploring Text-to-Motion Generation with Human Preference', 'Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept\\n  Understanding', 'Ethical-Lens: Curbing Malicious Usages of Open-Source Text-to-Image\\n  Models', 'An Economic Solution to Copyright Challenges of Generative AI', '©Plug-in Authorization for Human Content Copyright Protection\\n  in Text-to-Image Model'}\n",
      "For the paper Sparks of Artificial General Intelligence: Early experiments with GPT-4  we have the following recommendation:  {'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models', 'Analyzing the Roles of Language and Vision in Learning from Limited Data', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models', 'On the Empirical Complexity of Reasoning and Planning in LLMs', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper Online Deep Learning: Learning Deep Neural Networks on the Fly  we have the following recommendation:  {'AntBatchInfer: Elastic Batch Inference in the Kubernetes Cluster', 'FedMeS: Personalized Federated Continual Learning Leveraging Local\\n  Memory', 'Employing Layerwised Unsupervised Learning to Lessen Data and Loss\\n  Requirements in Forward-Forward Algorithms', 'Online Algorithms with Limited Data Retention', 'A Unified Replay-based Continuous Learning Framework for Spatio-Temporal\\n  Prediction on Streaming Data', 'Actor-Critic Reinforcement Learning with Phased Actor', 'Single-Task Continual Offline Reinforcement Learning', 'QCore: Data-Efficient, On-Device Continual Calibration for Quantized\\n  Models -- Extended Version'}\n",
      "For the paper Dynamically Modular and Sparse General Continual Learning  we have the following recommendation:  {'FedMeS: Personalized Federated Continual Learning Leveraging Local\\n  Memory', 'Continual Offline Reinforcement Learning via Diffusion-based Dual\\n  Generative Replay', 'Graph Continual Learning with Debiased Lossless Memory Replay', 'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective', 'A Unified Replay-based Continuous Learning Framework for Spatio-Temporal\\n  Prediction on Streaming Data', 'Delayed Bottlenecking: Alleviating Forgetting in Pre-trained Graph\\n  Neural Networks', 'Dynamically Anchored Prompting for Task-Imbalanced Continual Learning', 'Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World', 'Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer', 'Continual Learning on a Diet: Learning from Sparsely Labeled Streams\\n  Under Constrained Computation'}\n",
      "For the paper Meta-learning via Language Model In-context Tuning  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning', 'When LLMs are Unfit Use FastFit: Fast and Effective Text Classification\\n  with Many Classes', 'Stronger Random Baselines for In-Context Learning', 'Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy\\n  Data', 'OTTER: Improving Zero-Shot Classification via Optimal Transport', 'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees', 'Improving Generalization via Meta-Learning on Hard Samples'}\n",
      "For the paper SemSup: Semantic Supervision for Simple and Scalable Zero-shot Generalization  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning', 'When LLMs are Unfit Use FastFit: Fast and Effective Text Classification\\n  with Many Classes', 'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality', 'OTTER: Improving Zero-Shot Classification via Optimal Transport', 'RankCLIP: Ranking-Consistent Language-Image Pretraining', 'Data Alignment for Zero-Shot Concept Generation in Dermatology AI', 'Compositional Chain-of-Thought Prompting for Large Multimodal Models'}\n",
      "For the paper A Neuro-vector-symbolic Architecture for Solving Raven's Progressive Matrices  we have the following recommendation:  {'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium', 'Symbolic Integration Algorithm Selection with Machine Learning: LSTMs vs\\n  Tree LSTMs', 'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective', 'FR-NAS: Forward-and-Reverse Graph Predictor for Efficient Neural\\n  Architecture Search', 'Complexity of Probabilistic Reasoning for Neurosymbolic Classification\\n  Techniques', 'Neuro-Inspired Information-Theoretic Hierarchical Perception for\\n  Multimodal Learning', 'On the Independence Assumption in Neurosymbolic Learning', 'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory'}\n",
      "For the paper Transfer Learning with Deep Tabular Models  we have the following recommendation:  {'Machine Learning Techniques for MRI Data Processing at Expanding Scale', 'An improved tabular data generator with VAE-GMM integration', 'Time-aware Heterogeneous Graph Transformer with Adaptive Attention\\n  Merging for Health Event Prediction', 'Measuring Feature Dependency of Neural Networks by Collapsing Feature\\n  Dimensions in the Data Manifold', 'Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science', 'Explainable Lung Disease Classification from Chest X-Ray Images\\n  Utilizing Deep Learning and XAI', 'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning', 'Joint chest X-ray diagnosis and clinical visual attention prediction with multi-stage cooperative learning: enhancing interpretability'}\n",
      "For the paper Improving Code Generation by Training with Natural Language Feedback  we have the following recommendation:  {'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Aligning language models with human preferences', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'NExT: Teaching Large Language Models to Reason about Code Execution', 'XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\\n  Upcycled Mixture-of-Experts', 'Pre-training Small Base LMs with Fewer Tokens', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper Self-Refine: Iterative Refinement with Self-Feedback  we have the following recommendation:  {'Many-Shot In-Context Learning', 'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level\\n  Knowledge Distillation', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Towards a Robust Retrieval-Based Summarization System', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'Towards Logically Consistent Language Models via Probabilistic Reasoning', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper A Comparison of Reinforcement Learning and Deep Trajectory Based Stochastic Control Agents for Stepwise Mean-Variance Hedging  we have the following recommendation:  {'VC Theory for Inventory Policies', 'DPO: Differential reinforcement learning with application to optimal\\n  configuration search', 'Actor-Critic Reinforcement Learning with Phased Actor', 'A Mean-Field Analysis of Neural Gradient Descent-Ascent: Applications to\\n  Functional Conditional Moment Equations', 'Continuous-time Risk-sensitive Reinforcement Learning via Quadratic\\n  Variation Penalty', 'Reinforcement Learning in Agent-Based Market Simulation: Unveiling Realistic Stylized Facts and Behavior'}\n",
      "For the paper Language Models can Solve Computer Tasks  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Retrieval Augmented Generation for Domain-specific Question Answering', 'AgentKit: Flow Engineering with Graphs, not Coding', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models', 'On the Empirical Complexity of Reasoning and Planning in LLMs', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity'}\n",
      "For the paper HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face  we have the following recommendation:  {'Analyzing the Roles of Language and Vision in Learning from Limited Data', 'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models', 'AgentKit: Flow Engineering with Graphs, not Coding', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'On the Empirical Complexity of Reasoning and Planning in LLMs'}\n",
      "For the paper Better Language Models of Code through Self-Improvement  we have the following recommendation:  {'Automated Multi-Language to English Machine Translation Using Generative\\n  Pre-Trained Transformers', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level\\n  Knowledge Distillation', 'Pre-training Small Base LMs with Fewer Tokens', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'CroissantLLM: A Truly Bilingual French-English Language Model', 'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models', 'NExT: Teaching Large Language Models to Reason about Code Execution', 'XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\\n  Upcycled Mixture-of-Experts', 'Jamba: A Hybrid Transformer-Mamba Language Model'}\n",
      "For the paper Generative Agents: Interactive Simulacra of Human Behavior  we have the following recommendation:  {'iRAG: An Incremental Retrieval Augmented Generation System for Videos', 'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'iTBLS: A Dataset of Interactive Conversations Over Tabular Information', 'Decoding AI: The inside story of data analysis in ChatGPT', 'AgentKit: Flow Engineering with Graphs, not Coding', 'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models', 'PhyScene: Physically Interactable 3D Scene Synthesis for Embodied AI'}\n",
      "For the paper Evaluating Verifiability in Generative Search Engines  we have the following recommendation:  {'iRAG: An Incremental Retrieval Augmented Generation System for Videos', 'STRUM-LLM: Attributed and Structured Contrastive Summarization', 'Retrieval Augmented Generation for Domain-specific Question Answering', 'MUGC: Machine Generated versus User Generated Content Detection', 'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation', 'AI-Generated Faces in the Real World: A Large-Scale Case Study of\\n  Twitter Profile Images', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'Towards a Robust Retrieval-Based Summarization System', 'Reliability Estimation of News Media Sources: Birds of a Feather Flock\\n  Together', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity'}\n",
      "For the paper Prompting Is Programming: A Query Language for Large Language Models  we have the following recommendation:  {'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\\n  Prediction', 'Retrieval Augmented Generation for Domain-specific Question Answering', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation', 'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction'}\n",
      "For the paper Scaling Transformer to 1M tokens and beyond with RMT  we have the following recommendation:  {'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory', 'SpaceByte: Towards Deleting Tokenization from Large Language Modeling', 'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving', 'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models', 'DiJiang: Efficient Large Language Models through Compact Kernelization', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory'}\n",
      "For the paper Repository-Level Prompt Generation for Large Language Models of Code  we have the following recommendation:  {'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation', 'LLM-Powered Test Case Generation for Detecting Tricky Bugs', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'NExT: Teaching Large Language Models to Reason about Code Execution', 'XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\\n  Upcycled Mixture-of-Experts', 'Beyond Code Generation: An Observational Study of ChatGPT Usage in\\n  Software Engineering Practice'}\n",
      "For the paper Self-Instruct: Aligning Language Models with Self-Generated Instructions  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'CroissantLLM: A Truly Bilingual French-English Language Model', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'Pre-training Small Base LMs with Fewer Tokens', 'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper Unlimiformer: Long-Range Transformers with Unlimited Length Input  we have the following recommendation:  {'Automated Multi-Language to English Machine Translation Using Generative\\n  Pre-Trained Transformers', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models', 'SpaceByte: Towards Deleting Tokenization from Large Language Modeling', 'GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers', 'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models', 'DiJiang: Efficient Large Language Models through Compact Kernelization', 'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory'}\n",
      "For the paper Causal Reasoning and Large Language Models: Opening a New Frontier for Causality  we have the following recommendation:  {'Neural Networks with Causal Graph Constraints: A New Approach for\\n  Treatment Effects Estimation', 'Benchmarking Counterfactual Image Generation', 'Bias patterns in the application of LLMs for clinical decision support:\\n  A comprehensive study', 'LLM-Enhanced Causal Discovery in Temporal Domain from Interventional\\n  Data', 'Redefining the Shortest Path Problem Formulation of the Linear\\n  Non-Gaussian Acyclic Model: Pairwise Likelihood Ratios, Prior Knowledge, and\\n  Path Enumeration', 'CT-Agent: Clinical Trial Multi-Agent with Large Language Model-based\\n  Reasoning', 'Enhancing Counterfactual Explanation Search with Diffusion Distance and\\n  Directional Coherence', 'On the Empirical Complexity of Reasoning and Planning in LLMs'}\n",
      "For the paper Masked Trajectory Models for Prediction, Representation, and Control  we have the following recommendation:  {'CtRL-Sim: Reactive and Controllable Driving Agents with Offline Reinforcement Learning', 'Continual Offline Reinforcement Learning via Diffusion-based Dual\\n  Generative Replay', 'Deep Policy Optimization with Temporal Logic Constraints', 'DPO: Differential reinforcement learning with application to optimal\\n  configuration search', 'Actor-Critic Reinforcement Learning with Phased Actor', 'Adversarial Imitation Learning via Boosting', 'Adaptive Regularization of Representation Rank as an Implicit Constraint\\n  of Bellman Equation', 'Offline Trajectory Generalization for Offline Reinforcement Learning'}\n",
      "For the paper Replacing Rewards with Examples: Example-Based Policy Search via Recursive Classification  we have the following recommendation:  {'Generalized Population-Based Training for Hyperparameter Optimization in\\n  Reinforcement Learning', 'Deep Policy Optimization with Temporal Logic Constraints', 'Dataset Reset Policy Optimization for RLHF', 'DPO: Differential reinforcement learning with application to optimal\\n  configuration search', 'Actor-Critic Reinforcement Learning with Phased Actor', 'Adaptive Regularization of Representation Rank as an Implicit Constraint\\n  of Bellman Equation', 'Meta Reinforcement Learning with Finite Training Tasks -- a Density Estimation Approach', 'From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function'}\n",
      "For the paper Pretraining Without Attention  we have the following recommendation:  {'Toward a Theory of Tokenization in LLMs', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'SpaceByte: Towards Deleting Tokenization from Large Language Modeling', 'LayerNorm: A key component in parameter-efficient fine-tuning', 'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models', 'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey', 'Pre-training Small Base LMs with Fewer Tokens', 'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory'}\n",
      "For the paper Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision  we have the following recommendation:  {'Stepwise Alignment for Constrained Language Model Policy Optimization', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Aligning language models with human preferences', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'Towards Logically Consistent Language Models via Probabilistic Reasoning', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers  we have the following recommendation:  {'LD-Pruner: Efficient Pruning of Latent Diffusion Models using\\n  Task-Agnostic Insights', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models', 'SpaceByte: Towards Deleting Tokenization from Large Language Modeling', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'DiJiang: Efficient Large Language Models through Compact Kernelization', 'Pre-training Small Base LMs with Fewer Tokens', 'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory'}\n",
      "For the paper The ConceptARC Benchmark: Evaluating Understanding and Generalization in the ARC Domain  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Analyzing the Roles of Language and Vision in Learning from Limited Data', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Decoding AI: The inside story of data analysis in ChatGPT', 'Pre-trained Vision-Language Models Learn Discoverable Visual Concepts', 'Compositional Chain-of-Thought Prompting for Large Multimodal Models', 'Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept\\n  Understanding', 'TextSquare: Scaling up Text-Centric Visual Instruction Tuning', 'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models', 'On the Empirical Complexity of Reasoning and Planning in LLMs'}\n",
      "For the paper Symbol tuning improves in-context learning in language models  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy\\n  Data', 'Aligning language models with human preferences', 'LayerNorm: A key component in parameter-efficient fine-tuning', 'CroissantLLM: A Truly Bilingual French-English Language Model', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper Online Continual Learning Without the Storage Constraint  we have the following recommendation:  {'FedMeS: Personalized Federated Continual Learning Leveraging Local\\n  Memory', 'Continual Offline Reinforcement Learning via Diffusion-based Dual\\n  Generative Replay', 'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective', 'A Unified Replay-based Continuous Learning Framework for Spatio-Temporal\\n  Prediction on Streaming Data', 'Single-Task Continual Offline Reinforcement Learning', 'Dynamically Anchored Prompting for Task-Imbalanced Continual Learning', 'QCore: Data-Efficient, On-Device Continual Calibration for Quantized\\n  Models -- Extended Version', 'kNN-CLIP: Retrieval Enables Training-Free Segmentation on Continually\\n  Expanding Large Vocabularies', 'Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer', 'Continual Learning on a Diet: Learning from Sparsely Labeled Streams\\n  Under Constrained Computation'}\n",
      "For the paper TinyStories: How Small Can Language Models Be and Still Speak Coherent English?  we have the following recommendation:  {'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models', 'Automated Multi-Language to English Machine Translation Using Generative\\n  Pre-Trained Transformers', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level\\n  Knowledge Distillation', 'Pre-training Small Base LMs with Fewer Tokens', 'CroissantLLM: A Truly Bilingual French-English Language Model', 'Towards Logically Consistent Language Models via Probabilistic Reasoning', 'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models', 'Jamba: A Hybrid Transformer-Mamba Language Model'}\n",
      "For the paper Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task  we have the following recommendation:  {'Analyzing the Roles of Language and Vision in Learning from Limited Data', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Self-playing Adversarial Language Game Enhances LLM Reasoning', \"On Large Language Models' Hallucination with Regard to Known Facts\", 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models', 'Transformers Can Represent $n$-gram Language Models', 'Localizing Paragraph Memorization in Language Models', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper Tree of Thoughts: Deliberate Problem Solving with Large Language Models  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'AgentKit: Flow Engineering with Graphs, not Coding', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'Compositional Chain-of-Thought Prompting for Large Multimodal Models', 'TextSquare: Scaling up Text-Centric Visual Instruction Tuning', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models', 'On the Empirical Complexity of Reasoning and Planning in LLMs', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity'}\n",
      "For the paper Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions  we have the following recommendation:  {'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Self-playing Adversarial Language Game Enhances LLM Reasoning', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction', 'Towards Logically Consistent Language Models via Probabilistic Reasoning', 'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models'}\n",
      "For the paper Has It All Been Solved? Open NLP Research Questions Not Solved by Large Language Models  we have the following recommendation:  {'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning', 'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction', 'Towards Logically Consistent Language Models via Probabilistic Reasoning', 'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages', 'On the Empirical Complexity of Reasoning and Planning in LLMs'}\n",
      "For the paper Improving Factuality and Reasoning in Language Models through Multiagent Debate  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', \"On Large Language Models' Hallucination with Regard to Known Facts\", 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'Towards Logically Consistent Language Models via Probabilistic Reasoning', 'On the Empirical Complexity of Reasoning and Planning in LLMs', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper Meta-Learning Online Adaptation of Language Models  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Stronger Random Baselines for In-Context Learning', 'Retrieval Augmented Generation for Domain-specific Question Answering', 'Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy\\n  Data', 'LayerNorm: A key component in parameter-efficient fine-tuning', 'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees', 'Rethinking LLM Memorization through the Lens of Adversarial Compression', 'Pre-training Small Base LMs with Fewer Tokens', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback  we have the following recommendation:  {'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Stronger Random Baselines for In-Context Learning', 'Aligning language models with human preferences', 'FedEval-LLM: Federated Evaluation of Large Language Models on Downstream\\n  Tasks with Collective Wisdom', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity', 'Retrieval Augmented Generation for Domain-specific Question Answering', 'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction', 'Towards Logically Consistent Language Models via Probabilistic Reasoning', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper Direct Preference Optimization: Your Language Model is Secretly a Reward Model  we have the following recommendation:  {'Learn Your Reference Model for Real Good Alignment', 'Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy\\n  Data', 'Aligning language models with human preferences', 'Dataset Reset Policy Optimization for RLHF', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function', 'Filtered Direct Preference Optimization'}\n",
      "For the paper Thought Cloning: Learning to Think while Acting by Imitating Human Thinking  we have the following recommendation:  {'Analyzing the Roles of Language and Vision in Learning from Limited Data', 'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Adversarial Imitation Learning via Boosting', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper LIMA: Less Is More for Alignment  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy\\n  Data', 'Aligning language models with human preferences', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages', 'Pre-training Small Base LMs with Fewer Tokens', 'Filtered Direct Preference Optimization', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper Gorilla: Large Language Model Connected with Massive APIs  we have the following recommendation:  {'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium', 'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\\n  Prediction', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models', 'Pre-training Small Base LMs with Fewer Tokens'}\n",
      "For the paper Deductive Verification of Chain-of-Thought Reasoning  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'AgentKit: Flow Engineering with Graphs, not Coding', 'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation', \"On Large Language Models' Hallucination with Regard to Known Facts\", 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'Towards Logically Consistent Language Models via Probabilistic Reasoning', 'NExT: Teaching Large Language Models to Reason about Code Execution', 'On the Empirical Complexity of Reasoning and Planning in LLMs', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity'}\n",
      "For the paper INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models  we have the following recommendation:  {'Many-Shot In-Context Learning', 'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'LayerNorm: A key component in parameter-efficient fine-tuning', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'TextSquare: Scaling up Text-Centric Visual Instruction Tuning', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper PaLI-X: On Scaling up a Multilingual Vision and Language Model  we have the following recommendation:  {'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality', 'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models', 'RankCLIP: Ranking-Consistent Language-Image Pretraining', 'Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models', 'Compositional Chain-of-Thought Prompting for Large Multimodal Models', 'TextSquare: Scaling up Text-Centric Visual Instruction Tuning', 'Cross-Modal Adapter: Parameter-Efficient Transfer Learning Approach for\\n  Vision-Language Models', 'Bridging Vision and Language Spaces with Assignment Prediction', 'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models', 'Find The Gap: Knowledge Base Reasoning For Visual Question Answering'}\n",
      "For the paper Passive learning of active causal strategies in agents and language models  we have the following recommendation:  {'Analyzing the Roles of Language and Vision in Learning from Limited Data', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'LLM-Enhanced Causal Discovery in Temporal Domain from Interventional\\n  Data', 'Aligning language models with human preferences', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper Augmenting Language Models with Long-Term Memory  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'LongEmbed: Extending Embedding Models for Long Context Retrieval', 'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving', 'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models', 'Localizing Paragraph Memorization in Language Models'}\n",
      "For the paper V-LoL: A Diagnostic Dataset for Visual Logical Learning  we have the following recommendation:  {'Concept-based Analysis of Neural Networks via Vision-Language Models', 'Hyper Evidential Deep Learning to Quantify Composite Classification\\n  Uncertainty', 'Analyzing the Roles of Language and Vision in Learning from Limited Data', 'Deep Policy Optimization with Temporal Logic Constraints', 'Zero-Shot Stitching in Reinforcement Learning using Relative\\n  Representations', 'Pre-trained Vision-Language Models Learn Discoverable Visual Concepts', 'Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V', 'Compositional Chain-of-Thought Prompting for Large Multimodal Models', 'TextSquare: Scaling up Text-Centric Visual Instruction Tuning', 'LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos'}\n",
      "For the paper Fine-Tuning Language Models with Just Forward Passes  we have the following recommendation:  {'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium', 'Many-Shot In-Context Learning', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'CroissantLLM: A Truly Bilingual French-English Language Model', 'TriForce: Lossless Acceleration of Long Sequence Generation with\\n  Hierarchical Speculative Decoding', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models', 'Pre-training Small Base LMs with Fewer Tokens'}\n",
      "For the paper The Curse of Recursion: Training on Generated Data Makes Models Forget  we have the following recommendation:  {'LD-Pruner: Efficient Pruning of Latent Diffusion Models using\\n  Task-Agnostic Insights', 'EdgeFusion: On-Device Text-to-Image Generation', 'Residual-based Language Models are Free Boosters for Biomedical Imaging', 'Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept\\n  Understanding', 'Pre-training Small Base LMs with Fewer Tokens', 'Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging\\n  Perturbations That Efficiently Fool Customized Diffusion Models'}\n",
      "For the paper System-Level Natural Language Feedback  we have the following recommendation:  {'AgentKit: Flow Engineering with Graphs, not Coding', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation', 'ReALM: Reference Resolution As Language Modeling', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'NExT: Teaching Large Language Models to Reason about Code Execution', 'Beyond Code Generation: An Observational Study of ChatGPT Usage in\\n  Software Engineering Practice', 'Filtered Direct Preference Optimization'}\n",
      "For the paper Supervised Pretraining Can Learn In-Context Reinforcement Learning  we have the following recommendation:  {'Deep Policy Optimization with Temporal Logic Constraints', 'Decision Mamba: Reinforcement Learning via Sequence Modeling with Selective State Spaces', 'CAESAR: Enhancing Federated RL in Heterogeneous MDPs through Convergence-Aware Sampling with Screening', 'Goal Exploration via Adaptive Skill Distribution for Goal-Conditioned\\n  Reinforcement Learning', 'Randomized Exploration in Cooperative Multi-Agent Reinforcement Learning', 'Dataset Reset Policy Optimization for RLHF', 'Settling Constant Regrets in Linear Markov Decision Processes', 'Meta Reinforcement Learning with Finite Training Tasks -- a Density Estimation Approach', 'Tree Bandits for Generative Bayes', 'Offline Trajectory Generalization for Offline Reinforcement Learning'}\n",
      "For the paper ChemCrow: Augmenting large-language models with chemistry tools  we have the following recommendation:  {'RLSynC: Offline-Online Reinforcement Learning for Synthon Completion', 'Physical formula enhanced multi-task learning for pharmacokinetics\\n  prediction', '$\\\\texttt{MiniMol}$: A Parameter-Efficient Foundation Model for Molecular\\n  Learning', 'TacoGFN: Target-conditioned GFlowNet for Structure-based Drug Design', 'Physics-informed active learning for accelerating quantum chemical\\n  simulations', 'A Self-feedback Knowledge Elicitation Approach for Chemical Reaction\\n  Predictions', 'Mol-AIR: Molecular Reinforcement Learning with Adaptive Intrinsic Rewards for Goal-directed Molecular Generation', 'On the Scalability of GNNs for Molecular Graphs'}\n",
      "For the paper Scientists' Perspectives on the Potential for Generative AI in their Fields  we have the following recommendation:  {'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions', 'iTBLS: A Dataset of Interactive Conversations Over Tabular Information', 'Decoding AI: The inside story of data analysis in ChatGPT', 'MUGC: Machine Generated versus User Generated Content Detection', 'AI-Generated Faces in the Real World: A Large-Scale Case Study of\\n  Twitter Profile Images', 'Ethical-Lens: Curbing Malicious Usages of Open-Source Text-to-Image\\n  Models', 'An Economic Solution to Copyright Challenges of Generative AI', 'Language Model Prompt Selection via Simulation Optimization'}\n",
      "For the paper Validating Large Language Models with ReLM  we have the following recommendation:  {'Learning representations of learning representations', 'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models', 'Bias patterns in the application of LLMs for clinical decision support:\\n  A comprehensive study', 'Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path\\n  Forward', 'Aligning language models with human preferences', 'RTP-LX: Can LLMs Evaluate Toxicity in Multilingual Scenarios?', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Rethinking LLM Memorization through the Lens of Adversarial Compression'}\n",
      "For the paper LongNet: Scaling Transformers to 1,000,000,000 Tokens  we have the following recommendation:  {'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'SpaceByte: Towards Deleting Tokenization from Large Language Modeling', 'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models', 'DiJiang: Efficient Large Language Models through Compact Kernelization', 'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey', 'Pre-training Small Base LMs with Fewer Tokens', 'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory'}\n",
      "For the paper Retentive Network: A Successor to Transformer for Large Language Models  we have the following recommendation:  {'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'SpaceByte: Towards Deleting Tokenization from Large Language Modeling', 'TriForce: Lossless Acceleration of Long Sequence Generation with\\n  Hierarchical Speculative Decoding', 'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models', 'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory'}\n",
      "For the paper Conservative Prediction via Data-Driven Confidence Minimization  we have the following recommendation:  {'Would You Trust an AI Doctor? Building Reliable Medical Predictions with\\n  Kernel Dropout Uncertainty', 'Trusted Multi-view Learning with Label Noise', 'Gradient-Regularized Out-of-Distribution Detection', 'Explainable Machine Learning System for Predicting Chronic Kidney\\n  Disease in High-Risk Cardiovascular Patients', 'Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World', 'Toward a Realistic Benchmark for Out-of-Distribution Detection'}\n",
      "For the paper Learning to Model the World with Language  we have the following recommendation:  {'Analyzing the Roles of Language and Vision in Learning from Limited Data', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality', 'Zero-Shot Stitching in Reinforcement Learning using Relative\\n  Representations', 'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models', 'Compositional Chain-of-Thought Prompting for Large Multimodal Models', 'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models', 'LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos', 'Inferring Behavior-Specific Context Improves Zero-Shot Generalization in\\n  Reinforcement Learning', 'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models'}\n",
      "For the paper Path Shadowing Monte-Carlo  we have the following recommendation:  {'Neural Flow Diffusion Models: Learnable Forward Process for Improved\\n  Diffusion Modelling', 'Estimating the Distribution of Parameters in Differential Equations with\\n  Repeated Cross-Sectional Data', 'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods', 'Multidimensional Interpolants', 'Variational Flow Models: Flowing in Your Style', 'Neural McKean-Vlasov Processes: Distributional Dependence in Diffusion\\n  Processes', 'Multi-fidelity Gaussian process surrogate modeling for regression\\n  problems in physics', 'Reinforcement Learning in Agent-Based Market Simulation: Unveiling Realistic Stylized Facts and Behavior'}\n",
      "For the paper Self-Alignment with Instruction Backtranslation  we have the following recommendation:  {'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Learn Your Reference Model for Real Good Alignment', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'Aligning language models with human preferences', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages', 'Filtered Direct Preference Optimization', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper Bayesian Flow Networks  we have the following recommendation:  {'LD-Pruner: Efficient Pruning of Latent Diffusion Models using\\n  Task-Agnostic Insights', 'SparseDM: Toward Sparse Efficient Diffusion Models', 'Physics-integrated generative modeling using attentive planar\\n  normalizing flow based variational autoencoder', 'TMPQ-DM: Joint Timestep Reduction and Quantization Precision Selection\\n  for Efficient Diffusion Models', 'All-in-one simulation-based inference', 'Neural Flow Diffusion Models: Learnable Forward Process for Improved\\n  Diffusion Modelling'}\n",
      "For the paper Evaluating Embedding APIs for Information Retrieval  we have the following recommendation:  {'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning', 'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking', 'LongEmbed: Extending Embedding Models for Long Context Retrieval', 'CroissantLLM: A Truly Bilingual French-English Language Model', 'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees', 'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction', 'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages', 'VDTuner: Automated Performance Tuning for Vector Data Management Systems'}\n",
      "For the paper Learning One Abstract Bit at a Time Through Self-Invented Experiments Encoded as Neural Networks  we have the following recommendation:  {'Deep Policy Optimization with Temporal Logic Constraints', 'Decoding AI: The inside story of data analysis in ChatGPT', 'AgentKit: Flow Engineering with Graphs, not Coding', 'The Causal Chambers: Real Physical Systems as a Testbed for AI\\n  Methodology', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Mol-AIR: Molecular Reinforcement Learning with Adaptive Intrinsic Rewards for Goal-directed Molecular Generation', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function'}\n",
      "For the paper Teach LLMs to Personalize -- An Approach inspired by Writing Education  we have the following recommendation:  {'Automated Multi-Language to English Machine Translation Using Generative\\n  Pre-Trained Transformers', 'Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level\\n  Knowledge Distillation', 'MUGC: Machine Generated versus User Generated Content Detection', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'Towards a Robust Retrieval-Based Summarization System', 'Exploring Text-to-Motion Generation with Human Preference', 'Language Model Prompt Selection via Simulation Optimization', 'Filtered Direct Preference Optimization'}\n",
      "For the paper Efficient Guided Generation for Large Language Models  we have the following recommendation:  {'Toward a Theory of Tokenization in LLMs', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level\\n  Knowledge Distillation', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation', 'TriForce: Lossless Acceleration of Long Sequence Generation with\\n  Hierarchical Speculative Decoding', 'Language Model Prompt Selection via Simulation Optimization', 'σ-GPTs: A New Approach to Autoregressive Models'}\n",
      "For the paper Visual Instruction Tuning  we have the following recommendation:  {'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality', 'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models', 'RankCLIP: Ranking-Consistent Language-Image Pretraining', 'Self-Supervised Visual Preference Alignment', 'Compositional Chain-of-Thought Prompting for Large Multimodal Models', 'TextSquare: Scaling up Text-Centric Visual Instruction Tuning', 'Cross-Modal Adapter: Parameter-Efficient Transfer Learning Approach for\\n  Vision-Language Models', 'Bridging Vision and Language Spaces with Assignment Prediction', 'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models'}\n",
      "For the paper Giraffe: Adventures in Expanding Context Lengths in LLMs  we have the following recommendation:  {'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'LongEmbed: Extending Embedding Models for Long Context Retrieval', 'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models', 'Pre-training Small Base LMs with Fewer Tokens', 'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory'}\n",
      "For the paper Large Language Models as General Pattern Machines  we have the following recommendation:  {'Toward a Theory of Tokenization in LLMs', 'Many-Shot In-Context Learning', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models', 'σ-GPTs: A New Approach to Autoregressive Models'}\n",
      "For the paper A Neurodiversity-Inspired Solver for the Abstraction \\& Reasoning Corpus (ARC) Using Visual Imagery and Program Synthesis  we have the following recommendation:  {'Concept-based Analysis of Neural Networks via Vision-Language Models', 'Analyzing the Roles of Language and Vision in Learning from Limited Data', 'Pre-trained Vision-Language Models Learn Discoverable Visual Concepts', 'Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V', 'GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers', 'Compositional Chain-of-Thought Prompting for Large Multimodal Models', 'LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory', 'Find The Gap: Knowledge Base Reasoning For Visual Question Answering'}\n",
      "For the paper Learning to Learn Financial Networks for Optimising Momentum Strategies  we have the following recommendation:  {'OptiGrad: A Fair and more Efficient Price Elasticity Optimization via a\\n  Gradient Based Learning', 'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods', 'DFWLayer: Differentiable Frank-Wolfe Optimization Layer', 'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion', 'FR-NAS: Forward-and-Reverse Graph Predictor for Efficient Neural\\n  Architecture Search', 'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting', 'TSLANet: Rethinking Transformers for Time Series Representation Learning', 'A Survey of Deep Long-Tail Classification Advancements', 'A novel decision fusion approach for sale price prediction using Elastic Net and MOPSO', 'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection'}\n",
      "For the paper Network Momentum across Asset Classes  we have the following recommendation:  {'Conformal Predictive Systems Under Covariate Shift', 'Reinforcement Learning in Agent-Based Market Simulation: Unveiling Realistic Stylized Facts and Behavior', 'Using ARIMA to Predict the Expansion of Subscriber Data Consumption', 'Conformal Prediction for Stochastic Decision-Making of PV Power in Electricity Markets', 'Time-aware Heterogeneous Graph Transformer with Adaptive Attention\\n  Merging for Health Event Prediction', 'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods', 'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion', 'TSLANet: Rethinking Transformers for Time Series Representation Learning', 'AGHINT: Attribute-Guided Representation Learning on Heterogeneous\\n  Information Networks with Transformer', 'A novel decision fusion approach for sale price prediction using Elastic Net and MOPSO'}\n",
      "For the paper VolTS: A Volatility-based Trading System to forecast Stock Markets Trend using Statistics and Machine Learning  we have the following recommendation:  {'Using ARIMA to Predict the Expansion of Subscriber Data Consumption', 'Conformal Prediction for Stochastic Decision-Making of PV Power in Electricity Markets', 'A Guide to Feature Importance Methods for Scientific Inference', 'Integrating Marketing Channels into Quantile Transformation and Bayesian\\n  Optimization of Ensemble Kernels for Sales Prediction with Gaussian Process\\n  Models', 'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods', 'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion', 'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting', 'TSLANet: Rethinking Transformers for Time Series Representation Learning', 'A novel decision fusion approach for sale price prediction using Elastic Net and MOPSO', 'Reinforcement Learning in Agent-Based Market Simulation: Unveiling Realistic Stylized Facts and Behavior'}\n",
      "For the paper Graph of Thoughts: Solving Elaborate Problems with Large Language Models  we have the following recommendation:  {'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'AgentKit: Flow Engineering with Graphs, not Coding', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Compositional Chain-of-Thought Prompting for Large Multimodal Models', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'On the Empirical Complexity of Reasoning and Planning in LLMs', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity'}\n",
      "For the paper Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies  we have the following recommendation:  {'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Bias patterns in the application of LLMs for clinical decision support:\\n  A comprehensive study', 'Aligning language models with human preferences', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model', 'RTP-LX: Can LLMs Evaluate Toxicity in Multilingual Scenarios?', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Filtered Direct Preference Optimization', 'Using LLMs to Model the Beliefs and Preferences of Targeted Populations'}\n",
      "For the paper Gated recurrent neural networks discover attention  we have the following recommendation:  {'Toward a Theory of Tokenization in LLMs', 'Training neural networks with structured noise improves classification and generalization', 'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory', 'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models', 'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection', 'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey', 'DiJiang: Efficient Large Language Models through Compact Kernelization', 'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory'}\n",
      "For the paper Code Llama: Open Foundation Models for Code  we have the following recommendation:  {'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium', 'Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\\n  Prediction', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study', 'TriForce: Lossless Acceleration of Long Sequence Generation with\\n  Hierarchical Speculative Decoding', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models', 'XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\\n  Upcycled Mixture-of-Experts', 'Pre-training Small Base LMs with Fewer Tokens'}\n",
      "For the paper One Wide Feedforward is All You Need  we have the following recommendation:  {'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory', 'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models', 'SpaceByte: Towards Deleting Tokenization from Large Language Modeling', 'DiJiang: Efficient Large Language Models through Compact Kernelization', 'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory'}\n",
      "For the paper Nougat: Neural Optical Understanding for Academic Documents  we have the following recommendation:  {'Towards a Foundation Model for Partial Differential Equation:\\n  Multi-Operator Learning and Extrapolation', 'AlloyBERT: Alloy Property Prediction with Large Language Models', 'MathWriting: A Dataset For Handwritten Mathematical Expression\\n  Recognition', 'Residual-based Language Models are Free Boosters for Biomedical Imaging', 'GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers', 'GatedLexiconNet: A Comprehensive End-to-End Handwritten Paragraph Text\\n  Recognition System', 'PARAMANU-GANITA: Language Model with Mathematical Capabilities', 'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory'}\n",
      "For the paper Prompt2Model: Generating Deployable Models from Natural Language Instructions  we have the following recommendation:  {'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning', 'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models', 'Many-Shot In-Context Learning', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Stronger Random Baselines for In-Context Learning', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models', 'Retrieval Augmented Generation for Domain-specific Question Answering', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs'}\n",
      "For the paper Trading via Selective Classification  we have the following recommendation:  {'Using ARIMA to Predict the Expansion of Subscriber Data Consumption', 'OptiGrad: A Fair and more Efficient Price Elasticity Optimization via a\\n  Gradient Based Learning', 'Conformal Prediction for Stochastic Decision-Making of PV Power in Electricity Markets', 'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods', 'VC Theory for Inventory Policies', 'DeepHeteroIoT: Deep Local and Global Learning over Heterogeneous IoT Sensor Data', 'Learning using granularity statistical invariants for classification', 'A novel decision fusion approach for sale price prediction using Elastic Net and MOPSO', 'Mining Invariance from Nonlinear Multi-Environment Data: Binary\\n  Classification', 'Reinforcement Learning in Agent-Based Market Simulation: Unveiling Realistic Stylized Facts and Behavior'}\n",
      "For the paper Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning  we have the following recommendation:  {'Data-Efficient Multimodal Fusion on a Single GPU', 'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality', 'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models', 'RankCLIP: Ranking-Consistent Language-Image Pretraining', 'Residual-based Language Models are Free Boosters for Biomedical Imaging', 'CroissantLLM: A Truly Bilingual French-English Language Model', 'Compositional Chain-of-Thought Prompting for Large Multimodal Models', 'Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept\\n  Understanding', 'Cross-Modal Adapter: Parameter-Efficient Transfer Learning Approach for\\n  Vision-Language Models', 'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models'}\n",
      "For the paper Are Emergent Abilities in Large Language Models just In-Context Learning?  we have the following recommendation:  {'Many-Shot In-Context Learning', 'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction', 'On the Empirical Complexity of Reasoning and Planning in LLMs', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper Large Language Models as Optimizers  we have the following recommendation:  {'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium', 'Many-Shot In-Context Learning', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\\n  Prediction', 'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing'}\n",
      "For the paper Fluid Transformers and Creative Analogies: Exploring Large Language Models' Capacity for Augmenting Cross-Domain Analogical Creativity  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level\\n  Knowledge Distillation', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Compositional Chain-of-Thought Prompting for Large Multimodal Models', 'TextSquare: Scaling up Text-Centric Visual Instruction Tuning', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept\\n  Understanding', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity'}\n",
      "For the paper NExT-GPT: Any-to-Any Multimodal LLM  we have the following recommendation:  {'Data-Efficient Multimodal Fusion on a Single GPU', 'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models', 'Compositional Chain-of-Thought Prompting for Large Multimodal Models', 'TextSquare: Scaling up Text-Centric Visual Instruction Tuning', 'Sparse multimodal fusion with modal channel attention', 'Cross-Modal Adapter: Parameter-Efficient Transfer Learning Approach for\\n  Vision-Language Models', 'Bridging Vision and Language Spaces with Assignment Prediction', 'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models'}\n",
      "For the paper Cognitive Architectures for Language Agents  we have the following recommendation:  {'Analyzing the Roles of Language and Vision in Learning from Limited Data', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'AgentKit: Flow Engineering with Graphs, not Coding', 'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'On the Empirical Complexity of Reasoning and Planning in LLMs'}\n",
      "For the paper RAIN: Your Language Models Can Align Themselves without Finetuning  we have the following recommendation:  {'Stepwise Alignment for Constrained Language Model Policy Optimization', 'Learn Your Reference Model for Real Good Alignment', 'Aligning language models with human preferences', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'Towards Logically Consistent Language Models via Probabilistic Reasoning', 'Filtered Direct Preference Optimization', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper RWKV: Reinventing RNNs for the Transformer Era  we have the following recommendation:  {'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory', 'SpaceByte: Towards Deleting Tokenization from Large Language Modeling', 'DiJiang: Efficient Large Language Models through Compact Kernelization', 'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory'}\n",
      "For the paper Large Language Model for Science: A Study on P vs. NP  we have the following recommendation:  {'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Self-playing Adversarial Language Game Enhances LLM Reasoning', 'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models', 'CT-Agent: Clinical Trial Multi-Agent with Large Language Model-based\\n  Reasoning', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction', 'On the Empirical Complexity of Reasoning and Planning in LLMs', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity'}\n",
      "For the paper When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale  we have the following recommendation:  {'Stronger Random Baselines for In-Context Learning', 'CroissantLLM: A Truly Bilingual French-English Language Model', 'FedEval-LLM: Federated Evaluation of Large Language Models on Downstream\\n  Tasks with Collective Wisdom', 'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees', 'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages', 'Rethinking LLM Memorization through the Lens of Adversarial Compression', 'Pre-training Small Base LMs with Fewer Tokens', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper Agents: An Open-source Framework for Autonomous Language Agents  we have the following recommendation:  {'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\\n  Prediction', 'AgentKit: Flow Engineering with Graphs, not Coding', 'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models'}\n",
      "For the paper Test-Time Training with Self-Supervision for Generalization under Distribution Shifts  we have the following recommendation:  {'Mining Invariance from Nonlinear Multi-Environment Data: Binary\\n  Classification', 'NC-TTT: A Noise Contrastive Approach for Test-Time Training', 'Distilled Datamodel with Reverse Gradient Matching', 'Gradient-Regularized Out-of-Distribution Detection', 'OTTER: Improving Zero-Shot Classification via Optimal Transport', 'Toward a Realistic Benchmark for Out-of-Distribution Detection', 'A Survey of Deep Long-Tail Classification Advancements', 'Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer'}\n",
      "For the paper The Rise and Potential of Large Language Model Based Agents: A Survey  we have the following recommendation:  {'Analyzing the Roles of Language and Vision in Learning from Limited Data', 'Stepwise Alignment for Constrained Language Model Policy Optimization', 'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'AgentKit: Flow Engineering with Graphs, not Coding', 'Self-playing Adversarial Language Game Enhances LLM Reasoning', 'Aligning language models with human preferences', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Aligning LLM Agents by Learning Latent Preference from User Edits'}\n",
      "For the paper Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers  we have the following recommendation:  {'Many-Shot In-Context Learning', 'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models', 'Language Model Prompt Selection via Simulation Optimization'}\n",
      "For the paper Textbooks Are All You Need II: phi-1.5 technical report  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction', 'On the Empirical Complexity of Reasoning and Planning in LLMs', 'Rethinking LLM Memorization through the Lens of Adversarial Compression', 'Transformers Can Represent $n$-gram Language Models', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper PDFTriage: Question Answering over Long, Structured Documents  we have the following recommendation:  {'Latxa: An Open Language Model and Evaluation Suite for Basque', 'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models', 'Retrieval Augmented Generation for Domain-specific Question Answering', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'LongEmbed: Extending Embedding Models for Long Context Retrieval', 'TextSquare: Scaling up Text-Centric Visual Instruction Tuning', 'Towards a Robust Retrieval-Based Summarization System', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity'}\n",
      "For the paper Training Convolutional Networks with Noisy Labels  we have the following recommendation:  {'NC-TTT: A Noise Contrastive Approach for Test-Time Training', 'Gradient-Regularized Out-of-Distribution Detection', 'OTTER: Improving Zero-Shot Classification via Optimal Transport', 'Can We Break Free from Strong Data Augmentations in Self-Supervised\\n  Learning?', 'DeNetDM: Debiasing by Network Depth Modulation', 'Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World', 'Toward a Realistic Benchmark for Out-of-Distribution Detection', 'Noise contrastive estimation with soft targets for conditional models'}\n",
      "For the paper Understanding and Utilizing Deep Neural Networks Trained with Noisy Labels  we have the following recommendation:  {'NC-TTT: A Noise Contrastive Approach for Test-Time Training', 'Trusted Multi-view Learning with Label Noise', 'Positive Unlabeled Contrastive Learning', 'Optimizing Calibration by Gaining Aware of Prediction Correctness', 'Can We Break Free from Strong Data Augmentations in Self-Supervised\\n  Learning?', 'DeNetDM: Debiasing by Network Depth Modulation', 'Reactive Model Correction: Mitigating Harm to Task-Relevant Features via\\n  Conditional Bias Suppression', 'Noise contrastive estimation with soft targets for conditional models'}\n",
      "For the paper Generating Images with Multimodal Language Models  we have the following recommendation:  {'iRAG: An Incremental Retrieval Augmented Generation System for Videos', 'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality', 'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models', 'Residual-based Language Models are Free Boosters for Biomedical Imaging', 'Compositional Chain-of-Thought Prompting for Large Multimodal Models', 'Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept\\n  Understanding', 'TextSquare: Scaling up Text-Centric Visual Instruction Tuning', 'Cross-Modal Adapter: Parameter-Efficient Transfer Learning Approach for\\n  Vision-Language Models', 'Bridging Vision and Language Spaces with Assignment Prediction', 'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models'}\n",
      "For the paper Selective Mixup Helps with Distribution Shifts, But Not (Only) because of Mixup  we have the following recommendation:  {'NC-TTT: A Noise Contrastive Approach for Test-Time Training', 'Gradient-Regularized Out-of-Distribution Detection', 'OTTER: Improving Zero-Shot Classification via Optimal Transport', 'DeNetDM: Debiasing by Network Depth Modulation', 'A Survey of Deep Long-Tail Classification Advancements', 'Mining Invariance from Nonlinear Multi-Environment Data: Binary\\n  Classification', 'Improving Generalization via Meta-Learning on Hard Samples', 'Noise contrastive estimation with soft targets for conditional models'}\n",
      "For the paper The Future of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence  we have the following recommendation:  {'Explainable Artificial Intelligence Techniques for Accurate Fault\\n  Detection and Diagnosis: A Review', 'Decoding AI: The inside story of data analysis in ChatGPT', 'A Self-feedback Knowledge Elicitation Approach for Chemical Reaction\\n  Predictions', 'Learning to Solve the Constrained Most Probable Explanation Task in\\n  Probabilistic Graphical Models', 'LLM-Enhanced Causal Discovery in Temporal Domain from Interventional\\n  Data', 'FMint: Bridging Human Designed and Data Pretrained Models for\\n  Differential Equation Foundation Model', 'The Causal Chambers: Real Physical Systems as a Testbed for AI\\n  Methodology', 'Sparse Explanations of Neural Networks Using Pruned Layer-Wise Relevance\\n  Propagation', 'Mol-AIR: Molecular Reinforcement Learning with Adaptive Intrinsic Rewards for Goal-directed Molecular Generation', 'Enhancing Counterfactual Explanation Search with Diffusion Distance and\\n  Directional Coherence'}\n",
      "For the paper Auto-Regressive Next-Token Predictors are Universal Learners  we have the following recommendation:  {'Toward a Theory of Tokenization in LLMs', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'SpaceByte: Towards Deleting Tokenization from Large Language Modeling', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'On the Empirical Complexity of Reasoning and Planning in LLMs', 'Rethinking LLM Memorization through the Lens of Adversarial Compression', 'Transformers Can Represent $n$-gram Language Models', 'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory'}\n",
      "For the paper Neural Graphical Models  we have the following recommendation:  {'Graph Neural Aggregation-diffusion with Metastability', 'Improving the interpretability of GNN predictions through\\n  conformal-based graph sparsification', 'Graph Machine Learning in the Era of Large Language Models (LLMs)', 'Time-aware Heterogeneous Graph Transformer with Adaptive Attention\\n  Merging for Health Event Prediction', 'Graph Neural Networks for Protein-Protein Interactions - A Short Survey', 'A Review of Graph Neural Networks in Epidemic Modeling', 'Uncertainty Quantification on Graph Learning: A Survey', 'Multi-View Subgraph Neural Networks: Self-Supervised Learning with\\n  Scarce Labeled Data'}\n",
      "For the paper TSMixer: An All-MLP Architecture for Time Series Forecasting  we have the following recommendation:  {'Variational quantization for state space models', 'Feature-Based Echo-State Networks: A Step Towards Interpretability and Minimalism in Reservoir Computer', 'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods', 'Computationally and Memory-Efficient Robust Predictive Analytics Using Big Data', 'DeepHeteroIoT: Deep Local and Global Learning over Heterogeneous IoT Sensor Data', 'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting', 'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion', 'TSLANet: Rethinking Transformers for Time Series Representation Learning', 'Generating Synthetic Time Series Data for Cyber-Physical Systems', 'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection'}\n",
      "For the paper Physics of Language Models: Part 1, Context-Free Grammar  we have the following recommendation:  {'Toward a Theory of Tokenization in LLMs', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Transformers Can Represent $n$-gram Language Models', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'What do Transformers Know about Government?', 'Pre-training Small Base LMs with Fewer Tokens', 'σ-GPTs: A New Approach to Autoregressive Models', 'Localizing Paragraph Memorization in Language Models', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper Directly Fine-Tuning Diffusion Models on Differentiable Rewards  we have the following recommendation:  {'Balanced Mixed-Type Tabular Data Synthesis with Diffusion Models', 'SparseDM: Toward Sparse Efficient Diffusion Models', 'EdgeFusion: On-Device Text-to-Image Generation', 'Gradient Guidance for Diffusion Models: An Optimization Perspective', 'Dataset Reset Policy Optimization for RLHF', 'Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept\\n  Understanding', 'TMPQ-DM: Joint Timestep Reduction and Quantization Precision Selection\\n  for Efficient Diffusion Models', 'Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging\\n  Perturbations That Efficiently Fool Customized Diffusion Models', 'Filtered Direct Preference Optimization', 'Neural Flow Diffusion Models: Learnable Forward Process for Improved\\n  Diffusion Modelling'}\n",
      "For the paper The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)  we have the following recommendation:  {'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality', 'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models', 'Pre-trained Vision-Language Models Learn Discoverable Visual Concepts', 'Self-Supervised Visual Preference Alignment', 'Harnessing GPT-4V(ision) for Insurance: A Preliminary Exploration', 'Compositional Chain-of-Thought Prompting for Large Multimodal Models', 'TextSquare: Scaling up Text-Centric Visual Instruction Tuning', 'Bridging Vision and Language Spaces with Assignment Prediction', 'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models', 'Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models'}\n",
      "For the paper Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Self-playing Adversarial Language Game Enhances LLM Reasoning', 'Aligning language models with human preferences', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity'}\n",
      "For the paper Hypothesis Search: Inductive Reasoning with Language Models  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Pre-trained Vision-Language Models Learn Discoverable Visual Concepts', 'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models', 'Compositional Chain-of-Thought Prompting for Large Multimodal Models', 'TextSquare: Scaling up Text-Centric Visual Instruction Tuning', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'Towards Logically Consistent Language Models via Probabilistic Reasoning', 'On the Empirical Complexity of Reasoning and Planning in LLMs', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity'}\n",
      "For the paper Representation Engineering: A Top-Down Approach to AI Transparency  we have the following recommendation:  {'Holistic Safety and Responsibility Evaluations of Advanced AI Models', 'Self-playing Adversarial Language Game Enhances LLM Reasoning', 'GenFighter: A Generative and Evolutive Textual Attack Removal', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model', 'Closing the Gap in the Trade-off between Fair Representations and\\n  Accuracy', 'RTP-LX: Can LLMs Evaluate Toxicity in Multilingual Scenarios?', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Decomposing and Editing Predictions by Modeling Model Computation', 'Rethinking LLM Memorization through the Lens of Adversarial Compression', 'Private Attribute Inference from Images with Vision-Language Models'}\n",
      "For the paper Can large language models provide useful feedback on research papers? A large-scale empirical analysis  we have the following recommendation:  {'Learning representations of learning representations', 'STRUM-LLM: Attributed and Structured Contrastive Summarization', 'Bias patterns in the application of LLMs for clinical decision support:\\n  A comprehensive study', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'FedEval-LLM: Federated Evaluation of Large Language Models on Downstream\\n  Tasks with Collective Wisdom', 'CT-Agent: Clinical Trial Multi-Agent with Large Language Model-based\\n  Reasoning', \"Is ChatGPT Transforming Academics' Writing Style?\", 'Beyond Code Generation: An Observational Study of ChatGPT Usage in\\n  Software Engineering Practice'}\n",
      "For the paper Borges and AI  we have the following recommendation:  {'Analyzing the Roles of Language and Vision in Learning from Limited Data', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'MUGC: Machine Generated versus User Generated Content Detection', \"On Large Language Models' Hallucination with Regard to Known Facts\", 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models', 'Taxonomy to Regulation: A (Geo)Political Taxonomy for AI Risks and\\n  Regulatory Measures in the EU AI Act', 'Transformers Can Represent $n$-gram Language Models'}\n",
      "For the paper Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation  we have the following recommendation:  {'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'AgentKit: Flow Engineering with Graphs, not Coding', 'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation', 'Automating REST API Postman Test Cases Using LLM', 'LLM-Powered Test Case Generation for Detecting Tricky Bugs', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'NExT: Teaching Large Language Models to Reason about Code Execution', 'XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\\n  Upcycled Mixture-of-Experts'}\n",
      "For the paper Deep Neural Networks Tend To Extrapolate Predictably  we have the following recommendation:  {'Combining Statistical Depth and Fermat Distance for Uncertainty\\n  Quantification', 'Gradient-Regularized Out-of-Distribution Detection', 'Strong Transferable Adversarial Attacks via Ensembled Asymptotically Normal Distribution Learning', 'A Survey of Deep Long-Tail Classification Advancements', 'Reactive Model Correction: Mitigating Harm to Task-Relevant Features via\\n  Conditional Bias Suppression', 'Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World', 'Toward a Realistic Benchmark for Out-of-Distribution Detection', 'Noise contrastive estimation with soft targets for conditional models'}\n",
      "For the paper How Two-Layer Neural Networks Learn, One (Giant) Step at a Time  we have the following recommendation:  {'Training neural networks with structured noise improves classification and generalization', 'Deep Neural Networks via Complex Network Theory: a Perspective', 'Strong Transferable Adversarial Attacks via Ensembled Asymptotically Normal Distribution Learning', 'How Deep Networks Learn Sparse and Hierarchical Data: the Sparse Random\\n  Hierarchy Model', 'Matching the Statistical Query Lower Bound for k-sparse Parity Problems\\n  with Stochastic Gradient Descent', 'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning', 'Sliding down the stairs: how correlated latent variables accelerate\\n  learning with neural networks', 'Noise contrastive estimation with soft targets for conditional models'}\n",
      "For the paper Explaining grokking through circuit efficiency  we have the following recommendation:  {'Training neural networks with structured noise improves classification and generalization', 'Deep Neural Networks via Complex Network Theory: a Perspective', 'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective', 'Deep Regression Representation Learning with Topology', 'Delayed Bottlenecking: Alleviating Forgetting in Pre-trained Graph\\n  Neural Networks', 'Can We Break Free from Strong Data Augmentations in Self-Supervised\\n  Learning?', 'Rethinking LLM Memorization through the Lens of Adversarial Compression', 'Mining Invariance from Nonlinear Multi-Environment Data: Binary\\n  Classification', 'Localizing Paragraph Memorization in Language Models', 'On Size and Hardness Generalization in Unsupervised Learning for the Travelling Salesman Problem'}\n",
      "For the paper MAPTree: Beating \"Optimal\" Decision Trees with Bayesian Decision Trees  we have the following recommendation:  {'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning', 'Finding Decision Tree Splits in Streaming and Massively Parallel Models', 'floZ: Evidence estimation from posterior samples with normalizing flows', 'BayesJudge: Bayesian Kernel Language Modelling with Confidence\\n  Uncertainty in Legal Judgment Prediction', 'Explainable LightGBM Approach for Predicting Myocardial Infarction\\n  Mortality', 'Application of the representative measure approach to assess the\\n  reliability of decision trees in dealing with unseen vehicle collision data', 'AI Competitions and Benchmarks: Dataset Development', 'Tree Bandits for Generative Bayes'}\n",
      "For the paper Neuro-Modulated Hebbian Learning for Fully Test-Time Adaptation  we have the following recommendation:  {'NC-TTT: A Noise Contrastive Approach for Test-Time Training', 'Employing Layerwised Unsupervised Learning to Lessen Data and Loss\\n  Requirements in Forward-Forward Algorithms', 'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective', 'Actor-Critic Reinforcement Learning with Phased Actor', 'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning', 'Reactive Model Correction: Mitigating Harm to Task-Relevant Features via\\n  Conditional Bias Suppression', 'Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models', 'ColA: Collaborative Adaptation with Gradient Learning'}\n",
      "For the paper DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines  we have the following recommendation:  {'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\\n  Prediction', 'AgentKit: Flow Engineering with Graphs, not Coding', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models'}\n",
      "For the paper Large Language Models can Learn Rules  we have the following recommendation:  {'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning', 'Many-Shot In-Context Learning', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Retrieval Augmented Generation for Domain-specific Question Answering', \"On Large Language Models' Hallucination with Regard to Known Facts\", 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction', 'Towards Logically Consistent Language Models via Probabilistic Reasoning', 'On the Empirical Complexity of Reasoning and Planning in LLMs', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity'}\n",
      "For the paper MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens  we have the following recommendation:  {'iRAG: An Incremental Retrieval Augmented Generation System for Videos', 'Data-Efficient Multimodal Fusion on a Single GPU', 'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality', 'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models', 'Compositional Chain-of-Thought Prompting for Large Multimodal Models', 'TextSquare: Scaling up Text-Centric Visual Instruction Tuning', 'Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept\\n  Understanding', 'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models'}\n",
      "For the paper VeRA: Vector-based Random Matrix Adaptation  we have the following recommendation:  {'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium', 'Pre-training Small Base LMs with Fewer Tokens', 'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models', 'CroissantLLM: A Truly Bilingual French-English Language Model', 'Personalized Collaborative Fine-Tuning for On-Device Large Language\\n  Models', 'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning', 'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages', 'LoRA Dropout as a Sparsity Regularizer for Overfitting Control', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models'}\n",
      "For the paper Context-Aware Meta-Learning  we have the following recommendation:  {'Many-Shot In-Context Learning', 'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality', 'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models', 'RankCLIP: Ranking-Consistent Language-Image Pretraining', 'Compositional Chain-of-Thought Prompting for Large Multimodal Models', 'TextSquare: Scaling up Text-Centric Visual Instruction Tuning', 'Cross-Modal Adapter: Parameter-Efficient Transfer Learning Approach for\\n  Vision-Language Models', 'Bridging Vision and Language Spaces with Assignment Prediction', 'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models', 'Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models'}\n",
      "For the paper In-Context Learning for Few-Shot Molecular Property Prediction  we have the following recommendation:  {'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning', 'Many-Shot In-Context Learning', 'When LLMs are Unfit Use FastFit: Fast and Effective Text Classification\\n  with Many Classes', 'A Self-feedback Knowledge Elicitation Approach for Chemical Reaction\\n  Predictions', 'A Python library for efficient computation of molecular fingerprints', 'HiGraphDTI: Hierarchical Graph Representation Learning for Drug-Target\\n  Interaction Prediction', '$\\\\texttt{MiniMol}$: A Parameter-Efficient Foundation Model for Molecular\\n  Learning', 'Simple, Efficient and Scalable Structure-aware Adapter Boosts Protein\\n  Language Models'}\n",
      "For the paper Reset It and Forget It: Relearning Last-Layer Weights Improves Continual and Transfer Learning  we have the following recommendation:  {'SOPHON: Non-Fine-Tunable Learning to Restrain Task Transferability For\\n  Pre-trained Models', 'NC-TTT: A Noise Contrastive Approach for Test-Time Training', 'Improving Generalization via Meta-Learning on Hard Samples', 'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective', 'OTTER: Improving Zero-Shot Classification via Optimal Transport', 'Can We Break Free from Strong Data Augmentations in Self-Supervised\\n  Learning?', 'Dynamically Anchored Prompting for Task-Imbalanced Continual Learning', 'Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer', 'Continual Learning on a Diet: Learning from Sparsely Labeled Streams\\n  Under Constrained Computation', 'ColA: Collaborative Adaptation with Gradient Learning'}\n",
      "For the paper Few-Shot Learning Patterns in Financial Time-Series for Trend-Following Strategies  we have the following recommendation:  {'A Comparison of Traditional and Deep Learning Methods for Parameter\\n  Estimation of the Ornstein-Uhlenbeck Process', 'Variational quantization for state space models', 'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods', 'DeepHeteroIoT: Deep Local and Global Learning over Heterogeneous IoT Sensor Data', 'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting', 'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion', 'TSLANet: Rethinking Transformers for Time Series Representation Learning', 'Generating Synthetic Time Series Data for Cyber-Physical Systems', 'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection', 'Reinforcement Learning in Agent-Based Market Simulation: Unveiling Realistic Stylized Facts and Behavior'}\n",
      "For the paper Pseudointelligence: A Unifying Framework for Language Model Evaluation  we have the following recommendation:  {'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Stronger Random Baselines for In-Context Learning', 'FedEval-LLM: Federated Evaluation of Large Language Models on Downstream\\n  Tasks with Collective Wisdom', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction', 'Rethinking LLM Memorization through the Lens of Adversarial Compression', 'Transformers Can Represent $n$-gram Language Models', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper Retrieving Texts based on Abstract Descriptions  we have the following recommendation:  {'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models', 'Retrieval Augmented Generation for Domain-specific Question Answering', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking', 'LongEmbed: Extending Embedding Models for Long Context Retrieval', 'Conversational Financial Information Retrieval Model (ConFIRM)', 'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees'}\n",
      "For the paper Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence  we have the following recommendation:  {'Toward a Theory of Tokenization in LLMs', 'Talk Too Much: Poisoning Large Language Models under Token Limit', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'GenFighter: A Generative and Evolutive Textual Attack Removal', 'SpaceByte: Towards Deleting Tokenization from Large Language Modeling', 'LongEmbed: Extending Embedding Models for Long Context Retrieval', 'Rethinking LLM Memorization through the Lens of Adversarial Compression', 'Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging\\n  Perturbations That Efficiently Fool Customized Diffusion Models'}\n",
      "For the paper Attention over pre-trained Sentence Embeddings for Long Document Classification  we have the following recommendation:  {'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models', 'SpaceByte: Towards Deleting Tokenization from Large Language Modeling', 'LayerNorm: A key component in parameter-efficient fine-tuning', 'LongEmbed: Extending Embedding Models for Long Context Retrieval', 'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey', 'DiJiang: Efficient Large Language Models through Compact Kernelization', 'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory'}\n",
      "For the paper The Consensus Game: Language Model Generation via Equilibrium Search  we have the following recommendation:  {'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity', 'Retrieval Augmented Generation for Domain-specific Question Answering', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'Aligning language models with human preferences', 'Towards Logically Consistent Language Models via Probabilistic Reasoning', 'Towards a Robust Retrieval-Based Summarization System', 'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper Branch-Solve-Merge Improves Large Language Model Evaluation and Generation  we have the following recommendation:  {'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level\\n  Knowledge Distillation', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'Towards a Robust Retrieval-Based Summarization System', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'Towards Logically Consistent Language Models via Probabilistic Reasoning', 'Language Model Prompt Selection via Simulation Optimization'}\n",
      "For the paper What Algorithms can Transformers Learn? A Study in Length Generalization  we have the following recommendation:  {'Toward a Theory of Tokenization in LLMs', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Transformers Can Represent $n$-gram Language Models', 'Pre-training Small Base LMs with Fewer Tokens', 'What do Transformers Know about Government?', 'Rethinking LLM Memorization through the Lens of Adversarial Compression', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory'}\n",
      "For the paper In-Context Learning Creates Task Vectors  we have the following recommendation:  {'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning', 'Many-Shot In-Context Learning', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Stronger Random Baselines for In-Context Learning', 'Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy\\n  Data', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees', 'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction', 'Jamba: A Hybrid Transformer-Mamba Language Model'}\n",
      "For the paper PaRaDe: Passage Ranking using Demonstrations with Large Language Models  we have the following recommendation:  {'Many-Shot In-Context Learning', 'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Stronger Random Baselines for In-Context Learning', 'Retrieval Augmented Generation for Domain-specific Question Answering', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'Towards a Robust Retrieval-Based Summarization System', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'Language Model Prompt Selection via Simulation Optimization'}\n",
      "For the paper FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions  we have the following recommendation:  {'Many-Shot In-Context Learning', 'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model', \"On Large Language Models' Hallucination with Regard to Known Facts\", 'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models', 'TextSquare: Scaling up Text-Centric Visual Instruction Tuning', 'Towards Logically Consistent Language Models via Probabilistic Reasoning', 'On the Empirical Complexity of Reasoning and Planning in LLMs', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity'}\n",
      "For the paper PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents  we have the following recommendation:  {'Many-Shot In-Context Learning', 'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Retrieval Augmented Generation for Domain-specific Question Answering', 'Compositional Chain-of-Thought Prompting for Large Multimodal Models', 'Towards a Robust Retrieval-Based Summarization System', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models', 'On the Empirical Complexity of Reasoning and Planning in LLMs', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity'}\n",
      "For the paper Papeos: Augmenting Research Papers with Talk Videos  we have the following recommendation:  {'iRAG: An Incremental Retrieval Augmented Generation System for Videos', 'iTBLS: A Dataset of Interactive Conversations Over Tabular Information', 'STRUM-LLM: Attributed and Structured Contrastive Summarization', 'AgentKit: Flow Engineering with Graphs, not Coding', 'Exploring Text-to-Motion Generation with Human Preference', 'SuperNOVA: Design Strategies and Opportunities for Interactive Visualization in Computational Notebooks', \"Is ChatGPT Transforming Academics' Writing Style?\", 'Beyond Code Generation: An Observational Study of ChatGPT Usage in\\n  Software Engineering Practice'}\n",
      "For the paper Codebook Features: Sparse and Discrete Interpretability for Neural Networks  we have the following recommendation:  {'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium', 'Tripod: Three Complementary Inductive Biases for Disentangled\\n  Representation Learning', 'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory', 'SpaceByte: Towards Deleting Tokenization from Large Language Modeling', 'Neuro-Inspired Information-Theoretic Hierarchical Perception for\\n  Multimodal Learning', 'Rethinking LLM Memorization through the Lens of Adversarial Compression', 'Pre-training Small Base LMs with Fewer Tokens', 'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory'}\n",
      "For the paper How do Language Models Bind Entities in Context?  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning', 'Analyzing the Roles of Language and Vision in Learning from Limited Data', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Transformers Can Represent $n$-gram Language Models', 'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction', 'Semantic Cells: Evolutional Process to Acquire Sense Diversity of Items', 'On the Empirical Complexity of Reasoning and Planning in LLMs', 'Localizing Paragraph Memorization in Language Models', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity'}\n",
      "For the paper CodeFusion: A Pre-trained Diffusion Model for Code Generation  we have the following recommendation:  {'Automated Multi-Language to English Machine Translation Using Generative\\n  Pre-Trained Transformers', 'Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level\\n  Knowledge Distillation', 'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation', 'CroissantLLM: A Truly Bilingual French-English Language Model', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'NExT: Teaching Large Language Models to Reason about Code Execution', 'XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\\n  Upcycled Mixture-of-Experts', 'Pre-training Small Base LMs with Fewer Tokens', 'σ-GPTs: A New Approach to Autoregressive Models'}\n",
      "For the paper AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation  we have the following recommendation:  {'iTBLS: A Dataset of Interactive Conversations Over Tabular Information', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'AgentKit: Flow Engineering with Graphs, not Coding', 'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation', 'Automating REST API Postman Test Cases Using LLM', 'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'Beyond Code Generation: An Observational Study of ChatGPT Usage in\\n  Software Engineering Practice', 'ReALM: Reference Resolution As Language Modeling'}\n",
      "For the paper Universal Self-Adaptive Prompting  we have the following recommendation:  {'Many-Shot In-Context Learning', 'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models', 'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Stronger Random Baselines for In-Context Learning', 'Retrieval Augmented Generation for Domain-specific Question Answering', 'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing'}\n",
      "For the paper Beyond Words: A Mathematical Framework for Interpreting Large Language Models  we have the following recommendation:  {'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation', 'Detecting and Mitigating Hallucination in Large Vision Language Models\\n  via Fine-Grained AI Feedback', \"On Large Language Models' Hallucination with Regard to Known Facts\", 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'Towards Logically Consistent Language Models via Probabilistic Reasoning', 'On the Empirical Complexity of Reasoning and Planning in LLMs', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity'}\n",
      "For the paper GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling  we have the following recommendation:  {'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models', 'TriForce: Lossless Acceleration of Long Sequence Generation with\\n  Hierarchical Speculative Decoding', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models', 'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey', 'DiJiang: Efficient Large Language Models through Compact Kernelization', 'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory'}\n",
      "For the paper Unraveling the ARC Puzzle: Mimicking Human Solutions with Object-Centric Decision Transformer  we have the following recommendation:  {'Analyzing the Roles of Language and Vision in Learning from Limited Data', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Decoding AI: The inside story of data analysis in ChatGPT', 'Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V', 'ASID: Active Exploration for System Identification in Robotic\\n  Manipulation', 'From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function'}\n",
      "For the paper LILO: Learning Interpretable Libraries by Compressing and Documenting Code  we have the following recommendation:  {'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium', 'Automated Multi-Language to English Machine Translation Using Generative\\n  Pre-Trained Transformers', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'NExT: Teaching Large Language Models to Reason about Code Execution', 'XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\\n  Upcycled Mixture-of-Experts', 'Pre-training Small Base LMs with Fewer Tokens'}\n",
      "For the paper Meta-learning of semi-supervised learning from tasks with heterogeneous attribute spaces  we have the following recommendation:  {'DeNetDM: Debiasing by Network Depth Modulation', 'Meta-Learning with Generalized Ridge Regression: High-dimensional Asymptotics, Optimality and Hyper-covariance Estimation', 'Beyond the Known: Novel Class Discovery for Open-world Graph Learning', 'OTTER: Improving Zero-Shot Classification via Optimal Transport', 'VCC-INFUSE: Towards Accurate and Efficient Selection of Unlabeled\\n  Examples in Semi-supervised Learning', 'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning', 'Integration of Self-Supervised BYOL in Semi-Supervised Medical Image\\n  Recognition', 'Improving Generalization via Meta-Learning on Hard Samples'}\n",
      "For the paper Dense Passage Retrieval for Open-Domain Question Answering  we have the following recommendation:  {'Latxa: An Open Language Model and Evaluation Suite for Basque', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity', 'Retrieval Augmented Generation for Domain-specific Question Answering', 'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking', 'LongEmbed: Extending Embedding Models for Long Context Retrieval', 'Small Models Are (Still) Effective Cross-Domain Argument Extractors', 'QAGCN: Answering Multi-Relation Questions via Single-Step Implicit Reasoning over Knowledge Graphs', 'Find The Gap: Knowledge Base Reasoning For Visual Question Answering'}\n",
      "For the paper Agent Lumos: Unified and Modular Training for Open-Source Language Agents  we have the following recommendation:  {'Inferring Behavior-Specific Context Improves Zero-Shot Generalization in\\n  Reinforcement Learning', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality', 'AgentKit: Flow Engineering with Graphs, not Coding', 'Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'TextSquare: Scaling up Text-Centric Visual Instruction Tuning', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'Towards Logically Consistent Language Models via Probabilistic Reasoning', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper Evaluation of Neural Architectures Trained with Square Loss vs Cross-Entropy in Classification Tasks  we have the following recommendation:  {'Toward a Theory of Tokenization in LLMs', 'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium', 'NC-TTT: A Noise Contrastive Approach for Test-Time Training', 'Nonlinearity Enhanced Adaptive Activation Function', 'SpaceByte: Towards Deleting Tokenization from Large Language Modeling', 'Next Generation Loss Function for Image Classification', 'Noise contrastive estimation with soft targets for conditional models', 'Neuro-Inspired Information-Theoretic Hierarchical Perception for\\n  Multimodal Learning', 'Pre-training Small Base LMs with Fewer Tokens', 'PQA: Exploring the Potential of Product Quantization in DNN Hardware Acceleration'}\n",
      "For the paper The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4  we have the following recommendation:  {'Towards a Foundation Model for Partial Differential Equation:\\n  Multi-Operator Learning and Extrapolation', 'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models', 'AlloyBERT: Alloy Property Prediction with Large Language Models', 'A Self-feedback Knowledge Elicitation Approach for Chemical Reaction\\n  Predictions', 'CT-Agent: Clinical Trial Multi-Agent with Large Language Model-based\\n  Reasoning', 'Simple, Efficient and Scalable Structure-aware Adapter Boosts Protein\\n  Language Models', 'On the Scalability of GNNs for Molecular Graphs', 'Jamba: A Hybrid Transformer-Mamba Language Model'}\n",
      "For the paper Cappy: Outperforming and Boosting Large Multi-Task LMs with a Small Scorer  we have the following recommendation:  {'Many-Shot In-Context Learning', 'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium', 'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning', 'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models', 'CroissantLLM: A Truly Bilingual French-English Language Model', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models'}\n",
      "For the paper Ghostbuster: Detecting Text Ghostwritten by Large Language Models  we have the following recommendation:  {'Learning representations of learning representations', 'Talk Too Much: Poisoning Large Language Models under Token Limit', 'Does It Make Sense to Explain a Black Box With Another Black Box?', 'ParaFusion: A Large-Scale LLM-Driven English Paraphrase Dataset Infused\\n  with High-Quality Lexical and Syntactic Diversity', 'MUGC: Machine Generated versus User Generated Content Detection', 'GenFighter: A Generative and Evolutive Textual Attack Removal', 'AI-Generated Faces in the Real World: A Large-Scale Case Study of\\n  Twitter Profile Images', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'Robust CLIP-Based Detector for Exposing Diffusion Model-Generated Images', 'Rethinking LLM Memorization through the Lens of Adversarial Compression'}\n",
      "For the paper Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality', 'Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V', 'Compositional Chain-of-Thought Prompting for Large Multimodal Models', 'TextSquare: Scaling up Text-Centric Visual Instruction Tuning', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'On the Empirical Complexity of Reasoning and Planning in LLMs', 'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity'}\n",
      "For the paper TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second  we have the following recommendation:  {'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning', 'Distilled Datamodel with Reverse Gradient Matching', 'OTTER: Improving Zero-Shot Classification via Optimal Transport', 'Learning to Solve the Constrained Most Probable Explanation Task in\\n  Probabilistic Graphical Models', 'Measuring Feature Dependency of Neural Networks by Collapsing Feature\\n  Dimensions in the Data Manifold', 'Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science', 'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning', 'Decomposing and Editing Predictions by Modeling Model Computation'}\n",
      "For the paper When Do Neural Nets Outperform Boosted Trees on Tabular Data?  we have the following recommendation:  {'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning', 'Distilled Datamodel with Reverse Gradient Matching', 'AI Competitions and Benchmarks: Dataset Development', 'Structured Evaluation of Synthetic Tabular Data', 'Towards Stable Machine Learning Model Retraining via Slowly Varying Sequences', 'Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science', 'Application of the representative measure approach to assess the\\n  reliability of decision trees in dealing with unseen vehicle collision data', 'DeNetDM: Debiasing by Network Depth Modulation', 'A Survey of Deep Long-Tail Classification Advancements', 'Improving Generalization via Meta-Learning on Hard Samples'}\n",
      "For the paper Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models  we have the following recommendation:  {'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'AgentKit: Flow Engineering with Graphs, not Coding', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models', 'On the Empirical Complexity of Reasoning and Planning in LLMs', 'From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function'}\n",
      "For the paper Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as an Alternative to Attention Layers in Transformers  we have the following recommendation:  {'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory', 'Enhancing Efficiency in Vision Transformer Networks: Design Techniques and Insights', 'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models', 'GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers', 'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection', 'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey', 'DiJiang: Efficient Large Language Models through Compact Kernelization', 'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory'}\n",
      "For the paper System 2 Attention (is something you might need too)  we have the following recommendation:  {'Many-Shot In-Context Learning', 'DiJiang: Efficient Large Language Models through Compact Kernelization', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models', 'Towards Logically Consistent Language Models via Probabilistic Reasoning', 'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity'}\n",
      "For the paper RA-DIT: Retrieval-Augmented Dual Instruction Tuning  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Retrieval Augmented Generation for Domain-specific Question Answering', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking', 'LongEmbed: Extending Embedding Models for Long Context Retrieval', 'CroissantLLM: A Truly Bilingual French-English Language Model', 'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees'}\n",
      "For the paper MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning  we have the following recommendation:  {'Would You Trust an AI Doctor? Building Reliable Medical Predictions with\\n  Kernel Dropout Uncertainty', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Bias patterns in the application of LLMs for clinical decision support:\\n  A comprehensive study', 'Data Alignment for Zero-Shot Concept Generation in Dermatology AI', 'FedEval-LLM: Federated Evaluation of Large Language Models on Downstream\\n  Tasks with Collective Wisdom', 'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models', 'CT-Agent: Clinical Trial Multi-Agent with Large Language Model-based\\n  Reasoning', 'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction'}\n",
      "For the paper Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models  we have the following recommendation:  {'Toward a Theory of Tokenization in LLMs', 'Many-Shot In-Context Learning', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Stronger Random Baselines for In-Context Learning', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'LayerNorm: A key component in parameter-efficient fine-tuning', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models', 'DiJiang: Efficient Large Language Models through Compact Kernelization', 'Pre-training Small Base LMs with Fewer Tokens'}\n",
      "For the paper Text Embeddings Reveal (Almost) As Much As Text  we have the following recommendation:  {'PATE-TripleGAN: Privacy-Preserving Image Synthesis with Gaussian\\n  Differential Privacy', 'SpaceByte: Towards Deleting Tokenization from Large Language Modeling', 'VFLGAN: Vertical Federated Learning-based Generative Adversarial Network\\n  for Vertically Partitioned Data Publication', 'LongEmbed: Extending Embedding Models for Long Context Retrieval', 'Distributional Black-Box Model Inversion Attack with Multi-Agent\\n  Reinforcement Learning', 'Rethinking LLM Memorization through the Lens of Adversarial Compression', 'Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging\\n  Perturbations That Efficiently Fool Customized Diffusion Models', 'Private Attribute Inference from Images with Vision-Language Models'}\n",
      "For the paper Scaling TabPFN: Sketching and Feature Selection for Tabular Prior-Data Fitted Networks  we have the following recommendation:  {'Synthesizing Realistic Data for Table Recognition', 'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning', 'An improved tabular data generator with VAE-GMM integration', 'Deep Regression Representation Learning with Topology', 'Measuring Feature Dependency of Neural Networks by Collapsing Feature\\n  Dimensions in the Data Manifold', 'Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science', 'Application of the representative measure approach to assess the\\n  reliability of decision trees in dealing with unseen vehicle collision data', 'Structured Evaluation of Synthetic Tabular Data'}\n",
      "For the paper Fine-Tuning the Retrieval Mechanism for Tabular Deep Learning  we have the following recommendation:  {'Synthesizing Realistic Data for Table Recognition', 'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning', 'Delayed Bottlenecking: Alleviating Forgetting in Pre-trained Graph\\n  Neural Networks', 'FR-NAS: Forward-and-Reverse Graph Predictor for Efficient Neural\\n  Architecture Search', 'Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science', 'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning', 'Structured Evaluation of Synthetic Tabular Data', 'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory'}\n",
      "For the paper XTab: Cross-table Pretraining for Tabular Transformers  we have the following recommendation:  {'Synthesizing Realistic Data for Table Recognition', 'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning', 'An improved tabular data generator with VAE-GMM integration', 'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning', 'Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science', 'Can We Break Free from Strong Data Augmentations in Self-Supervised\\n  Learning?', 'One-Shot Sequential Federated Learning for Non-IID Data by Enhancing\\n  Local Model Diversity', 'Structured Evaluation of Synthetic Tabular Data'}\n",
      "For the paper Deep incremental learning models for financial temporal tabular datasets with distribution shifts  we have the following recommendation:  {'Using ARIMA to Predict the Expansion of Subscriber Data Consumption', 'Variational quantization for state space models', 'Integrating Marketing Channels into Quantile Transformation and Bayesian\\n  Optimization of Ensemble Kernels for Sales Prediction with Gaussian Process\\n  Models', 'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods', 'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting', 'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion', 'Towards Stable Machine Learning Model Retraining via Slowly Varying Sequences', 'TSLANet: Rethinking Transformers for Time Series Representation Learning', 'Mining Invariance from Nonlinear Multi-Environment Data: Binary\\n  Classification', 'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection'}\n",
      "For the paper Deep Regression Ensembles  we have the following recommendation:  {'Gradient-Regularized Out-of-Distribution Detection', 'Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World', 'Measuring Feature Dependency of Neural Networks by Collapsing Feature\\n  Dimensions in the Data Manifold', 'Can We Break Free from Strong Data Augmentations in Self-Supervised\\n  Learning?', 'DeNetDM: Debiasing by Network Depth Modulation', 'Reactive Model Correction: Mitigating Harm to Task-Relevant Features via\\n  Conditional Bias Suppression'}\n",
      "For the paper War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars  we have the following recommendation:  {'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Social Choice for AI Alignment: Dealing with Diverse Human Feedback', 'Self-playing Adversarial Language Game Enhances LLM Reasoning', 'LLM-Enhanced Causal Discovery in Temporal Domain from Interventional\\n  Data', 'Taxonomy to Regulation: A (Geo)Political Taxonomy for AI Risks and\\n  Regulatory Measures in the EU AI Act', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction', 'A survey of air combat behavior modeling using machine learning', 'Using LLMs to Model the Beliefs and Preferences of Targeted Populations'}\n",
      "For the paper Mamba: Linear-Time Sequence Modeling with Selective State Spaces  we have the following recommendation:  {'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory', 'SpaceByte: Towards Deleting Tokenization from Large Language Modeling', 'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection', 'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory'}\n",
      "For the paper The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning  we have the following recommendation:  {'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Learn Your Reference Model for Real Good Alignment', 'Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy\\n  Data', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'Aligning language models with human preferences', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper SciRepEval: A Multi-Format Benchmark for Scientific Document Representations  we have the following recommendation:  {'Learning representations of learning representations', 'Many-Shot In-Context Learning', 'STRUM-LLM: Attributed and Structured Contrastive Summarization', 'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking', 'Learning Word Embedding with Better Distance Weighting and Window Size\\n  Scheduling', 'LongEmbed: Extending Embedding Models for Long Context Retrieval', 'Sparse multimodal fusion with modal channel attention', 'Simple, Efficient and Scalable Structure-aware Adapter Boosts Protein\\n  Language Models'}\n",
      "For the paper Generalization to New Sequential Decision Making Tasks with In-Context Learning  we have the following recommendation:  {'Inferring Behavior-Specific Context Improves Zero-Shot Generalization in\\n  Reinforcement Learning', 'Continual Offline Reinforcement Learning via Diffusion-based Dual\\n  Generative Replay', 'Deep Policy Optimization with Temporal Logic Constraints', 'Decision Mamba: Reinforcement Learning via Sequence Modeling with Selective State Spaces', 'Zero-Shot Stitching in Reinforcement Learning using Relative\\n  Representations', 'Higher Replay Ratio Empowers Sample-Efficient Multi-Agent Reinforcement\\n  Learning', 'Adversarial Imitation Learning via Boosting', 'Meta Reinforcement Learning with Finite Training Tasks -- a Density Estimation Approach', 'Dynamically Anchored Prompting for Task-Imbalanced Continual Learning', 'Offline Trajectory Generalization for Offline Reinforcement Learning'}\n",
      "For the paper Errors are Useful Prompts: Instruction Guided Task Programming with Verifier-Assisted Iterative Prompting  we have the following recommendation:  {'Concept-based Analysis of Neural Networks via Vision-Language Models', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'iTBLS: A Dataset of Interactive Conversations Over Tabular Information', 'AgentKit: Flow Engineering with Graphs, not Coding', 'A Self-feedback Knowledge Elicitation Approach for Chemical Reaction\\n  Predictions', 'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation', 'Automating REST API Postman Test Cases Using LLM', 'Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'NExT: Teaching Large Language Models to Reason about Code Execution'}\n",
      "For the paper AI and Jobs: Has the Inflection Point Arrived? Evidence from an Online Labor Platform  we have the following recommendation:  {'An Economic Solution to Copyright Challenges of Generative AI', 'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions', 'Fair Concurrent Training of Multiple Models in Federated Learning', 'Decoding AI: The inside story of data analysis in ChatGPT', 'LLM-Enhanced Causal Discovery in Temporal Domain from Interventional\\n  Data', 'The Impact of Machine Learning on Society: An Analysis of Current Trends\\n  and Future Implications', 'Taxonomy to Regulation: A (Geo)Political Taxonomy for AI Risks and\\n  Regulatory Measures in the EU AI Act', 'Enhancing Fairness and Performance in Machine Learning Models: A\\n  Multi-Task Learning Approach with Monte-Carlo Dropout and Pareto Optimality', 'AI Competitions and Benchmarks: Dataset Development', 'Beyond Code Generation: An Observational Study of ChatGPT Usage in\\n  Software Engineering Practice'}\n",
      "For the paper Dense X Retrieval: What Retrieval Granularity Should We Use?  we have the following recommendation:  {'Retrieval Augmented Generation for Domain-specific Question Answering', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking', 'LongEmbed: Extending Embedding Models for Long Context Retrieval', 'Small Models Are (Still) Effective Cross-Domain Argument Extractors', 'Conversational Financial Information Retrieval Model (ConFIRM)', 'Towards a Robust Retrieval-Based Summarization System', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity'}\n",
      "For the paper MotherNet: A Foundational Hypernetwork for Tabular Classification  we have the following recommendation:  {'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning', 'Distilled Datamodel with Reverse Gradient Matching', 'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium', 'Employing Layerwised Unsupervised Learning to Lessen Data and Loss\\n  Requirements in Forward-Forward Algorithms', 'Engineering software 2.0 by interpolating neural networks: unifying\\n  training, solving, and calibration', 'Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science', 'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning', 'Pre-training Small Base LMs with Fewer Tokens', 'Improving Generalization via Meta-Learning on Hard Samples', 'Noise contrastive estimation with soft targets for conditional models'}\n",
      "For the paper Weight subcloning: direct initialization of transformers using larger pretrained ones  we have the following recommendation:  {'Efficient Transformer Encoders for Mask2Former-style models', 'Pre-training Small Base LMs with Fewer Tokens', 'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models', 'SpaceByte: Towards Deleting Tokenization from Large Language Modeling', 'Residual-based Language Models are Free Boosters for Biomedical Imaging', 'LayerNorm: A key component in parameter-efficient fine-tuning', 'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning', 'DiJiang: Efficient Large Language Models through Compact Kernelization', 'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey', 'Jamba: A Hybrid Transformer-Mamba Language Model'}\n",
      "For the paper A mathematical perspective on Transformers  we have the following recommendation:  {'Toward a Theory of Tokenization in LLMs', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Transformers Can Represent $n$-gram Language Models', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory', 'DiJiang: Efficient Large Language Models through Compact Kernelization', 'What do Transformers Know about Government?', 'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'Localizing Paragraph Memorization in Language Models'}\n",
      "For the paper Perspectives on the State and Future of Deep Learning - 2023  we have the following recommendation:  {'Learning representations of learning representations', 'Explainable Artificial Intelligence Techniques for Accurate Fault\\n  Detection and Diagnosis: A Review', 'Does It Make Sense to Explain a Black Box With Another Black Box?', 'DXAI: Explaining Classification by Image Decomposition', 'MUGC: Machine Generated versus User Generated Content Detection', 'Closing the Gap in the Trade-off between Fair Representations and\\n  Accuracy', 'Toward Understanding the Disagreement Problem in Neural Network Feature\\n  Attribution', 'Gradient strikes back: How filtering out high frequencies improves explanations', 'Enhancing Counterfactual Explanation Search with Diffusion Distance and\\n  Directional Coherence', 'AI Competitions and Benchmarks: Dataset Development'}\n",
      "For the paper Retrieval-Augmented Generation for Large Language Models: A Survey  we have the following recommendation:  {'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Retrieval Augmented Generation for Domain-specific Question Answering', 'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'Towards a Robust Retrieval-Based Summarization System', 'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity'}\n",
      "For the paper Time is Encoded in the Weights of Finetuned Language Models  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Stronger Random Baselines for In-Context Learning', 'Pre-training Small Base LMs with Fewer Tokens', 'SpaceByte: Towards Deleting Tokenization from Large Language Modeling', 'CroissantLLM: A Truly Bilingual French-English Language Model', 'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees', 'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'Localizing Paragraph Memorization in Language Models'}\n",
      "For the paper Recasting Continual Learning as Sequence Modeling  we have the following recommendation:  {'Toward a Theory of Tokenization in LLMs', 'FedMeS: Personalized Federated Continual Learning Leveraging Local\\n  Memory', 'Continual Learning on a Diet: Learning from Sparsely Labeled Streams\\n  Under Constrained Computation', 'Decision Mamba: Reinforcement Learning via Sequence Modeling with Selective State Spaces', 'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective', 'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection', 'Dynamically Anchored Prompting for Task-Imbalanced Continual Learning', 'Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer', 'Improving Generalization via Meta-Learning on Hard Samples', 'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory'}\n",
      "For the paper An In-depth Look at Gemini's Language Abilities  we have the following recommendation:  {'Many-Shot In-Context Learning', 'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model', 'CroissantLLM: A Truly Bilingual French-English Language Model', 'TextSquare: Scaling up Text-Centric Visual Instruction Tuning', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity'}\n",
      "For the paper Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning  we have the following recommendation:  {'Analyzing the Roles of Language and Vision in Learning from Limited Data', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Deep Policy Optimization with Temporal Logic Constraints', 'AgentKit: Flow Engineering with Graphs, not Coding', 'Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'Towards Logically Consistent Language Models via Probabilistic Reasoning', 'On the Empirical Complexity of Reasoning and Planning in LLMs', 'From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function'}\n",
      "For the paper DBOS: A Proposal for a Data-Centric Operating System  we have the following recommendation:  {'Apodotiko: Enabling Efficient Serverless Federated Learning in\\n  Heterogeneous Environments', 'Privacy-Preserving Intrusion Detection using Convolutional Neural\\n  Networks', 'Privacy-Preserving Training-as-a-Service for On-Device Intelligence:\\n  Concept, Architectural Scheme, and Open Problems', 'AntDT: A Self-Adaptive Distributed Training Framework for Leader and\\n  Straggler Nodes', 'I/O in Machine Learning Applications on HPC Systems: A 360-degree Survey', 'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models', 'Confidential Federated Computations', 'Dynamic Frequency-Based Fingerprinting Attacks against Modern Sandbox\\n  Environments', 'FedFa: A Fully Asynchronous Training Paradigm for Federated Learning', 'End-to-End Verifiable Decentralized Federated Learning'}\n",
      "For the paper Mindstorms in Natural Language-Based Societies of Mind  we have the following recommendation:  {'Analyzing the Roles of Language and Vision in Learning from Limited Data', 'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'AgentKit: Flow Engineering with Graphs, not Coding', 'Compositional Chain-of-Thought Prompting for Large Multimodal Models', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'On the Empirical Complexity of Reasoning and Planning in LLMs', 'Social Choice for AI Alignment: Dealing with Diverse Human Feedback'}\n",
      "For the paper TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones  we have the following recommendation:  {'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality', 'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models', 'RankCLIP: Ranking-Consistent Language-Image Pretraining', 'Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models', 'Compositional Chain-of-Thought Prompting for Large Multimodal Models', 'TextSquare: Scaling up Text-Centric Visual Instruction Tuning', 'Cross-Modal Adapter: Parameter-Efficient Transfer Learning Approach for\\n  Vision-Language Models', 'Bridging Vision and Language Spaces with Assignment Prediction', 'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models', 'Find The Gap: Knowledge Base Reasoning For Visual Question Answering'}\n",
      "For the paper Fast Inference of Mixture-of-Experts Language Models with Offloading  we have the following recommendation:  {'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium', 'LD-Pruner: Efficient Pruning of Latent Diffusion Models using\\n  Task-Agnostic Insights', 'Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\\n  Prediction', 'How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study', 'TriForce: Lossless Acceleration of Long Sequence Generation with\\n  Hierarchical Speculative Decoding', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'Pre-training Small Base LMs with Fewer Tokens'}\n",
      "For the paper Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language Models  we have the following recommendation:  {'Many-Shot In-Context Learning', 'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality', 'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models', 'Self-Supervised Visual Preference Alignment', 'Compositional Chain-of-Thought Prompting for Large Multimodal Models', 'Find The Gap: Knowledge Base Reasoning For Visual Question Answering', 'TextSquare: Scaling up Text-Centric Visual Instruction Tuning', 'Bridging Vision and Language Spaces with Assignment Prediction', 'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models', 'Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models'}\n",
      "For the paper LLM Augmented LLMs: Expanding Capabilities through Composition  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'Pre-training Small Base LMs with Fewer Tokens', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'CroissantLLM: A Truly Bilingual French-English Language Model', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'Towards Logically Consistent Language Models via Probabilistic Reasoning', 'Jamba: A Hybrid Transformer-Mamba Language Model'}\n",
      "For the paper Mixtral of Experts  we have the following recommendation:  {'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'Pre-training Small Base LMs with Fewer Tokens', 'Multi-Head Mixture-of-Experts', 'CroissantLLM: A Truly Bilingual French-English Language Model', 'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving', 'TriForce: Lossless Acceleration of Long Sequence Generation with\\n  Hierarchical Speculative Decoding', 'Jamba: A Hybrid Transformer-Mamba Language Model'}\n",
      "For the paper ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT  we have the following recommendation:  {'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models', 'Retrieval Augmented Generation for Domain-specific Question Answering', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking', 'LayerNorm: A key component in parameter-efficient fine-tuning', 'LongEmbed: Extending Embedding Models for Long Context Retrieval', 'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving', 'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees'}\n",
      "For the paper Transformers are Multi-State RNNs  we have the following recommendation:  {'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory', 'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models', 'SpaceByte: Towards Deleting Tokenization from Large Language Modeling', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models', 'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory'}\n",
      "For the paper GenCast: Diffusion-based ensemble forecasting for medium-range weather  we have the following recommendation:  {'Probabilistic forecasting of power system imbalance using neural\\n  network-based ensembles', 'Four-hour thunderstorm nowcasting using deep diffusion models of\\n  satellite', 'Uncertainty Aware Tropical Cyclone Wind Speed Estimation from Satellite\\n  Data', 'Recurrent Neural Networks for Modelling Gross Primary Production', 'Fuxi-DA: A Generalized Deep Learning Data Assimilation Framework for\\n  Assimilating Satellite Observations', 'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods', 'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion', 'Toward Routing River Water in Land Surface Models with Recurrent Neural\\n  Networks', 'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting', 'Scalable Data Assimilation with Message Passing'}\n",
      "For the paper The Unreasonable Effectiveness of Easy Training Data for Hard Tasks  we have the following recommendation:  {'Latxa: An Open Language Model and Evaluation Suite for Basque', 'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning', 'Many-Shot In-Context Learning', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Stronger Random Baselines for In-Context Learning', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model', 'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees', 'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction', 'Rethinking LLM Memorization through the Lens of Adversarial Compression', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity'}\n",
      "For the paper Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy  we have the following recommendation:  {'DeNetDM: Debiasing by Network Depth Modulation', 'Graph Neural Aggregation-diffusion with Metastability', 'Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World', 'Multi-Head Mixture-of-Experts', 'Delayed Bottlenecking: Alleviating Forgetting in Pre-trained Graph\\n  Neural Networks', 'A multiobjective continuation method to compute the regularization path of deep neural networks', 'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning', 'Separate, Dynamic and Differentiable (SMART) Pruner for Block/Output Channel Pruning on Computer Vision Tasks', 'Neuro-Inspired Information-Theoretic Hierarchical Perception for\\n  Multimodal Learning', 'Jamba: A Hybrid Transformer-Mamba Language Model'}\n",
      "For the paper Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering  we have the following recommendation:  {'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'AgentKit: Flow Engineering with Graphs, not Coding', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'Automating REST API Postman Test Cases Using LLM', 'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation', 'LLM-Powered Test Case Generation for Detecting Tricky Bugs', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'NExT: Teaching Large Language Models to Reason about Code Execution', 'XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\\n  Upcycled Mixture-of-Experts', 'Language Model Prompt Selection via Simulation Optimization'}\n",
      "For the paper Foundations of Vector Retrieval  we have the following recommendation:  {'iRAG: An Incremental Retrieval Augmented Generation System for Videos', 'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking', 'LongEmbed: Extending Embedding Models for Long Context Retrieval', 'GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers', 'VDTuner: Automated Performance Tuning for Vector Data Management Systems', 'Bridging Vision and Language Spaces with Assignment Prediction'}\n",
      "For the paper An Emulator for Fine-Tuning Large Language Models using Small Language Models  we have the following recommendation:  {'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Learn Your Reference Model for Real Good Alignment', 'Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy\\n  Data', 'Aligning language models with human preferences', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Rethinking LLM Memorization through the Lens of Adversarial Compression', 'Filtered Direct Preference Optimization', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper Tuning Language Models by Proxy  we have the following recommendation:  {'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium', 'Many-Shot In-Context Learning', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy\\n  Data', 'Pre-training Small Base LMs with Fewer Tokens', 'LayerNorm: A key component in parameter-efficient fine-tuning', 'CroissantLLM: A Truly Bilingual French-English Language Model', 'XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\\n  Upcycled Mixture-of-Experts', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper Context is Environment  we have the following recommendation:  {'Many-Shot In-Context Learning', 'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models', 'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Stronger Random Baselines for In-Context Learning', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction'}\n",
      "For the paper Self-Rewarding Language Models  we have the following recommendation:  {'Generalized Population-Based Training for Hyperparameter Optimization in\\n  Reinforcement Learning', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Self-playing Adversarial Language Game Enhances LLM Reasoning', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function', 'Filtered Direct Preference Optimization', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory', 'LongEmbed: Extending Embedding Models for Long Context Retrieval', 'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving', 'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models', 'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory'}\n",
      "For the paper InRanker: Distilled Rankers for Zero-shot Information Retrieval  we have the following recommendation:  {'Estimating the Hessian Matrix of Ranking Objectives for Stochastic\\n  Learning to Rank with Gradient Boosted Trees', 'Retrieval Augmented Generation for Domain-specific Question Answering', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking', 'LongEmbed: Extending Embedding Models for Long Context Retrieval', 'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees'}\n",
      "For the paper Explaining Anomalies using Denoising Autoencoders for Financial Tabular Data  we have the following recommendation:  {'DACAD: Domain Adaptation Contrastive Learning for Anomaly Detection in\\n  Multivariate Time Series', 'A Customer Level Fraudulent Activity Detection Benchmark for Enhancing\\n  Machine Learning Model Research and Evaluation', 'Explainable Artificial Intelligence Techniques for Accurate Fault\\n  Detection and Diagnosis: A Review', 'An improved tabular data generator with VAE-GMM integration', 'Sparse Explanations of Neural Networks Using Pruned Layer-Wise Relevance\\n  Propagation', 'Anomaly Correction of Business Processes Using Transformer Autoencoder', 'Fin-Fed-OD: Federated Outlier Detection on Financial Tabular Data', 'Toward Understanding the Disagreement Problem in Neural Network Feature\\n  Attribution', 'BayesJudge: Bayesian Kernel Language Modelling with Confidence\\n  Uncertainty in Legal Judgment Prediction', 'Gradient strikes back: How filtering out high frequencies improves explanations'}\n",
      "For the paper Learning to Reweight Examples for Robust Deep Learning  we have the following recommendation:  {'SOPHON: Non-Fine-Tunable Learning to Restrain Task Transferability For\\n  Pre-trained Models', 'Distilled Datamodel with Reverse Gradient Matching', 'NC-TTT: A Noise Contrastive Approach for Test-Time Training', 'Gradient-Regularized Out-of-Distribution Detection', 'OTTER: Improving Zero-Shot Classification via Optimal Transport', 'Positive Unlabeled Contrastive Learning', 'A Survey of Deep Long-Tail Classification Advancements', 'DeNetDM: Debiasing by Network Depth Modulation', 'Reactive Model Correction: Mitigating Harm to Task-Relevant Features via\\n  Conditional Bias Suppression', 'Improving Generalization via Meta-Learning on Hard Samples'}\n",
      "For the paper In-Context Learning for Extreme Multi-Label Classification  we have the following recommendation:  {'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning', 'Many-Shot In-Context Learning', 'When LLMs are Unfit Use FastFit: Fast and Effective Text Classification\\n  with Many Classes', 'Stronger Random Baselines for In-Context Learning', 'Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy\\n  Data', 'CroissantLLM: A Truly Bilingual French-English Language Model', 'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees', 'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages'}\n",
      "For the paper Introducing Neural Bag of Whole-Words with ColBERTer: Contextualized Late Interactions using Enhanced Reduction  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'Retrieval Augmented Generation for Domain-specific Question Answering', 'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking', 'LongEmbed: Extending Embedding Models for Long Context Retrieval', 'Conversational Financial Information Retrieval Model (ConFIRM)', 'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees', 'VDTuner: Automated Performance Tuning for Vector Data Management Systems'}\n",
      "For the paper Progress measures for grokking via mechanistic interpretability  we have the following recommendation:  {'Deep Learning as Ricci Flow', 'Training neural networks with structured noise improves classification and generalization', 'Deep Neural Networks via Complex Network Theory: a Perspective', 'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective', 'DXAI: Explaining Classification by Image Decomposition', 'Listen to the Waves: Using a Neuronal Model of the Human Auditory System\\n  to Predict Ocean Waves', 'Engineering software 2.0 by interpolating neural networks: unifying\\n  training, solving, and calibration', 'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory'}\n",
      "For the paper ChatQA: Building GPT-4 Level Conversational QA Models  we have the following recommendation:  {'Many-Shot In-Context Learning', 'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Retrieval Augmented Generation for Domain-specific Question Answering', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving', 'TextSquare: Scaling up Text-Centric Visual Instruction Tuning', 'Towards a Robust Retrieval-Based Summarization System', 'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity'}\n",
      "For the paper Qlib: An AI-oriented Quantitative Investment Platform  we have the following recommendation:  {'Using ARIMA to Predict the Expansion of Subscriber Data Consumption', 'OptiGrad: A Fair and more Efficient Price Elasticity Optimization via a\\n  Gradient Based Learning', 'Decision Mamba: Reinforcement Learning via Sequence Modeling with Selective State Spaces', 'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods', 'VC Theory for Inventory Policies', 'The New Agronomists: Language Models are Experts in Crop Management', 'Digital Twins for forecasting and decision optimisation with machine\\n  learning: applications in wastewater treatment', 'Towards Stable Machine Learning Model Retraining via Slowly Varying Sequences', 'AI Competitions and Benchmarks: Dataset Development', 'Reinforcement Learning in Agent-Based Market Simulation: Unveiling Realistic Stylized Facts and Behavior'}\n",
      "For the paper Matryoshka Representation Learning  we have the following recommendation:  {'Many-Shot In-Context Learning', 'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality', 'RankCLIP: Ranking-Consistent Language-Image Pretraining', 'LongEmbed: Extending Embedding Models for Long Context Retrieval', 'Compositional Chain-of-Thought Prompting for Large Multimodal Models', 'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning', 'Pre-training Small Base LMs with Fewer Tokens', 'Bridging Vision and Language Spaces with Assignment Prediction'}\n",
      "For the paper Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding  we have the following recommendation:  {'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'iTBLS: A Dataset of Interactive Conversations Over Tabular Information', 'AgentKit: Flow Engineering with Graphs, not Coding', 'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation', 'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction', 'Beyond Code Generation: An Observational Study of ChatGPT Usage in\\n  Software Engineering Practice'}\n",
      "For the paper MambaTab: A Simple Yet Effective Approach for Handling Tabular Data  we have the following recommendation:  {'Synthesizing Realistic Data for Table Recognition', 'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning', 'An improved tabular data generator with VAE-GMM integration', 'Towards Universal Performance Modeling for Machine Learning Training on\\n  Multi-GPU Platforms', 'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting', 'Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science', 'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning', 'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey', 'Structured Evaluation of Synthetic Tabular Data', 'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection'}\n",
      "For the paper ReGAL: Refactoring Programs to Discover Generalizable Abstractions  we have the following recommendation:  {'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning', 'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\\n  Prediction', 'Pre-training Small Base LMs with Fewer Tokens', 'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation', 'LLM-Powered Test Case Generation for Detecting Tricky Bugs', 'NExT: Teaching Large Language Models to Reason about Code Execution', 'XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\\n  Upcycled Mixture-of-Experts', 'Jamba: A Hybrid Transformer-Mamba Language Model'}\n",
      "For the paper Learning Transformer Programs  we have the following recommendation:  {'A Learning Paradigm for Interpretable Gradients', 'DXAI: Explaining Classification by Image Decomposition', 'Sparse Explanations of Neural Networks Using Pruned Layer-Wise Relevance\\n  Propagation', 'AlloyBERT: Alloy Property Prediction with Large Language Models', 'LayerNorm: A key component in parameter-efficient fine-tuning', 'Gradient strikes back: How filtering out high frequencies improves explanations', 'Decomposing and Editing Predictions by Modeling Model Computation', 'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory'}\n",
      "For the paper RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval  we have the following recommendation:  {'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models', 'Retrieval Augmented Generation for Domain-specific Question Answering', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'LongEmbed: Extending Embedding Models for Long Context Retrieval', 'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees', 'Towards a Robust Retrieval-Based Summarization System', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity'}\n",
      "For the paper Learning Universal Predictors  we have the following recommendation:  {'Training neural networks with structured noise improves classification and generalization', 'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective', 'MPC of Uncertain Nonlinear Systems with Meta-Learning for Fast\\n  Adaptation of Neural Predictive Models', 'Meta Reinforcement Learning with Finite Training Tasks -- a Density Estimation Approach', 'Improving Generalization via Meta-Learning on Hard Samples', 'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory'}\n",
      "For the paper DoubleEnsemble: A New Ensemble Method Based on Sample Reweighting and Feature Selection for Financial Data Analysis  we have the following recommendation:  {'A Guide to Feature Importance Methods for Scientific Inference', 'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods', 'Towards Stable Machine Learning Model Retraining via Slowly Varying Sequences', 'Explainable LightGBM Approach for Predicting Myocardial Infarction\\n  Mortality', 'Application of the representative measure approach to assess the\\n  reliability of decision trees in dealing with unseen vehicle collision data', 'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion', 'TSLANet: Rethinking Transformers for Time Series Representation Learning', 'A Survey of Deep Long-Tail Classification Advancements', 'A novel decision fusion approach for sale price prediction using Elastic Net and MOPSO', 'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection'}\n",
      "For the paper Time-LLM: Time Series Forecasting by Reprogramming Large Language Models  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Variational quantization for state space models', 'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods', 'DeepHeteroIoT: Deep Local and Global Learning over Heterogeneous IoT Sensor Data', 'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting', 'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion', 'TSLANet: Rethinking Transformers for Time Series Representation Learning', 'σ-GPTs: A New Approach to Autoregressive Models', 'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection', 'Generating Synthetic Time Series Data for Cyber-Physical Systems'}\n",
      "For the paper DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines  we have the following recommendation:  {'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\\n  Prediction', 'Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path\\n  Forward', 'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'LLM-Powered Test Case Generation for Detecting Tricky Bugs', 'Towards a Robust Retrieval-Based Summarization System', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'NExT: Teaching Large Language Models to Reason about Code Execution', 'Language Model Prompt Selection via Simulation Optimization'}\n",
      "For the paper A decoder-only foundation model for time-series forecasting  we have the following recommendation:  {'Learning representations of learning representations', 'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium', 'Advancing Long-Term Multi-Energy Load Forecasting with Patchformer: A\\n  Patch and Transformer-Based Approach', 'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods', 'LayerNorm: A key component in parameter-efficient fine-tuning', 'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting', 'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion', 'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection', 'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey', 'DiJiang: Efficient Large Language Models through Compact Kernelization'}\n",
      "For the paper A Time Series is Worth 64 Words: Long-term Forecasting with Transformers  we have the following recommendation:  {'Variational quantization for state space models', 'Advancing Long-Term Multi-Energy Load Forecasting with Patchformer: A\\n  Patch and Transformer-Based Approach', 'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods', 'DeepHeteroIoT: Deep Local and Global Learning over Heterogeneous IoT Sensor Data', 'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting', 'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion', 'TSLANet: Rethinking Transformers for Time Series Representation Learning', 'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection', 'Generating Synthetic Time Series Data for Cyber-Physical Systems', 'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory'}\n",
      "For the paper TransTab: Learning Transferable Tabular Transformers Across Tables  we have the following recommendation:  {'Synthesizing Realistic Data for Table Recognition', 'Guided Discrete Diffusion for Electronic Health Record Generation', 'Segmentation, Classification and Interpretation of Breast Cancer Medical Images using Human-in-the-Loop Machine Learning', 'Machine Learning Techniques for MRI Data Processing at Expanding Scale', 'Supervised Contrastive Vision Transformer for Breast Histopathological\\n  Image Classification', 'An improved tabular data generator with VAE-GMM integration', 'Time-aware Heterogeneous Graph Transformer with Adaptive Attention\\n  Merging for Health Event Prediction', 'Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science', 'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning', 'Structured Evaluation of Synthetic Tabular Data'}\n",
      "For the paper Spectral State Space Models  we have the following recommendation:  {'Variational quantization for state space models', 'Feature-Based Echo-State Networks: A Step Towards Interpretability and Minimalism in Reservoir Computer', 'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory', 'Computationally and Memory-Efficient Robust Predictive Analytics Using Big Data', 'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting', 'TSLANet: Rethinking Transformers for Time Series Representation Learning', 'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey', 'σ-GPTs: A New Approach to Autoregressive Models', 'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection', 'Generating Synthetic Time Series Data for Cyber-Physical Systems'}\n",
      "For the paper Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models', 'SpaceByte: Towards Deleting Tokenization from Large Language Modeling', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models', 'DiJiang: Efficient Large Language Models through Compact Kernelization', 'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey', 'Pre-training Small Base LMs with Fewer Tokens', 'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory'}\n",
      "For the paper Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Analyzing the Roles of Language and Vision in Learning from Limited Data', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models', 'Towards Logically Consistent Language Models via Probabilistic Reasoning', 'On the Empirical Complexity of Reasoning and Planning in LLMs', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper In-Context Principle Learning from Mistakes  we have the following recommendation:  {'Latxa: An Open Language Model and Evaluation Suite for Basque', 'Many-Shot In-Context Learning', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity', 'Retrieval Augmented Generation for Domain-specific Question Answering', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'TextSquare: Scaling up Text-Centric Visual Instruction Tuning', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'QAGCN: Answering Multi-Relation Questions via Single-Step Implicit Reasoning over Knowledge Graphs', 'Find The Gap: Knowledge Base Reasoning For Visual Question Answering'}\n",
      "For the paper Neural Network Diffusion  we have the following recommendation:  {'LD-Pruner: Efficient Pruning of Latent Diffusion Models using\\n  Task-Agnostic Insights', 'An improved tabular data generator with VAE-GMM integration', 'SparseDM: Toward Sparse Efficient Diffusion Models', 'Physics-integrated generative modeling using attentive planar\\n  normalizing flow based variational autoencoder', 'Generative Modelling with High-Order Langevin Dynamics', 'Neural Flow Diffusion Models: Learnable Forward Process for Improved\\n  Diffusion Modelling'}\n",
      "For the paper RAGIC: Risk-Aware Generative Adversarial Model for Stock Interval Construction  we have the following recommendation:  {'Guided Discrete Diffusion for Electronic Health Record Generation', 'Conformal Prediction for Stochastic Decision-Making of PV Power in Electricity Markets', 'Variational quantization for state space models', 'An improved tabular data generator with VAE-GMM integration', 'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods', 'TSLANet: Rethinking Transformers for Time Series Representation Learning', 'σ-GPTs: A New Approach to Autoregressive Models', 'Reinforcement Learning in Agent-Based Market Simulation: Unveiling Realistic Stylized Facts and Behavior'}\n",
      "For the paper OmniPred: Language Models as Universal Regressors  we have the following recommendation:  {'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning', 'Learning representations of learning representations', 'CroissantLLM: A Truly Bilingual French-English Language Model', 'FedEval-LLM: Federated Evaluation of Large Language Models on Downstream\\n  Tasks with Collective Wisdom', 'Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science', 'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages', 'Pre-training Small Base LMs with Fewer Tokens', 'Using LLMs to Model the Beliefs and Preferences of Targeted Populations'}\n",
      "For the paper A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts  we have the following recommendation:  {'Many-Shot In-Context Learning', 'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs'}\n",
      "For the paper A Comparative Study of AI-Generated (GPT-4) and Human-crafted MCQs in Programming Education  we have the following recommendation:  {'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'LLM-Powered Test Case Generation for Detecting Tricky Bugs', 'TextSquare: Scaling up Text-Centric Visual Instruction Tuning', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'Towards a Robust Retrieval-Based Summarization System', 'NExT: Teaching Large Language Models to Reason about Code Execution', 'Beyond Code Generation: An Observational Study of ChatGPT Usage in\\n  Software Engineering Practice', 'Language Model Prompt Selection via Simulation Optimization', 'CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\\n  Granularity'}\n",
      "For the paper Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models  we have the following recommendation:  {'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection', 'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey', 'Pre-training Small Base LMs with Fewer Tokens', 'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory'}\n",
      "For the paper Latent Attention for Linear Time Transformers  we have the following recommendation:  {'Toward a Theory of Tokenization in LLMs', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving', 'TriForce: Lossless Acceleration of Long Sequence Generation with\\n  Hierarchical Speculative Decoding', 'DiJiang: Efficient Large Language Models through Compact Kernelization', 'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey', 'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models', 'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory'}\n",
      "For the paper Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution  we have the following recommendation:  {'Toward a Theory of Tokenization in LLMs', 'LD-Pruner: Efficient Pruning of Latent Diffusion Models using\\n  Task-Agnostic Insights', 'EdgeFusion: On-Device Text-to-Image Generation', 'Pre-training Small Base LMs with Fewer Tokens', 'Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging\\n  Perturbations That Efficiently Fool Customized Diffusion Models', 'Neural Flow Diffusion Models: Learnable Forward Process for Improved\\n  Diffusion Modelling'}\n",
      "For the paper Repetition Improves Language Model Embeddings  we have the following recommendation:  {'Many-Shot In-Context Learning', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'SpaceByte: Towards Deleting Tokenization from Large Language Modeling', 'LongEmbed: Extending Embedding Models for Long Context Retrieval', 'CroissantLLM: A Truly Bilingual French-English Language Model', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\\n  in Large Language Models', 'Pre-training Small Base LMs with Fewer Tokens'}\n",
      "For the paper Fine-tuning with Very Large Dropout  we have the following recommendation:  {'Distilled Datamodel with Reverse Gradient Matching', 'Gradient-Regularized Out-of-Distribution Detection', 'A Survey of Deep Long-Tail Classification Advancements', 'DeNetDM: Debiasing by Network Depth Modulation', 'LoRA Dropout as a Sparsity Regularizer for Overfitting Control', 'Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World', 'Toward a Realistic Benchmark for Out-of-Distribution Detection', 'Noise contrastive estimation with soft targets for conditional models'}\n",
      "For the paper Is Cosine-Similarity of Embeddings Really About Similarity?  we have the following recommendation:  {'How Deep Networks Learn Sparse and Hierarchical Data: the Sparse Random\\n  Hierarchy Model', 'Deep Regression Representation Learning with Topology', 'Closing the Gap in the Trade-off between Fair Representations and\\n  Accuracy', 'LongEmbed: Extending Embedding Models for Long Context Retrieval', 'Hyperbolic Delaunay Geometric Alignment', 'A multiobjective continuation method to compute the regularization path of deep neural networks', 'Review-Based Cross-Domain Recommendation via Hyperbolic Embedding and Hierarchy-Aware Domain Disentanglement', 'LipSim: A Provably Robust Perceptual Similarity Metric'}\n",
      "For the paper Dual Operating Modes of In-Context Learning  we have the following recommendation:  {'Many-Shot In-Context Learning', 'NC-TTT: A Noise Contrastive Approach for Test-Time Training', 'Meta-Learning with Generalized Ridge Regression: High-dimensional Asymptotics, Optimality and Hyper-covariance Estimation', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Stronger Random Baselines for In-Context Learning', 'OTTER: Improving Zero-Shot Classification via Optimal Transport', 'Meta Reinforcement Learning with Finite Training Tasks -- a Density Estimation Approach', 'MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning', 'Dynamically Anchored Prompting for Task-Imbalanced Continual Learning', 'Localizing Paragraph Memorization in Language Models'}\n",
      "For the paper Chronos: Learning the Language of Time Series  we have the following recommendation:  {'Toward a Theory of Tokenization in LLMs', 'Variational quantization for state space models', 'Advancing Long-Term Multi-Energy Load Forecasting with Patchformer: A\\n  Patch and Transformer-Based Approach', 'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods', 'DeepHeteroIoT: Deep Local and Global Learning over Heterogeneous IoT Sensor Data', 'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting', 'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion', 'TSLANet: Rethinking Transformers for Time Series Representation Learning', 'Generating Synthetic Time Series Data for Cyber-Physical Systems', 'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection'}\n",
      "For the paper How Far Are We from Intelligent Visual Deductive Reasoning?  we have the following recommendation:  {'Concept-based Analysis of Neural Networks via Vision-Language Models', 'Analyzing the Roles of Language and Vision in Learning from Limited Data', 'Pre-trained Vision-Language Models Learn Discoverable Visual Concepts', 'Exploring the Transferability of Visual Prompting for Multimodal Large\\n  Language Models', 'Self-Supervised Visual Preference Alignment', 'Compositional Chain-of-Thought Prompting for Large Multimodal Models', 'Find The Gap: Knowledge Base Reasoning For Visual Question Answering', 'TextSquare: Scaling up Text-Centric Visual Instruction Tuning', 'Bridging Vision and Language Spaces with Assignment Prediction', 'Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models'}\n",
      "For the paper Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset  we have the following recommendation:  {'Concept-based Analysis of Neural Networks via Vision-Language Models', 'Analyzing the Roles of Language and Vision in Learning from Limited Data', 'Pre-trained Vision-Language Models Learn Discoverable Visual Concepts', 'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation', 'Self-Supervised Visual Preference Alignment', 'GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers', 'TextSquare: Scaling up Text-Centric Visual Instruction Tuning', 'NExT: Teaching Large Language Models to Reason about Code Execution', 'Beyond Code Generation: An Observational Study of ChatGPT Usage in\\n  Software Engineering Practice', 'Groma: Localized Visual Tokenization for Grounding Multimodal Large\\n  Language Models'}\n",
      "For the paper Data Interpreter: An LLM Agent For Data Science  we have the following recommendation:  {'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\\n  Prediction', 'AgentKit: Flow Engineering with Graphs, not Coding', 'Automating REST API Postman Test Cases Using LLM', 'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models', 'CT-Agent: Clinical Trial Multi-Agent with Large Language Model-based\\n  Reasoning', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'On the Empirical Complexity of Reasoning and Planning in LLMs'}\n",
      "For the paper Mechanics of Next Token Prediction with Self-Attention  we have the following recommendation:  {'Toward a Theory of Tokenization in LLMs', 'Transformers Can Represent $n$-gram Language Models', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\\n  Compression for Large Language Models', 'SpaceByte: Towards Deleting Tokenization from Large Language Modeling', 'Localizing Paragraph Memorization in Language Models', 'What do Transformers Know about Government?', 'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey', 'DiJiang: Efficient Large Language Models through Compact Kernelization', 'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory'}\n",
      "For the paper Transformers for Supervised Online Continual Learning  we have the following recommendation:  {'Continual Offline Reinforcement Learning via Diffusion-based Dual\\n  Generative Replay', 'FedMeS: Personalized Federated Continual Learning Leveraging Local\\n  Memory', 'Continual Learning on a Diet: Learning from Sparsely Labeled Streams\\n  Under Constrained Computation', 'Revisiting Neural Networks for Continual Learning: An Architectural\\n  Perspective', 'A Unified Replay-based Continuous Learning Framework for Spatio-Temporal\\n  Prediction on Streaming Data', 'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection', 'Dynamically Anchored Prompting for Task-Imbalanced Continual Learning', 'Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer', 'Offline Trajectory Generalization for Offline Reinforcement Learning', 'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory'}\n",
      "For the paper Transformer-Based Models Are Not Yet Perfect At Learning to Emulate Structural Recursion  we have the following recommendation:  {'Transformers Can Represent $n$-gram Language Models', 'Symbolic Integration Algorithm Selection with Machine Learning: LSTMs vs\\n  Tree LSTMs', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'What do Transformers Know about Government?', 'Rethinking LLM Memorization through the Lens of Adversarial Compression', 'On the Empirical Complexity of Reasoning and Planning in LLMs', 'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory'}\n",
      "For the paper TnT-LLM: Text Mining at Scale with Large Language Models  we have the following recommendation:  {'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning', 'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Retrieval Augmented Generation for Domain-specific Question Answering', 'Conversational Financial Information Retrieval Model (ConFIRM)', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees', 'Towards Reliable Latent Knowledge Estimation in LLMs: In-Context\\n  Learning vs. Prompting Based Factual Knowledge Extraction', 'CT-Agent: Clinical Trial Multi-Agent with Large Language Model-based\\n  Reasoning', 'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages'}\n",
      "For the paper FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions  we have the following recommendation:  {'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models', 'Retrieval Augmented Generation for Domain-specific Question Answering', 'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation', 'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking', 'LongEmbed: Extending Embedding Models for Long Context Retrieval', 'Conversational Financial Information Retrieval Model (ConFIRM)', 'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees', 'Towards a Robust Retrieval-Based Summarization System'}\n",
      "For the paper Gecko: Versatile Text Embeddings Distilled from Large Language Models  we have the following recommendation:  {'ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'Retrieval Augmented Generation for Domain-specific Question Answering', 'Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking', 'Learning Word Embedding with Better Distance Weighting and Window Size\\n  Scheduling', 'LongEmbed: Extending Embedding Models for Long Context Retrieval', 'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees', 'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages'}\n",
      "For the paper Stream of Search (SoS): Learning to Search in Language  we have the following recommendation:  {'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'Self-playing Adversarial Language Game Enhances LLM Reasoning', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models', 'NExT: Teaching Large Language Models to Reason about Code Execution', 'From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function', 'Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\\n  Large Language Model'}\n",
      "For the paper TimeGPT-1  we have the following recommendation:  {'A Comparison of Traditional and Deep Learning Methods for Parameter\\n  Estimation of the Ornstein-Uhlenbeck Process', 'Variational quantization for state space models', 'FMint: Bridging Human Designed and Data Pretrained Models for\\n  Differential Equation Foundation Model', 'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods', 'Computationally and Memory-Efficient Robust Predictive Analytics Using Big Data', 'DeepHeteroIoT: Deep Local and Global Learning over Heterogeneous IoT Sensor Data', 'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting', 'TSLANet: Rethinking Transformers for Time Series Representation Learning', 'Generating Synthetic Time Series Data for Cyber-Physical Systems', 'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection'}\n",
      "For the paper Addressing the Abstraction and Reasoning Corpus via Procedural Example Generation  we have the following recommendation:  {'NC-TTT: A Noise Contrastive Approach for Test-Time Training', 'Zero-Shot Stitching in Reinforcement Learning using Relative\\n  Representations', 'PyTorchGeoNodes: Enabling Differentiable Shape Programs for 3D Shape\\n  Reconstruction', 'GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers', 'ASID: Active Exploration for System Identification in Robotic\\n  Manipulation', 'Enhancing Length Extrapolation in Sequential Models with\\n  Pointer-Augmented Neural Memory'}\n",
      "For the paper From Words to Numbers: Your Large Language Model Is Secretly A Capable Regressor When Given In-Context Examples  we have the following recommendation:  {'Large Language Models Can Automatically Engineer Features for Few-Shot\\n  Tabular Learning', 'Many-Shot In-Context Learning', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Stronger Random Baselines for In-Context Learning', 'AdapterSwap: Continuous Training of LLMs with Data Removal and\\n  Access-Control Guarantees', 'Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science', 'Nearly Optimal Algorithms for Contextual Dueling Bandits from\\n  Adversarial Feedback', 'Quantifying Multilingual Performance of Large Language Models Across\\n  Languages', 'Rethinking LLM Memorization through the Lens of Adversarial Compression', 'Pre-training Small Base LMs with Fewer Tokens'}\n",
      "For the paper The Rise of Diffusion Models in Time-Series Forecasting  we have the following recommendation:  {'Using ARIMA to Predict the Expansion of Subscriber Data Consumption', 'Variational quantization for state space models', 'Integrating Marketing Channels into Quantile Transformation and Bayesian\\n  Optimization of Ensemble Kernels for Sales Prediction with Gaussian Process\\n  Models', 'TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods', 'ICST-DNET: An Interpretable Causal Spatio-Temporal Diffusion Network for\\n  Traffic Speed Prediction', 'Integrating Mamba and Transformer for Long-Short Range Time Series\\n  Forecasting', 'SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\\n  Fusion', 'MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection', 'Neural McKean-Vlasov Processes: Distributional Dependence in Diffusion\\n  Processes', 'Neural Flow Diffusion Models: Learnable Forward Process for Improved\\n  Diffusion Modelling'}\n",
      "For the paper The Illusion of State in State-Space Models  we have the following recommendation:  {'Toward a Theory of Tokenization in LLMs', 'Uncertainty Quantification for In-Context Learning of Large Language Models', 'Transformers Can Represent $n$-gram Language Models', 'Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\\n  Prediction', 'Hierarchical Context Merging: Better Long Context Understanding for\\n  Pre-trained LLMs', 'What do Transformers Know about Government?', 'State Space Model for New-Generation Network Alternative to\\n  Transformers: A Survey', 'Jamba: A Hybrid Transformer-Mamba Language Model', 'LongVQ: Long Sequence Modeling with Vector Quantization on Structured\\n  Memory', 'Rethinking LLM Memorization through the Lens of Adversarial Compression'}\n",
      "For the paper AgentKit: Flow Engineering with Graphs, not Coding  we have the following recommendation:  {'Advancing Social Intelligence in AI Agents: Technical Challenges and\\n  Open Questions', 'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'iTBLS: A Dataset of Interactive Conversations Over Tabular Information', 'AgentKit: Flow Engineering with Graphs, not Coding', 'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation', 'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'Toward Self-Improvement of LLMs via Imagination, Searching, and\\n  Criticizing', 'NExT: Teaching Large Language Models to Reason about Code Execution', 'Beyond Code Generation: An Observational Study of ChatGPT Usage in\\n  Software Engineering Practice'}\n",
      "For the paper GoEX: Perspectives and Designs Towards a Runtime for Autonomous LLM Applications  we have the following recommendation:  {'Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning', 'AgentKit: Flow Engineering with Graphs, not Coding', 'Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path\\n  Forward', 'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation', 'Automating REST API Postman Test Cases Using LLM', 'RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\\n  Human Feedback for LLMs', 'Aligning LLM Agents by Learning Latent Preference from User Edits', 'Demonstration of DB-GPT: Next Generation Data Interaction System\\n  Empowered by Large Language Models', 'NExT: Teaching Large Language Models to Reason about Code Execution', 'Beyond Code Generation: An Observational Study of ChatGPT Usage in\\n  Software Engineering Practice'}\n"
     ]
    }
   ],
   "source": [
    "for i in list(recom_euc.keys()):\n",
    "    ints = recom_euc[i].intersection(recom[i])\n",
    "    print(\"For the paper\",i, \" we have the following recommendation: \",ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.83251965, 0.83714686, 0.83714686, 0.83714686, 0.84503516,\n",
       "        0.87948477, 0.89165143, 0.89840561, 0.89925162, 0.89932382]),\n",
       " array([0.65345549, 0.64959256, 0.64959256, 0.64959256, 0.6429578 ,\n",
       "        0.61325325, 0.60247883, 0.59643367, 0.59567325, 0.5956083 ]))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recom_by_summary_score_euc[0], recom_by_summary_score_cos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
