{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOYmCZo9ItMS",
        "outputId": "22593436-2db9-4095-edcf-52f4f6be778c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Check if TensorFlow can detect a GPU\n",
        "gpu_device_name = tf.test.gpu_device_name()\n",
        "\n",
        "if gpu_device_name:\n",
        "    print('GPU device found:', gpu_device_name)\n",
        "else:\n",
        "    print(\"No GPU available. Using CPU instead.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFb7ETMHI2Vi",
        "outputId": "65f9dedf-f7e5-44b3-973a-0e1bea74546a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU available. Using CPU instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "i-EpBCWlI7dU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "4LqN0JlZHZz0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading (with noise ) and preprocessing"
      ],
      "metadata": {
        "id": "9c3a58sAJDMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data= pd.read_csv(\"/content/drive/MyDrive/AML/A2/cf_train.csv\")\n",
        "test_data = pd.read_csv(\"/content/drive/MyDrive/AML/A2/cf_test.csv\")"
      ],
      "metadata": {
        "id": "cTw0dXmjJCej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape, test_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzOsFblLJH7O",
        "outputId": "c2f7e100-eb31-4a56-8e1b-b84b8ba42bd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((62400, 31), (62400, 31))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the mapping of values to indices\n",
        "value_to_index = {0.00: 0, 0.25: 1, 0.5: 2, 0.75: 3, 1.00: 4}\n",
        "\n",
        "# Replace values with corresponding indices\n",
        "train_data[\"target_10_val\"] = train_data[\"target_10_val\"].replace(value_to_index).astype(int)\n",
        "test_data[\"target_10_val\"] = test_data[\"target_10_val\"].replace(value_to_index).astype(int)"
      ],
      "metadata": {
        "id": "kt-hYefvJKsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "\n",
        "train_data[\"sigma\"] = label_encoder.fit_transform(train_data[\"sigma\"])\n",
        "test_data[\"sigma\"] = label_encoder.fit_transform(test_data[\"sigma\"])"
      ],
      "metadata": {
        "id": "eAvMu2AsJN-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize scaler for feature scaling\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Train the model on cf_train.csv\n",
        "X_train = train_data.drop(columns=[\"era\", \"day\", \"target_10_val\"])\n",
        "y_train = train_data[\"target_10_val\"]\n",
        "X_train_scaled = scaler.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "eYxFIgltJeTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MAML (Model-Agnostic Meta Learning)"
      ],
      "metadata": {
        "id": "QdYppnVgJhVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(5, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "vio4OBiEJqYE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define MAML hyperparameters\n",
        "learning_rate_inner = 0.01\n",
        "learning_rate_outer = 0.001\n",
        "num_inner_updates = 5\n",
        "batch_size_outer = 32"
      ],
      "metadata": {
        "id": "1o4iLnq-JuBe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function and optimizer\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate_outer)"
      ],
      "metadata": {
        "id": "5rEV1PTKJwA1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure X_train has the correct shape\n",
        "X_train_arr = np.array(X_train)  # Convert to numpy array if not already\n",
        "X_train_arr = X_train_arr.reshape(-1, 28)  # R"
      ],
      "metadata": {
        "id": "zDKXKrv7Bdvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-train MAML\n",
        "for _ in range(100):  # Number of pre-training epochs\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = model(X_train_arr)\n",
        "        loss = loss_fn(y_train, y_pred)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
      ],
      "metadata": {
        "id": "m8CJTMT7Jxu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Continual learning loop\n",
        "predictions = []\n",
        "for i, test_row in test_data[:10000].iterrows():\n",
        "    if i < 15:\n",
        "      predictions.append(3)\n",
        "      continue\n",
        "\n",
        "    # Adapt model to new task on using 5 rows from i-10 to i - 5\n",
        "    X_task = scaler.transform(test_data.iloc[i-10:i-5].drop(columns=[\"era\", \"day\", \"target_10_val\"]))\n",
        "    y_task = test_data.iloc[i-10:i-5][\"target_10_val\"]\n",
        "    inner_optimizer = tf.keras.optimizers.Adam(learning_rate_inner)\n",
        "\n",
        "    # Ensure X_train has the correct shape\n",
        "    X_task = np.array(X_task)  # Convert to numpy array if not already\n",
        "    X_task = X_task.reshape(-1, 28)  # R\n",
        "\n",
        "    for _ in range(num_inner_updates):\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred_task = model(X_task)\n",
        "            task_loss = loss_fn(y_task, y_pred_task)\n",
        "        gradients_task = tape.gradient(task_loss, model.trainable_variables)\n",
        "        inner_optimizer.apply_gradients(zip(gradients_task, model.trainable_variables))\n",
        "\n",
        "    # Predict on the current sample\n",
        "    X_current = scaler.transform(test_row.drop(columns=[\"era\", \"day\", \"target_10_val\"])[X_train.columns].values.reshape(1, -1))\n",
        "    y_pred_current = model(X_current)\n",
        "    y_pred = np.argmax(y_pred_current, axis=1)[0]  # Extract the predicted class\n",
        "    predictions.append(y_pred)\n"
      ],
      "metadata": {
        "id": "8TslqWQGJz8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Evaluate accuracy\n",
        "true_labels = test_data[\"target_10_val\"]\n",
        "accuracy = np.mean(predictions == true_labels[:len(predictions)])\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8n6-Quj6o5W",
        "outputId": "589ac721-c1fd-4ed3-a685-44f98ef73832"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.4833\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Without Noise"
      ],
      "metadata": {
        "id": "dOGDTs1_LfQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset loading and prepreocessing"
      ],
      "metadata": {
        "id": "HKDVcYSBgn9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data= pd.read_csv(\"/content/drive/MyDrive/AML/A2/cf_train_no_noise.csv\")\n",
        "test_data = pd.read_csv(\"/content/drive/MyDrive/AML/A2/cf_test_no_noise.csv\")"
      ],
      "metadata": {
        "id": "X2b-KCY_Zre0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the mapping of values to indices\n",
        "value_to_index = {0.00: 0, 0.25: 1, 0.5: 2, 0.75: 3, 1.00: 4}\n",
        "\n",
        "# Replace values with corresponding indices\n",
        "train_data[\"target_10_val\"] = train_data[\"target_10_val\"].replace(value_to_index).astype(int)\n",
        "test_data[\"target_10_val\"] = test_data[\"target_10_val\"].replace(value_to_index).astype(int)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "train_data[\"sigma\"] = label_encoder.fit_transform(train_data[\"sigma\"])\n",
        "test_data[\"sigma\"] = label_encoder.fit_transform(test_data[\"sigma\"])\n",
        "\n",
        "# Initialize scaler for feature scaling\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Train the model on cf_train.csv\n",
        "X_train = train_data.drop(columns=[\"era\", \"day\", \"target_10_val\"])\n",
        "y_train = train_data[\"target_10_val\"]\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n"
      ],
      "metadata": {
        "id": "c5WVoOPpgh3D"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure X_train has the correct shape\n",
        "X_train_arr = np.array(X_train_scaled)  # Convert to numpy array if not already\n",
        "X_train_arr = X_train_arr.reshape(-1, 28)  # R\n",
        "\n",
        "\n",
        "# Pre-train MAML\n",
        "for _ in range(100):  # Number of pre-training epochs\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = model(X_train_arr)\n",
        "        loss = loss_fn(y_train, y_pred)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n"
      ],
      "metadata": {
        "id": "o9IzSHbVgycG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Continual learning loop\n",
        "predictions = []\n",
        "for i, test_row in test_data[:10000].iterrows():\n",
        "    if i < 15:\n",
        "      predictions.append(3)\n",
        "      continue\n",
        "\n",
        "    # Adapt model to new task on using 5 rows from i-10 to i - 5\n",
        "    X_task = scaler.transform(test_data.iloc[i-10:i-5].drop(columns=[\"era\", \"day\", \"target_10_val\"]))\n",
        "    y_task = test_data.iloc[i-10:i-5][\"target_10_val\"]\n",
        "    inner_optimizer = tf.keras.optimizers.Adam(learning_rate_inner)\n",
        "\n",
        "    # Ensure X_train has the correct shape\n",
        "    X_task = np.array(X_task)  # Convert to numpy array if not already\n",
        "    X_task = X_task.reshape(-1, 28)  # R\n",
        "\n",
        "    for _ in range(num_inner_updates):\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred_task = model(X_task)\n",
        "            task_loss = loss_fn(y_task, y_pred_task)\n",
        "        gradients_task = tape.gradient(task_loss, model.trainable_variables)\n",
        "        inner_optimizer.apply_gradients(zip(gradients_task, model.trainable_variables))\n",
        "\n",
        "    # Predict on the current sample\n",
        "    X_current = scaler.transform(test_row.drop(columns=[\"era\", \"day\", \"target_10_val\"])[X_train.columns].values.reshape(1, -1))\n",
        "    y_pred_current = model(X_current)\n",
        "    y_pred = np.argmax(y_pred_current, axis=1)[0]  # Extract the predicted class\n",
        "    predictions.append(y_pred)\n"
      ],
      "metadata": {
        "id": "QI8jy9Uxg44n"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Evaluate accuracy\n",
        "true_labels = test_data[\"target_10_val\"]\n",
        "accuracy = np.mean(predictions == true_labels[:len(predictions)])\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYkIV1W9g6Yx",
        "outputId": "ef62329e-3488-4516-d715-798cda4dedea"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8576\n"
          ]
        }
      ]
    }
  ]
}